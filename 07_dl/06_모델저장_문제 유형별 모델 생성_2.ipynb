{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장\n",
    "\n",
    "- 학습한 모델을 저장장치에 파일로 저장하고 나중에 불러와 사용(추가 학습, 예측 서비스) 할 수 있도록 한다. \n",
    "- 파이토치는 모델의 파라미터만 저장하는 방법과 모델 구조와 파라미터 모두를 저장하는 두가지 방식을 제공한다.\n",
    "- 저장 함수\n",
    "    - `torch.save(저장할 객체, 저장경로)`\n",
    "- 보통 저장파일의 확장자는 `pt`나 `pth` 를 지정한다.\n",
    "\n",
    "## 모델 전체 저장하기 및 불러오기\n",
    "\n",
    "- 저장하기\n",
    "    - `torch.save(model, 저장경로)`\n",
    "- 불러오기\n",
    "    - `load_model = torch.load(저장경로)`\n",
    "- 저장시 **pickle**을 이용해 직렬화하기 때문에 불어오는 실행환경에도 모델을 저장할 때 사용한 클래스가 있어야 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 모델의 파라미터만 저장\n",
    "- 모델을 구성하는 파라미터만 저장한다.\n",
    "- 모델의 구조는 저장하지 않기 때문에 불러올 때 **모델을 먼저 생성하고 생성한 모델에 불러온 파라미터를 덮어씌운다.**\n",
    "- 모델의 파라미터는 **state_dict** 형식으로 저장한다.\n",
    "\n",
    "### state_dict\n",
    "- 모델의 파라미터 Tensor들을 레이어 단위별로 나누어 저장한 Ordered Dictionary (OrderedDict)\n",
    "- `모델객체.state_dict()` 메소드를 이용해 조회한다.\n",
    "- 모델의 state_dict을 조회 후 저장한다.\n",
    "    - `torch.save(model.state_dict(), \"저장경로\")`\n",
    "- 생성된 모델에 읽어온 state_dict를 덮어씌운다.\n",
    "    - `new_model.load_state_dict(torch.load(\"state_dict저장경로\"))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_layer = nn.Linear(784, 64)\n",
    "        self.out = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = torch.flatten(X, start_dim=1)\n",
    "        X = nn.ReLU()(self.in_layer(X))\n",
    "        X = self.out(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (in_layer): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (out): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model = Network()\n",
    "sample_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['in_layer.weight', 'in_layer.bias', 'out.weight', 'out.bias'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dict 조회\n",
    "sd = sample_model.state_dict()\n",
    "print(type(sd))\n",
    "sd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 784]), torch.Size([64]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd['in_layer.weight'].shape, sd['in_layer.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001FDFEC2F1B0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0101,  0.0306, -0.0164,  0.0321, -0.0119,  0.0323,  0.0212, -0.0171,\n",
       "        -0.0073, -0.0351,  0.0132,  0.0166, -0.0245, -0.0214,  0.0279, -0.0108,\n",
       "        -0.0248,  0.0193, -0.0216, -0.0066, -0.0010, -0.0283, -0.0340,  0.0041,\n",
       "        -0.0054, -0.0333, -0.0097, -0.0122, -0.0289, -0.0154, -0.0025, -0.0081,\n",
       "        -0.0186,  0.0063, -0.0092, -0.0047, -0.0262, -0.0220,  0.0330,  0.0064,\n",
       "        -0.0155, -0.0262,  0.0228, -0.0118, -0.0217,  0.0313, -0.0102,  0.0343,\n",
       "         0.0271,  0.0144,  0.0318,  0.0012, -0.0118,  0.0072, -0.0020,  0.0168,\n",
       "        -0.0297,  0.0301, -0.0074, -0.0079,  0.0264, -0.0033,  0.0261, -0.0161],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model.in_layer.weight\n",
    "sample_model.in_layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint를 저장 및 불러오기\n",
    "- 학습이 끝나지 않은 모델을 저장 후 나중에 이어서 학습시킬 경우에는 모델의 구조, 파라미터 뿐만 아니라 optimizer, loss 함수등 학습에 필요한 객체들을 저장해야 한다.\n",
    "- Dictionary에 필요한 요소들을 key-value 쌍으로 저장후 `torch.save()`를 이용해 저장한다.\n",
    "```python\n",
    "# 저장\n",
    "torch.save({\n",
    "    'epoch':epoch,\n",
    "    'model_state_dict':model.state_dict(),\n",
    "    'optimizer_state_dict':optimizer.state_dict(),\n",
    "    'loss':train_loss\n",
    "}, \"저장경로\")\n",
    "\n",
    "# 불러오기\n",
    "model = MyModel()\n",
    "optimizer = optim.Adam(model.parameter())\n",
    "\n",
    "checkpoint = torch.load(\"저장경로\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "#### 이어학습\n",
    "model.train()\n",
    "#### 추론\n",
    "model.eval()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 문제 유형별 MLP 네트워크\n",
    "- MLP(Multi Layer Perceptron)\n",
    "    - Fully Connected Layer로 구성된 네트워크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchinfo\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Regression(회귀)\n",
    "\n",
    "## Boston Housing Dataset\n",
    "보스턴 주택가격 dataset은 다음과 같은 속성을 바탕으로 해당 타운 주택 가격의 중앙값을 예측하는 문제.\n",
    "- CRIM: 범죄율\n",
    "- ZN: 25,000 평방피트당 주거지역 비율\n",
    "- INDUS: 비소매 상업지구 비율\n",
    "- CHAS: 찰스강에 인접해 있는지 여부(인접:1, 아니면:0)\n",
    "- NOX: 일산화질소 농도(단위: 0.1ppm)\n",
    "- RM: 주택당 방의 수\n",
    "- AGE: 1940년 이전에 건설된 주택의 비율\n",
    "- DIS: 5개의 보스턴 직업고용센터와의 거리(가중 평균)\n",
    "- RAD: 고속도로 접근성\n",
    "- TAX: 재산세율\n",
    "- PTRATIO: 학생/교사 비율\n",
    "- B: 흑인 비율\n",
    "- LSTAT: 하위 계층 비율\n",
    "<br><br>\n",
    "- **Target**\n",
    "    - MEDV: 타운의 주택가격 중앙값(단위: 1,000달러)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset, DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506, 1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston = pd.read_csv('boston_hosing.csv')\n",
    "boston.shape\n",
    "\n",
    "X_boston = boston.drop(columns='MEDV').values\n",
    "y_boston = boston['MEDV'].values.reshape(-1, 1) # 2차원\n",
    "X_boston.shape, y_boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test set 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_boston, y_boston,\n",
    "                                                    test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = torch.tensor(scaler.fit_transform(X_train), dtype=torch.float32)\n",
    "X_test_scaled = torch.tensor(scaler.transform(X_test), dtype=torch.float32)\n",
    "# y를 Tensor 타입으로 변환.\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 102\n",
      "(tensor([-0.3726, -0.4996, -0.7049,  3.6645, -0.4249,  0.9357,  0.6937, -0.4372,\n",
      "        -0.1622, -0.5617, -0.4846,  0.3717, -0.4110]), tensor([26.7000]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset\n",
    "boston_train_set = TensorDataset(X_train_scaled, y_train_tensor)\n",
    "boston_test_set = TensorDataset(X_test_scaled, y_test_tensor)\n",
    "print(len(boston_train_set), len(boston_test_set))\n",
    "print(boston_train_set[0])\n",
    "\n",
    "# DataLoader\n",
    "boston_train_loader = DataLoader(boston_train_set, batch_size=200, \n",
    "                                 shuffle=True, drop_last=True)\n",
    "boston_test_loader = DataLoader(boston_test_set, batch_size=len(boston_test_set))\n",
    "len(boston_train_loader), len(boston_test_loader)  # epoch 당 step 수 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 입력 layer => in_feature: input data의 feature개수에 맞춘다.\n",
    "        self.lr1 = nn.Linear(13, 32)   # input\n",
    "        # Hidden layer => in_feature: 앞 Layer의 out_feature 개수에 맞춘다.\n",
    "        self.lr2 = nn.Linear(32, 16)  \n",
    "        # output layer => out_feature: 모델의 최종 출력 개수에 맞춘다. (집값 1개->1)\n",
    "        self.lr3 = nn.Linear(16, 1)\n",
    "    \n",
    "        \n",
    "    def forward(self, X):\n",
    "        # input layer\n",
    "        out = self.lr1(X)\n",
    "        out = nn.ReLU()(out)\n",
    "        # hidden\n",
    "        out = self.lr2(out)\n",
    "        out = nn.ReLU()(out)\n",
    "        # output -> 회귀처리 모델에서 output layer에서는 활성함수를 적용하지 않는다.\n",
    "        #   예외: 출력결과가 특정 활성함수의 출력과 매칭될 경우.\n",
    "        #        output: 0 ~ 1 => logistic 함수사용.\n",
    "        #        output: -1 ~ 1=> hyperbolic tangent (tanh)\n",
    "        out  = self.lr3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BostonModel                              [200, 1]                  --\n",
       "├─Linear: 1-1                            [200, 32]                 448\n",
       "├─Linear: 1-2                            [200, 16]                 528\n",
       "├─Linear: 1-3                            [200, 1]                  17\n",
       "==========================================================================================\n",
       "Total params: 993\n",
       "Trainable params: 993\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.20\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.08\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_model = BostonModel()\n",
    "torchinfo.summary(boston_model, (200, 13))  #(모델, 입력데이터shape-(batch size, feature) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCH = 1000\n",
    "LR = 0.001\n",
    "# 결과 저장할 리스트\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "# 모델, loss함수(회귀-MSE), optimizer\n",
    "boston_model = BostonModel()\n",
    "boston_model = boston_model.to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.RMSprop(boston_model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1000] train loss: 594.37888, val loss: 570.43408\n",
      "[2/1000] train loss: 588.66394, val loss: 562.88055\n",
      "[3/1000] train loss: 578.88843, val loss: 554.63916\n",
      "[4/1000] train loss: 570.28241, val loss: 545.62061\n",
      "[5/1000] train loss: 566.16193, val loss: 535.91327\n",
      "[6/1000] train loss: 548.04317, val loss: 525.59784\n",
      "[7/1000] train loss: 544.15288, val loss: 514.36932\n",
      "[8/1000] train loss: 533.58899, val loss: 502.53384\n",
      "[9/1000] train loss: 520.16202, val loss: 490.04745\n",
      "[10/1000] train loss: 503.44720, val loss: 477.12799\n",
      "[11/1000] train loss: 491.97881, val loss: 463.80646\n",
      "[12/1000] train loss: 477.20920, val loss: 450.13510\n",
      "[13/1000] train loss: 462.22078, val loss: 436.23578\n",
      "[14/1000] train loss: 444.92764, val loss: 422.03226\n",
      "[15/1000] train loss: 432.14780, val loss: 407.62064\n",
      "[16/1000] train loss: 415.96544, val loss: 393.07483\n",
      "[17/1000] train loss: 398.83139, val loss: 378.47281\n",
      "[18/1000] train loss: 383.42761, val loss: 363.88638\n",
      "[19/1000] train loss: 368.05484, val loss: 349.37817\n",
      "[20/1000] train loss: 353.38503, val loss: 334.92365\n",
      "[21/1000] train loss: 332.67911, val loss: 320.70105\n",
      "[22/1000] train loss: 321.87312, val loss: 306.36954\n",
      "[23/1000] train loss: 301.61053, val loss: 292.33179\n",
      "[24/1000] train loss: 288.17752, val loss: 278.65146\n",
      "[25/1000] train loss: 272.13469, val loss: 265.46292\n",
      "[26/1000] train loss: 259.12704, val loss: 252.69608\n",
      "[27/1000] train loss: 246.58121, val loss: 240.46005\n",
      "[28/1000] train loss: 231.71478, val loss: 228.92163\n",
      "[29/1000] train loss: 215.85726, val loss: 217.97432\n",
      "[30/1000] train loss: 207.66993, val loss: 207.50531\n",
      "[31/1000] train loss: 195.67847, val loss: 197.70747\n",
      "[32/1000] train loss: 184.70064, val loss: 188.51060\n",
      "[33/1000] train loss: 173.69517, val loss: 179.90436\n",
      "[34/1000] train loss: 163.70976, val loss: 171.89708\n",
      "[35/1000] train loss: 155.40461, val loss: 164.39897\n",
      "[36/1000] train loss: 147.08203, val loss: 157.43117\n",
      "[37/1000] train loss: 139.42784, val loss: 150.92471\n",
      "[38/1000] train loss: 132.05980, val loss: 144.90482\n",
      "[39/1000] train loss: 124.56440, val loss: 139.31673\n",
      "[40/1000] train loss: 118.96735, val loss: 134.11900\n",
      "[41/1000] train loss: 112.88076, val loss: 129.32040\n",
      "[42/1000] train loss: 106.91881, val loss: 124.91167\n",
      "[43/1000] train loss: 103.19195, val loss: 120.71027\n",
      "[44/1000] train loss: 95.64942, val loss: 116.93513\n",
      "[45/1000] train loss: 94.00289, val loss: 113.32394\n",
      "[46/1000] train loss: 89.68946, val loss: 109.93806\n",
      "[47/1000] train loss: 86.00298, val loss: 106.78872\n",
      "[48/1000] train loss: 81.77673, val loss: 103.86143\n",
      "[49/1000] train loss: 79.14735, val loss: 101.08900\n",
      "[50/1000] train loss: 76.01100, val loss: 98.45947\n",
      "[51/1000] train loss: 73.04960, val loss: 95.98785\n",
      "[52/1000] train loss: 70.08507, val loss: 93.65334\n",
      "[53/1000] train loss: 65.05212, val loss: 91.48329\n",
      "[54/1000] train loss: 65.53206, val loss: 89.35059\n",
      "[55/1000] train loss: 63.14630, val loss: 87.30978\n",
      "[56/1000] train loss: 61.38455, val loss: 85.41808\n",
      "[57/1000] train loss: 59.28687, val loss: 83.62177\n",
      "[58/1000] train loss: 57.10672, val loss: 81.92748\n",
      "[59/1000] train loss: 55.42293, val loss: 80.28802\n",
      "[60/1000] train loss: 53.60099, val loss: 78.71456\n",
      "[61/1000] train loss: 49.56720, val loss: 77.24380\n",
      "[62/1000] train loss: 50.28346, val loss: 75.69942\n",
      "[63/1000] train loss: 47.28542, val loss: 74.24863\n",
      "[64/1000] train loss: 47.40870, val loss: 72.83549\n",
      "[65/1000] train loss: 45.98719, val loss: 71.46269\n",
      "[66/1000] train loss: 43.60907, val loss: 70.25201\n",
      "[67/1000] train loss: 43.64853, val loss: 69.03398\n",
      "[68/1000] train loss: 41.93509, val loss: 67.89504\n",
      "[69/1000] train loss: 41.34433, val loss: 66.77949\n",
      "[70/1000] train loss: 40.04880, val loss: 65.70341\n",
      "[71/1000] train loss: 38.81893, val loss: 64.73434\n",
      "[72/1000] train loss: 38.34298, val loss: 63.77055\n",
      "[73/1000] train loss: 37.39608, val loss: 62.85468\n",
      "[74/1000] train loss: 36.09349, val loss: 61.95921\n",
      "[75/1000] train loss: 35.71299, val loss: 61.10288\n",
      "[76/1000] train loss: 34.58481, val loss: 60.30463\n",
      "[77/1000] train loss: 34.17209, val loss: 59.54082\n",
      "[78/1000] train loss: 33.60917, val loss: 58.77293\n",
      "[79/1000] train loss: 32.56846, val loss: 58.05486\n",
      "[80/1000] train loss: 31.90950, val loss: 57.33314\n",
      "[81/1000] train loss: 31.31500, val loss: 56.69246\n",
      "[82/1000] train loss: 30.87609, val loss: 56.04202\n",
      "[83/1000] train loss: 30.46867, val loss: 55.41656\n",
      "[84/1000] train loss: 29.85958, val loss: 54.81738\n",
      "[85/1000] train loss: 29.57012, val loss: 54.23070\n",
      "[86/1000] train loss: 28.95261, val loss: 53.61811\n",
      "[87/1000] train loss: 28.43888, val loss: 53.05193\n",
      "[88/1000] train loss: 27.97784, val loss: 52.49604\n",
      "[89/1000] train loss: 27.56157, val loss: 51.98011\n",
      "[90/1000] train loss: 27.21108, val loss: 51.45367\n",
      "[91/1000] train loss: 26.88984, val loss: 50.95412\n",
      "[92/1000] train loss: 25.67509, val loss: 50.47048\n",
      "[93/1000] train loss: 26.13541, val loss: 50.02700\n",
      "[94/1000] train loss: 25.92374, val loss: 49.54753\n",
      "[95/1000] train loss: 25.62069, val loss: 49.09598\n",
      "[96/1000] train loss: 23.23847, val loss: 48.80254\n",
      "[97/1000] train loss: 24.76808, val loss: 48.37637\n",
      "[98/1000] train loss: 24.66081, val loss: 47.91581\n",
      "[99/1000] train loss: 24.28041, val loss: 47.49018\n",
      "[100/1000] train loss: 24.20727, val loss: 47.10630\n",
      "[101/1000] train loss: 23.95191, val loss: 46.69148\n",
      "[102/1000] train loss: 23.59808, val loss: 46.25690\n",
      "[103/1000] train loss: 23.46990, val loss: 45.87302\n",
      "[104/1000] train loss: 23.06732, val loss: 45.52484\n",
      "[105/1000] train loss: 22.97179, val loss: 45.20013\n",
      "[106/1000] train loss: 22.80039, val loss: 44.88390\n",
      "[107/1000] train loss: 22.46075, val loss: 44.53638\n",
      "[108/1000] train loss: 22.44215, val loss: 44.20955\n",
      "[109/1000] train loss: 21.95626, val loss: 43.92160\n",
      "[110/1000] train loss: 21.60365, val loss: 43.63688\n",
      "[111/1000] train loss: 21.36169, val loss: 43.29761\n",
      "[112/1000] train loss: 21.66782, val loss: 43.02995\n",
      "[113/1000] train loss: 21.46171, val loss: 42.74675\n",
      "[114/1000] train loss: 21.11337, val loss: 42.43841\n",
      "[115/1000] train loss: 21.24596, val loss: 42.18327\n",
      "[116/1000] train loss: 20.70803, val loss: 41.93864\n",
      "[117/1000] train loss: 20.90133, val loss: 41.64139\n",
      "[118/1000] train loss: 18.96504, val loss: 41.51876\n",
      "[119/1000] train loss: 20.48494, val loss: 41.22355\n",
      "[120/1000] train loss: 20.50382, val loss: 40.94671\n",
      "[121/1000] train loss: 20.15074, val loss: 40.63837\n",
      "[122/1000] train loss: 20.09080, val loss: 40.37399\n",
      "[123/1000] train loss: 20.02295, val loss: 40.09501\n",
      "[124/1000] train loss: 19.89075, val loss: 39.85528\n",
      "[125/1000] train loss: 19.80662, val loss: 39.65081\n",
      "[126/1000] train loss: 19.68623, val loss: 39.44688\n",
      "[127/1000] train loss: 19.53397, val loss: 39.18604\n",
      "[128/1000] train loss: 19.37829, val loss: 38.99076\n",
      "[129/1000] train loss: 19.08894, val loss: 38.74807\n",
      "[130/1000] train loss: 19.05601, val loss: 38.50860\n",
      "[131/1000] train loss: 18.77457, val loss: 38.29783\n",
      "[132/1000] train loss: 18.93945, val loss: 38.04119\n",
      "[133/1000] train loss: 18.68240, val loss: 37.85474\n",
      "[134/1000] train loss: 18.56606, val loss: 37.65210\n",
      "[135/1000] train loss: 18.61533, val loss: 37.44573\n",
      "[136/1000] train loss: 18.40655, val loss: 37.22993\n",
      "[137/1000] train loss: 18.42374, val loss: 37.06831\n",
      "[138/1000] train loss: 18.21906, val loss: 36.93315\n",
      "[139/1000] train loss: 17.89433, val loss: 36.76494\n",
      "[140/1000] train loss: 18.01266, val loss: 36.57037\n",
      "[141/1000] train loss: 17.65322, val loss: 36.34428\n",
      "[142/1000] train loss: 17.86106, val loss: 36.20615\n",
      "[143/1000] train loss: 17.70987, val loss: 36.01036\n",
      "[144/1000] train loss: 17.40926, val loss: 35.86336\n",
      "[145/1000] train loss: 17.41410, val loss: 35.67381\n",
      "[146/1000] train loss: 17.17671, val loss: 35.46183\n",
      "[147/1000] train loss: 17.30713, val loss: 35.30429\n",
      "[148/1000] train loss: 17.19514, val loss: 35.13166\n",
      "[149/1000] train loss: 17.16928, val loss: 34.94283\n",
      "[150/1000] train loss: 17.08226, val loss: 34.79747\n",
      "[151/1000] train loss: 16.48721, val loss: 34.56900\n",
      "[152/1000] train loss: 16.97955, val loss: 34.38063\n",
      "[153/1000] train loss: 16.72632, val loss: 34.18617\n",
      "[154/1000] train loss: 16.64289, val loss: 34.06793\n",
      "[155/1000] train loss: 16.57964, val loss: 33.92138\n",
      "[156/1000] train loss: 16.42066, val loss: 33.83148\n",
      "[157/1000] train loss: 16.19176, val loss: 33.64802\n",
      "[158/1000] train loss: 16.31036, val loss: 33.49192\n",
      "[159/1000] train loss: 15.98592, val loss: 33.37859\n",
      "[160/1000] train loss: 16.18191, val loss: 33.23966\n",
      "[161/1000] train loss: 16.09659, val loss: 33.05972\n",
      "[162/1000] train loss: 15.63031, val loss: 32.95806\n",
      "[163/1000] train loss: 15.69392, val loss: 32.85466\n",
      "[164/1000] train loss: 15.85941, val loss: 32.74502\n",
      "[165/1000] train loss: 15.69601, val loss: 32.60904\n",
      "[166/1000] train loss: 15.67155, val loss: 32.39585\n",
      "[167/1000] train loss: 15.51041, val loss: 32.33676\n",
      "[168/1000] train loss: 15.08885, val loss: 32.11640\n",
      "[169/1000] train loss: 15.32249, val loss: 32.04140\n",
      "[170/1000] train loss: 15.28005, val loss: 31.93475\n",
      "[171/1000] train loss: 15.30472, val loss: 31.77993\n",
      "[172/1000] train loss: 15.15501, val loss: 31.67103\n",
      "[173/1000] train loss: 15.10671, val loss: 31.55642\n",
      "[174/1000] train loss: 14.72147, val loss: 31.53170\n",
      "[175/1000] train loss: 14.90168, val loss: 31.39972\n",
      "[176/1000] train loss: 14.78180, val loss: 31.21549\n",
      "[177/1000] train loss: 14.43297, val loss: 31.09301\n",
      "[178/1000] train loss: 14.70107, val loss: 31.02994\n",
      "[179/1000] train loss: 14.68054, val loss: 30.87478\n",
      "[180/1000] train loss: 14.57710, val loss: 30.77544\n",
      "[181/1000] train loss: 14.25503, val loss: 30.64750\n",
      "[182/1000] train loss: 14.43558, val loss: 30.48214\n",
      "[183/1000] train loss: 14.34798, val loss: 30.40458\n",
      "[184/1000] train loss: 14.32441, val loss: 30.29029\n",
      "[185/1000] train loss: 14.18427, val loss: 30.15951\n",
      "[186/1000] train loss: 14.07807, val loss: 30.03232\n",
      "[187/1000] train loss: 14.07722, val loss: 29.92075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[188/1000] train loss: 14.03581, val loss: 29.85709\n",
      "[189/1000] train loss: 13.43996, val loss: 29.75305\n",
      "[190/1000] train loss: 13.95107, val loss: 29.69041\n",
      "[191/1000] train loss: 13.42567, val loss: 29.68712\n",
      "[192/1000] train loss: 13.74028, val loss: 29.57350\n",
      "[193/1000] train loss: 13.67047, val loss: 29.48989\n",
      "[194/1000] train loss: 13.55108, val loss: 29.29179\n",
      "[195/1000] train loss: 13.52221, val loss: 29.23738\n",
      "[196/1000] train loss: 13.50324, val loss: 29.07234\n",
      "[197/1000] train loss: 13.39694, val loss: 29.00282\n",
      "[198/1000] train loss: 13.38988, val loss: 28.90586\n",
      "[199/1000] train loss: 13.27199, val loss: 28.76713\n",
      "[200/1000] train loss: 13.24864, val loss: 28.72983\n",
      "[201/1000] train loss: 13.17902, val loss: 28.60137\n",
      "[202/1000] train loss: 13.08836, val loss: 28.50629\n",
      "[203/1000] train loss: 13.09205, val loss: 28.39671\n",
      "[204/1000] train loss: 13.07368, val loss: 28.25850\n",
      "[205/1000] train loss: 12.96353, val loss: 28.17633\n",
      "[206/1000] train loss: 12.82444, val loss: 28.03016\n",
      "[207/1000] train loss: 12.67687, val loss: 27.98951\n",
      "[208/1000] train loss: 12.70986, val loss: 27.87662\n",
      "[209/1000] train loss: 12.66675, val loss: 27.80449\n",
      "[210/1000] train loss: 12.63183, val loss: 27.78686\n",
      "[211/1000] train loss: 12.71218, val loss: 27.69827\n",
      "[212/1000] train loss: 12.36597, val loss: 27.55536\n",
      "[213/1000] train loss: 12.62513, val loss: 27.46457\n",
      "[214/1000] train loss: 12.50241, val loss: 27.46326\n",
      "[215/1000] train loss: 12.41280, val loss: 27.34576\n",
      "[216/1000] train loss: 12.26592, val loss: 27.28527\n",
      "[217/1000] train loss: 12.24279, val loss: 27.10080\n",
      "[218/1000] train loss: 12.25381, val loss: 27.10903\n",
      "[219/1000] train loss: 12.18726, val loss: 27.02219\n",
      "[220/1000] train loss: 12.14659, val loss: 26.97163\n",
      "[221/1000] train loss: 12.02293, val loss: 27.00105\n",
      "[222/1000] train loss: 12.01487, val loss: 26.86224\n",
      "[223/1000] train loss: 12.00609, val loss: 26.68814\n",
      "[224/1000] train loss: 11.93788, val loss: 26.60067\n",
      "[225/1000] train loss: 11.92581, val loss: 26.58886\n",
      "[226/1000] train loss: 11.86964, val loss: 26.47900\n",
      "[227/1000] train loss: 11.73508, val loss: 26.38459\n",
      "[228/1000] train loss: 11.69593, val loss: 26.31735\n",
      "[229/1000] train loss: 11.74489, val loss: 26.20397\n",
      "[230/1000] train loss: 11.64393, val loss: 26.24422\n",
      "[231/1000] train loss: 11.55100, val loss: 26.04883\n",
      "[232/1000] train loss: 11.55110, val loss: 25.99339\n",
      "[233/1000] train loss: 11.60232, val loss: 26.01097\n",
      "[234/1000] train loss: 11.59488, val loss: 25.82929\n",
      "[235/1000] train loss: 11.21060, val loss: 25.77105\n",
      "[236/1000] train loss: 11.39755, val loss: 25.68769\n",
      "[237/1000] train loss: 11.14070, val loss: 25.64801\n",
      "[238/1000] train loss: 11.28085, val loss: 25.51578\n",
      "[239/1000] train loss: 11.26766, val loss: 25.43241\n",
      "[240/1000] train loss: 11.16425, val loss: 25.39799\n",
      "[241/1000] train loss: 10.99222, val loss: 25.25740\n",
      "[242/1000] train loss: 11.05197, val loss: 25.28619\n",
      "[243/1000] train loss: 11.01592, val loss: 25.28972\n",
      "[244/1000] train loss: 10.97615, val loss: 25.14513\n",
      "[245/1000] train loss: 11.01981, val loss: 25.20477\n",
      "[246/1000] train loss: 10.82013, val loss: 25.06525\n",
      "[247/1000] train loss: 10.88403, val loss: 24.95768\n",
      "[248/1000] train loss: 10.76104, val loss: 24.91826\n",
      "[249/1000] train loss: 10.78041, val loss: 24.95745\n",
      "[250/1000] train loss: 10.80693, val loss: 24.92653\n",
      "[251/1000] train loss: 10.66562, val loss: 24.73041\n",
      "[252/1000] train loss: 10.59292, val loss: 24.67128\n",
      "[253/1000] train loss: 10.59006, val loss: 24.62319\n",
      "[254/1000] train loss: 10.56822, val loss: 24.53422\n",
      "[255/1000] train loss: 10.54556, val loss: 24.53687\n",
      "[256/1000] train loss: 10.49152, val loss: 24.41533\n",
      "[257/1000] train loss: 10.57498, val loss: 24.26634\n",
      "[258/1000] train loss: 10.42486, val loss: 24.22130\n",
      "[259/1000] train loss: 10.31366, val loss: 24.26229\n",
      "[260/1000] train loss: 10.28528, val loss: 24.22242\n",
      "[261/1000] train loss: 10.30132, val loss: 24.21204\n",
      "[262/1000] train loss: 10.24222, val loss: 24.01513\n",
      "[263/1000] train loss: 10.19821, val loss: 23.91505\n",
      "[264/1000] train loss: 10.14103, val loss: 23.94556\n",
      "[265/1000] train loss: 10.12702, val loss: 23.88242\n",
      "[266/1000] train loss: 9.66882, val loss: 23.81281\n",
      "[267/1000] train loss: 10.07026, val loss: 23.85555\n",
      "[268/1000] train loss: 10.02216, val loss: 23.73909\n",
      "[269/1000] train loss: 9.85561, val loss: 23.72317\n",
      "[270/1000] train loss: 9.98178, val loss: 23.76594\n",
      "[271/1000] train loss: 9.96686, val loss: 23.53141\n",
      "[272/1000] train loss: 9.96854, val loss: 23.44014\n",
      "[273/1000] train loss: 9.87528, val loss: 23.52809\n",
      "[274/1000] train loss: 9.82330, val loss: 23.47962\n",
      "[275/1000] train loss: 9.72203, val loss: 23.35970\n",
      "[276/1000] train loss: 9.76649, val loss: 23.26747\n",
      "[277/1000] train loss: 9.69699, val loss: 23.24095\n",
      "[278/1000] train loss: 9.65801, val loss: 23.11929\n",
      "[279/1000] train loss: 9.59102, val loss: 23.20731\n",
      "[280/1000] train loss: 9.58087, val loss: 23.16298\n",
      "[281/1000] train loss: 9.62241, val loss: 23.29539\n",
      "[282/1000] train loss: 9.33344, val loss: 23.24485\n",
      "[283/1000] train loss: 9.50953, val loss: 23.03555\n",
      "[284/1000] train loss: 9.41317, val loss: 23.08447\n",
      "[285/1000] train loss: 9.44688, val loss: 23.13357\n",
      "[286/1000] train loss: 9.42465, val loss: 23.04274\n",
      "[287/1000] train loss: 9.36814, val loss: 22.85034\n",
      "[288/1000] train loss: 9.24898, val loss: 22.84921\n",
      "[289/1000] train loss: 9.30204, val loss: 22.83014\n",
      "[290/1000] train loss: 9.32965, val loss: 22.94359\n",
      "[291/1000] train loss: 9.28044, val loss: 22.78734\n",
      "[292/1000] train loss: 9.26466, val loss: 22.65631\n",
      "[293/1000] train loss: 9.22884, val loss: 22.75187\n",
      "[294/1000] train loss: 9.08291, val loss: 22.63802\n",
      "[295/1000] train loss: 9.13667, val loss: 22.59308\n",
      "[296/1000] train loss: 9.13111, val loss: 22.60541\n",
      "[297/1000] train loss: 9.05537, val loss: 22.56880\n",
      "[298/1000] train loss: 9.03827, val loss: 22.49344\n",
      "[299/1000] train loss: 9.05668, val loss: 22.41800\n",
      "[300/1000] train loss: 9.05271, val loss: 22.32274\n",
      "[301/1000] train loss: 9.00753, val loss: 22.27941\n",
      "[302/1000] train loss: 8.89554, val loss: 22.37279\n",
      "[303/1000] train loss: 8.91528, val loss: 22.30659\n",
      "[304/1000] train loss: 8.87551, val loss: 22.20521\n",
      "[305/1000] train loss: 8.85374, val loss: 22.26135\n",
      "[306/1000] train loss: 8.80971, val loss: 22.10411\n",
      "[307/1000] train loss: 8.80294, val loss: 22.11705\n",
      "[308/1000] train loss: 8.61108, val loss: 22.15240\n",
      "[309/1000] train loss: 8.78421, val loss: 22.13099\n",
      "[310/1000] train loss: 8.73566, val loss: 22.16265\n",
      "[311/1000] train loss: 8.62865, val loss: 22.02986\n",
      "[312/1000] train loss: 8.66538, val loss: 21.90084\n",
      "[313/1000] train loss: 8.74907, val loss: 21.93768\n",
      "[314/1000] train loss: 8.63366, val loss: 22.00946\n",
      "[315/1000] train loss: 8.66442, val loss: 21.78962\n",
      "[316/1000] train loss: 8.09188, val loss: 21.69755\n",
      "[317/1000] train loss: 8.65903, val loss: 21.85656\n",
      "[318/1000] train loss: 8.50888, val loss: 21.83022\n",
      "[319/1000] train loss: 8.55217, val loss: 21.84654\n",
      "[320/1000] train loss: 8.33702, val loss: 21.78170\n",
      "[321/1000] train loss: 8.53318, val loss: 21.78749\n",
      "[322/1000] train loss: 8.46816, val loss: 21.80314\n",
      "[323/1000] train loss: 7.84886, val loss: 21.81380\n",
      "[324/1000] train loss: 8.42308, val loss: 21.83437\n",
      "[325/1000] train loss: 8.33379, val loss: 21.62877\n",
      "[326/1000] train loss: 8.28043, val loss: 21.58463\n",
      "[327/1000] train loss: 8.36665, val loss: 21.40585\n",
      "[328/1000] train loss: 8.34686, val loss: 21.56573\n",
      "[329/1000] train loss: 8.27344, val loss: 21.64790\n",
      "[330/1000] train loss: 8.22656, val loss: 21.50201\n",
      "[331/1000] train loss: 8.34385, val loss: 21.38845\n",
      "[332/1000] train loss: 8.25229, val loss: 21.52815\n",
      "[333/1000] train loss: 8.25823, val loss: 21.39484\n",
      "[334/1000] train loss: 8.20572, val loss: 21.29698\n",
      "[335/1000] train loss: 8.18234, val loss: 21.31812\n",
      "[336/1000] train loss: 8.24981, val loss: 21.10219\n",
      "[337/1000] train loss: 8.07305, val loss: 21.41368\n",
      "[338/1000] train loss: 8.07861, val loss: 21.38037\n",
      "[339/1000] train loss: 8.03853, val loss: 21.29380\n",
      "[340/1000] train loss: 8.02716, val loss: 21.34398\n",
      "[341/1000] train loss: 8.07452, val loss: 21.50116\n",
      "[342/1000] train loss: 8.08144, val loss: 21.35050\n",
      "[343/1000] train loss: 7.80700, val loss: 21.28852\n",
      "[344/1000] train loss: 7.96274, val loss: 21.21440\n",
      "[345/1000] train loss: 7.92839, val loss: 21.26912\n",
      "[346/1000] train loss: 7.93607, val loss: 21.08392\n",
      "[347/1000] train loss: 7.91948, val loss: 20.88797\n",
      "[348/1000] train loss: 7.95258, val loss: 20.96720\n",
      "[349/1000] train loss: 7.93101, val loss: 21.01478\n",
      "[350/1000] train loss: 8.01045, val loss: 21.13121\n",
      "[351/1000] train loss: 7.85057, val loss: 21.01485\n",
      "[352/1000] train loss: 7.82201, val loss: 20.77231\n",
      "[353/1000] train loss: 7.88409, val loss: 21.05342\n",
      "[354/1000] train loss: 7.87889, val loss: 21.10302\n",
      "[355/1000] train loss: 7.73251, val loss: 20.87316\n",
      "[356/1000] train loss: 7.76127, val loss: 20.96920\n",
      "[357/1000] train loss: 7.77721, val loss: 20.92745\n",
      "[358/1000] train loss: 7.77487, val loss: 20.80115\n",
      "[359/1000] train loss: 7.70211, val loss: 20.89678\n",
      "[360/1000] train loss: 7.78552, val loss: 20.73849\n",
      "[361/1000] train loss: 7.72262, val loss: 20.69732\n",
      "[362/1000] train loss: 7.71013, val loss: 20.66927\n",
      "[363/1000] train loss: 7.63990, val loss: 20.65397\n",
      "[364/1000] train loss: 7.68150, val loss: 20.74825\n",
      "[365/1000] train loss: 7.69686, val loss: 20.74300\n",
      "[366/1000] train loss: 7.67845, val loss: 20.63718\n",
      "[367/1000] train loss: 7.64220, val loss: 20.77313\n",
      "[368/1000] train loss: 7.59205, val loss: 20.68545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[369/1000] train loss: 7.60804, val loss: 20.66871\n",
      "[370/1000] train loss: 7.54912, val loss: 20.60338\n",
      "[371/1000] train loss: 7.62600, val loss: 20.82917\n",
      "[372/1000] train loss: 7.58908, val loss: 20.41633\n",
      "[373/1000] train loss: 7.51596, val loss: 20.56753\n",
      "[374/1000] train loss: 7.54869, val loss: 20.52883\n",
      "[375/1000] train loss: 7.57290, val loss: 20.34956\n",
      "[376/1000] train loss: 7.47508, val loss: 20.49363\n",
      "[377/1000] train loss: 7.24138, val loss: 20.58097\n",
      "[378/1000] train loss: 7.43049, val loss: 20.48440\n",
      "[379/1000] train loss: 7.44113, val loss: 20.34090\n",
      "[380/1000] train loss: 7.43820, val loss: 20.29401\n",
      "[381/1000] train loss: 7.47553, val loss: 20.45416\n",
      "[382/1000] train loss: 7.42168, val loss: 20.62436\n",
      "[383/1000] train loss: 7.42469, val loss: 20.37425\n",
      "[384/1000] train loss: 7.49128, val loss: 20.38163\n",
      "[385/1000] train loss: 7.48760, val loss: 20.17632\n",
      "[386/1000] train loss: 7.30771, val loss: 20.30191\n",
      "[387/1000] train loss: 7.26711, val loss: 20.40251\n",
      "[388/1000] train loss: 7.39609, val loss: 20.36347\n",
      "[389/1000] train loss: 7.26633, val loss: 20.27132\n",
      "[390/1000] train loss: 7.31863, val loss: 20.37713\n",
      "[391/1000] train loss: 7.27064, val loss: 20.20066\n",
      "[392/1000] train loss: 7.28867, val loss: 20.37342\n",
      "[393/1000] train loss: 7.23167, val loss: 20.22309\n",
      "[394/1000] train loss: 7.20246, val loss: 20.37081\n",
      "[395/1000] train loss: 7.27257, val loss: 20.41336\n",
      "[396/1000] train loss: 7.25073, val loss: 20.31857\n",
      "[397/1000] train loss: 7.25477, val loss: 20.16534\n",
      "[398/1000] train loss: 7.17311, val loss: 20.08847\n",
      "[399/1000] train loss: 7.19676, val loss: 20.17995\n",
      "[400/1000] train loss: 7.23384, val loss: 20.25499\n",
      "[401/1000] train loss: 7.09945, val loss: 20.23890\n",
      "[402/1000] train loss: 7.15184, val loss: 20.03019\n",
      "[403/1000] train loss: 7.14391, val loss: 20.17805\n",
      "[404/1000] train loss: 7.09341, val loss: 20.07927\n",
      "[405/1000] train loss: 7.15191, val loss: 20.04379\n",
      "[406/1000] train loss: 7.10964, val loss: 19.92924\n",
      "[407/1000] train loss: 6.94126, val loss: 20.22589\n",
      "[408/1000] train loss: 7.09306, val loss: 20.03697\n",
      "[409/1000] train loss: 7.09890, val loss: 20.05600\n",
      "[410/1000] train loss: 7.15921, val loss: 19.82458\n",
      "[411/1000] train loss: 7.13786, val loss: 19.98990\n",
      "[412/1000] train loss: 7.03779, val loss: 19.93831\n",
      "[413/1000] train loss: 7.01910, val loss: 20.12156\n",
      "[414/1000] train loss: 6.94541, val loss: 19.87494\n",
      "[415/1000] train loss: 6.95963, val loss: 19.92639\n",
      "[416/1000] train loss: 7.00870, val loss: 19.92414\n",
      "[417/1000] train loss: 6.97601, val loss: 20.02238\n",
      "[418/1000] train loss: 6.97840, val loss: 19.94024\n",
      "[419/1000] train loss: 6.79651, val loss: 19.91020\n",
      "[420/1000] train loss: 6.90743, val loss: 19.78201\n",
      "[421/1000] train loss: 6.89622, val loss: 20.03865\n",
      "[422/1000] train loss: 6.94411, val loss: 19.79696\n",
      "[423/1000] train loss: 6.80466, val loss: 19.82132\n",
      "[424/1000] train loss: 6.88547, val loss: 19.76959\n",
      "[425/1000] train loss: 6.93294, val loss: 19.86389\n",
      "[426/1000] train loss: 6.90907, val loss: 19.92383\n",
      "[427/1000] train loss: 6.83386, val loss: 19.58194\n",
      "[428/1000] train loss: 6.85324, val loss: 19.76365\n",
      "[429/1000] train loss: 6.87000, val loss: 19.64162\n",
      "[430/1000] train loss: 6.80416, val loss: 19.84632\n",
      "[431/1000] train loss: 6.80403, val loss: 19.73771\n",
      "[432/1000] train loss: 6.84377, val loss: 19.49700\n",
      "[433/1000] train loss: 6.84449, val loss: 19.68327\n",
      "[434/1000] train loss: 6.81585, val loss: 19.44563\n",
      "[435/1000] train loss: 6.95308, val loss: 19.51394\n",
      "[436/1000] train loss: 6.81279, val loss: 19.79251\n",
      "[437/1000] train loss: 6.72095, val loss: 19.55271\n",
      "[438/1000] train loss: 6.77447, val loss: 19.71077\n",
      "[439/1000] train loss: 6.75611, val loss: 19.70464\n",
      "[440/1000] train loss: 6.74123, val loss: 19.57368\n",
      "[441/1000] train loss: 6.76719, val loss: 19.53972\n",
      "[442/1000] train loss: 6.68904, val loss: 19.45322\n",
      "[443/1000] train loss: 6.67352, val loss: 19.77830\n",
      "[444/1000] train loss: 6.65000, val loss: 19.56918\n",
      "[445/1000] train loss: 6.68788, val loss: 19.51771\n",
      "[446/1000] train loss: 6.62089, val loss: 19.43771\n",
      "[447/1000] train loss: 6.67991, val loss: 19.39404\n",
      "[448/1000] train loss: 6.63094, val loss: 19.37729\n",
      "[449/1000] train loss: 6.65150, val loss: 19.42646\n",
      "[450/1000] train loss: 6.66067, val loss: 19.58886\n",
      "[451/1000] train loss: 6.59522, val loss: 19.53298\n",
      "[452/1000] train loss: 6.64951, val loss: 19.36420\n",
      "[453/1000] train loss: 6.62371, val loss: 19.22954\n",
      "[454/1000] train loss: 6.67106, val loss: 19.47205\n",
      "[455/1000] train loss: 6.53524, val loss: 19.33268\n",
      "[456/1000] train loss: 6.61441, val loss: 19.16536\n",
      "[457/1000] train loss: 6.57780, val loss: 19.30242\n",
      "[458/1000] train loss: 6.52554, val loss: 19.17310\n",
      "[459/1000] train loss: 6.18849, val loss: 19.51669\n",
      "[460/1000] train loss: 6.58234, val loss: 19.44283\n",
      "[461/1000] train loss: 6.45329, val loss: 19.59193\n",
      "[462/1000] train loss: 6.51535, val loss: 19.30747\n",
      "[463/1000] train loss: 6.49609, val loss: 19.23093\n",
      "[464/1000] train loss: 6.52368, val loss: 19.19041\n",
      "[465/1000] train loss: 6.46662, val loss: 19.44311\n",
      "[466/1000] train loss: 6.51840, val loss: 19.24915\n",
      "[467/1000] train loss: 6.48865, val loss: 19.30582\n",
      "[468/1000] train loss: 6.57864, val loss: 19.64767\n",
      "[469/1000] train loss: 6.48076, val loss: 19.46966\n",
      "[470/1000] train loss: 6.48291, val loss: 19.28246\n",
      "[471/1000] train loss: 6.48136, val loss: 19.30641\n",
      "[472/1000] train loss: 6.51481, val loss: 19.31156\n",
      "[473/1000] train loss: 6.49025, val loss: 19.38817\n",
      "[474/1000] train loss: 6.43089, val loss: 19.35324\n",
      "[475/1000] train loss: 6.32154, val loss: 19.14684\n",
      "[476/1000] train loss: 6.44815, val loss: 19.06578\n",
      "[477/1000] train loss: 6.43782, val loss: 19.38461\n",
      "[478/1000] train loss: 6.41522, val loss: 19.06935\n",
      "[479/1000] train loss: 6.40786, val loss: 19.28513\n",
      "[480/1000] train loss: 6.41170, val loss: 18.96939\n",
      "[481/1000] train loss: 6.39416, val loss: 19.13498\n",
      "[482/1000] train loss: 6.41609, val loss: 19.08166\n",
      "[483/1000] train loss: 6.36028, val loss: 19.03915\n",
      "[484/1000] train loss: 6.29811, val loss: 19.16630\n",
      "[485/1000] train loss: 6.38236, val loss: 19.25828\n",
      "[486/1000] train loss: 6.30281, val loss: 19.21240\n",
      "[487/1000] train loss: 6.19753, val loss: 19.13707\n",
      "[488/1000] train loss: 6.35924, val loss: 19.14866\n",
      "[489/1000] train loss: 6.31556, val loss: 19.25537\n",
      "[490/1000] train loss: 6.35345, val loss: 19.23904\n",
      "[491/1000] train loss: 6.38409, val loss: 19.25571\n",
      "[492/1000] train loss: 6.29234, val loss: 19.10909\n",
      "[493/1000] train loss: 6.23661, val loss: 19.04719\n",
      "[494/1000] train loss: 6.11127, val loss: 19.01125\n",
      "[495/1000] train loss: 6.31826, val loss: 19.05908\n",
      "[496/1000] train loss: 6.17820, val loss: 19.07319\n",
      "[497/1000] train loss: 6.26734, val loss: 19.14851\n",
      "[498/1000] train loss: 6.31817, val loss: 18.91395\n",
      "[499/1000] train loss: 6.30793, val loss: 19.17114\n",
      "[500/1000] train loss: 6.28133, val loss: 18.69771\n",
      "[501/1000] train loss: 6.24413, val loss: 18.79174\n",
      "[502/1000] train loss: 6.18240, val loss: 18.72801\n",
      "[503/1000] train loss: 6.22033, val loss: 18.73559\n",
      "[504/1000] train loss: 6.16849, val loss: 18.81396\n",
      "[505/1000] train loss: 6.17681, val loss: 18.79388\n",
      "[506/1000] train loss: 6.17504, val loss: 18.81786\n",
      "[507/1000] train loss: 6.13789, val loss: 19.05464\n",
      "[508/1000] train loss: 6.17453, val loss: 18.76212\n",
      "[509/1000] train loss: 6.01887, val loss: 18.74762\n",
      "[510/1000] train loss: 6.07165, val loss: 18.93732\n",
      "[511/1000] train loss: 6.15057, val loss: 18.83058\n",
      "[512/1000] train loss: 6.04625, val loss: 18.72140\n",
      "[513/1000] train loss: 6.09071, val loss: 18.56072\n",
      "[514/1000] train loss: 6.04398, val loss: 18.73556\n",
      "[515/1000] train loss: 6.13745, val loss: 18.89410\n",
      "[516/1000] train loss: 5.99383, val loss: 18.83020\n",
      "[517/1000] train loss: 6.14453, val loss: 19.07492\n",
      "[518/1000] train loss: 5.73336, val loss: 18.84731\n",
      "[519/1000] train loss: 5.93926, val loss: 18.90130\n",
      "[520/1000] train loss: 6.04397, val loss: 18.50026\n",
      "[521/1000] train loss: 5.96995, val loss: 18.87834\n",
      "[522/1000] train loss: 5.99996, val loss: 18.79562\n",
      "[523/1000] train loss: 6.07389, val loss: 18.98294\n",
      "[524/1000] train loss: 6.03577, val loss: 18.65989\n",
      "[525/1000] train loss: 6.15874, val loss: 18.79989\n",
      "[526/1000] train loss: 6.11912, val loss: 18.52369\n",
      "[527/1000] train loss: 5.94151, val loss: 18.70560\n",
      "[528/1000] train loss: 5.81692, val loss: 18.69548\n",
      "[529/1000] train loss: 5.94464, val loss: 18.65697\n",
      "[530/1000] train loss: 5.99517, val loss: 18.76291\n",
      "[531/1000] train loss: 5.93256, val loss: 18.76052\n",
      "[532/1000] train loss: 5.95298, val loss: 18.62511\n",
      "[533/1000] train loss: 5.98980, val loss: 18.44343\n",
      "[534/1000] train loss: 5.85027, val loss: 18.72544\n",
      "[535/1000] train loss: 5.91907, val loss: 18.53121\n",
      "[536/1000] train loss: 5.96005, val loss: 18.74119\n",
      "[537/1000] train loss: 5.98989, val loss: 18.59821\n",
      "[538/1000] train loss: 5.95148, val loss: 18.64515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[539/1000] train loss: 5.87677, val loss: 18.62009\n",
      "[540/1000] train loss: 5.92120, val loss: 18.64030\n",
      "[541/1000] train loss: 5.58653, val loss: 18.84785\n",
      "[542/1000] train loss: 5.82917, val loss: 18.63009\n",
      "[543/1000] train loss: 5.80379, val loss: 18.73122\n",
      "[544/1000] train loss: 5.79611, val loss: 18.79180\n",
      "[545/1000] train loss: 5.80332, val loss: 18.60736\n",
      "[546/1000] train loss: 5.78727, val loss: 18.72855\n",
      "[547/1000] train loss: 5.80943, val loss: 18.45576\n",
      "[548/1000] train loss: 5.82752, val loss: 18.43685\n",
      "[549/1000] train loss: 5.76200, val loss: 18.58054\n",
      "[550/1000] train loss: 5.69265, val loss: 18.64350\n",
      "[551/1000] train loss: 5.44366, val loss: 18.65419\n",
      "[552/1000] train loss: 5.74092, val loss: 18.56622\n",
      "[553/1000] train loss: 5.81909, val loss: 18.50211\n",
      "[554/1000] train loss: 5.89867, val loss: 18.33023\n",
      "[555/1000] train loss: 5.79794, val loss: 18.45414\n",
      "[556/1000] train loss: 5.90812, val loss: 18.82593\n",
      "[557/1000] train loss: 5.76738, val loss: 18.68765\n",
      "[558/1000] train loss: 5.78983, val loss: 18.53084\n",
      "[559/1000] train loss: 5.80548, val loss: 18.68179\n",
      "[560/1000] train loss: 5.75840, val loss: 18.54592\n",
      "[561/1000] train loss: 5.73430, val loss: 18.57501\n",
      "[562/1000] train loss: 5.69395, val loss: 18.23940\n",
      "[563/1000] train loss: 5.84572, val loss: 18.64063\n",
      "[564/1000] train loss: 5.69724, val loss: 18.48219\n",
      "[565/1000] train loss: 5.75633, val loss: 18.30762\n",
      "[566/1000] train loss: 5.74690, val loss: 18.54694\n",
      "[567/1000] train loss: 5.70953, val loss: 18.60600\n",
      "[568/1000] train loss: 5.70292, val loss: 18.25219\n",
      "[569/1000] train loss: 5.68621, val loss: 18.48127\n",
      "[570/1000] train loss: 5.68447, val loss: 18.20570\n",
      "[571/1000] train loss: 5.75694, val loss: 18.21968\n",
      "[572/1000] train loss: 5.73328, val loss: 18.25580\n",
      "[573/1000] train loss: 5.61140, val loss: 18.42394\n",
      "[574/1000] train loss: 5.57376, val loss: 18.37617\n",
      "[575/1000] train loss: 5.69676, val loss: 18.30122\n",
      "[576/1000] train loss: 5.67091, val loss: 18.43646\n",
      "[577/1000] train loss: 5.65234, val loss: 18.47188\n",
      "[578/1000] train loss: 5.72865, val loss: 18.07321\n",
      "[579/1000] train loss: 5.61027, val loss: 18.53030\n",
      "[580/1000] train loss: 5.59163, val loss: 18.36886\n",
      "[581/1000] train loss: 5.60146, val loss: 18.39516\n",
      "[582/1000] train loss: 5.75833, val loss: 18.50217\n",
      "[583/1000] train loss: 5.77710, val loss: 18.15526\n",
      "[584/1000] train loss: 5.60665, val loss: 18.30994\n",
      "[585/1000] train loss: 5.62777, val loss: 18.23713\n",
      "[586/1000] train loss: 5.55069, val loss: 18.19283\n",
      "[587/1000] train loss: 5.64157, val loss: 18.46090\n",
      "[588/1000] train loss: 5.61352, val loss: 18.38951\n",
      "[589/1000] train loss: 5.53459, val loss: 18.25416\n",
      "[590/1000] train loss: 5.58541, val loss: 18.28917\n",
      "[591/1000] train loss: 5.60321, val loss: 18.36921\n",
      "[592/1000] train loss: 5.47177, val loss: 18.43038\n",
      "[593/1000] train loss: 5.67374, val loss: 18.57062\n",
      "[594/1000] train loss: 5.58123, val loss: 18.28361\n",
      "[595/1000] train loss: 5.53795, val loss: 18.34734\n",
      "[596/1000] train loss: 5.53615, val loss: 18.27664\n",
      "[597/1000] train loss: 5.54485, val loss: 18.22588\n",
      "[598/1000] train loss: 5.50822, val loss: 18.16627\n",
      "[599/1000] train loss: 5.55533, val loss: 18.08992\n",
      "[600/1000] train loss: 5.53333, val loss: 18.06320\n",
      "[601/1000] train loss: 5.50083, val loss: 18.02261\n",
      "[602/1000] train loss: 5.56221, val loss: 17.92973\n",
      "[603/1000] train loss: 5.44555, val loss: 18.16269\n",
      "[604/1000] train loss: 5.46071, val loss: 18.18912\n",
      "[605/1000] train loss: 5.46491, val loss: 18.21912\n",
      "[606/1000] train loss: 5.42876, val loss: 18.17229\n",
      "[607/1000] train loss: 5.51864, val loss: 17.98678\n",
      "[608/1000] train loss: 5.41803, val loss: 18.04298\n",
      "[609/1000] train loss: 5.46283, val loss: 18.24244\n",
      "[610/1000] train loss: 5.42144, val loss: 18.03875\n",
      "[611/1000] train loss: 5.54043, val loss: 17.94562\n",
      "[612/1000] train loss: 5.54288, val loss: 18.29216\n",
      "[613/1000] train loss: 5.51640, val loss: 17.99017\n",
      "[614/1000] train loss: 5.48892, val loss: 18.16389\n",
      "[615/1000] train loss: 5.43279, val loss: 18.18306\n",
      "[616/1000] train loss: 5.41689, val loss: 18.04863\n",
      "[617/1000] train loss: 5.47754, val loss: 18.00767\n",
      "[618/1000] train loss: 5.43101, val loss: 18.27341\n",
      "[619/1000] train loss: 5.49991, val loss: 18.38128\n",
      "[620/1000] train loss: 5.24582, val loss: 18.06964\n",
      "[621/1000] train loss: 5.50066, val loss: 18.00127\n",
      "[622/1000] train loss: 5.41969, val loss: 17.98110\n",
      "[623/1000] train loss: 5.41185, val loss: 18.19435\n",
      "[624/1000] train loss: 5.41443, val loss: 18.00977\n",
      "[625/1000] train loss: 5.41846, val loss: 18.23511\n",
      "[626/1000] train loss: 5.44167, val loss: 17.82363\n",
      "[627/1000] train loss: 5.45010, val loss: 17.84623\n",
      "[628/1000] train loss: 5.38584, val loss: 18.07594\n",
      "[629/1000] train loss: 5.29906, val loss: 17.96131\n",
      "[630/1000] train loss: 5.37275, val loss: 17.92315\n",
      "[631/1000] train loss: 5.37177, val loss: 18.13328\n",
      "[632/1000] train loss: 5.45305, val loss: 18.25117\n",
      "[633/1000] train loss: 5.34387, val loss: 18.01389\n",
      "[634/1000] train loss: 5.30982, val loss: 17.92278\n",
      "[635/1000] train loss: 5.28205, val loss: 17.93253\n",
      "[636/1000] train loss: 5.30437, val loss: 17.98380\n",
      "[637/1000] train loss: 5.14297, val loss: 17.94767\n",
      "[638/1000] train loss: 5.33805, val loss: 18.25078\n",
      "[639/1000] train loss: 5.34973, val loss: 18.16250\n",
      "[640/1000] train loss: 5.26908, val loss: 17.90148\n",
      "[641/1000] train loss: 5.27317, val loss: 17.96685\n",
      "[642/1000] train loss: 5.31532, val loss: 17.90070\n",
      "[643/1000] train loss: 5.30160, val loss: 18.02154\n",
      "[644/1000] train loss: 5.35894, val loss: 18.05422\n",
      "[645/1000] train loss: 5.30263, val loss: 18.07302\n",
      "[646/1000] train loss: 5.26673, val loss: 17.91038\n",
      "[647/1000] train loss: 5.32628, val loss: 18.22505\n",
      "[648/1000] train loss: 5.30694, val loss: 17.86525\n",
      "[649/1000] train loss: 5.30474, val loss: 17.97499\n",
      "[650/1000] train loss: 5.17368, val loss: 17.84778\n",
      "[651/1000] train loss: 5.21672, val loss: 18.15296\n",
      "[652/1000] train loss: 5.32518, val loss: 18.15705\n",
      "[653/1000] train loss: 5.19082, val loss: 18.20649\n",
      "[654/1000] train loss: 5.22843, val loss: 17.86223\n",
      "[655/1000] train loss: 5.23859, val loss: 17.83887\n",
      "[656/1000] train loss: 5.26582, val loss: 17.90664\n",
      "[657/1000] train loss: 5.21724, val loss: 18.15151\n",
      "[658/1000] train loss: 5.20176, val loss: 17.98979\n",
      "[659/1000] train loss: 5.22956, val loss: 17.69622\n",
      "[660/1000] train loss: 5.21859, val loss: 17.80055\n",
      "[661/1000] train loss: 5.21023, val loss: 18.06671\n",
      "[662/1000] train loss: 5.19602, val loss: 17.99282\n",
      "[663/1000] train loss: 5.15620, val loss: 17.78642\n",
      "[664/1000] train loss: 5.18661, val loss: 17.85160\n",
      "[665/1000] train loss: 5.19643, val loss: 18.19052\n",
      "[666/1000] train loss: 5.37053, val loss: 18.18777\n",
      "[667/1000] train loss: 5.24583, val loss: 17.86997\n",
      "[668/1000] train loss: 5.13088, val loss: 17.77338\n",
      "[669/1000] train loss: 5.17771, val loss: 17.80004\n",
      "[670/1000] train loss: 5.08516, val loss: 17.97489\n",
      "[671/1000] train loss: 5.24016, val loss: 18.14329\n",
      "[672/1000] train loss: 5.24613, val loss: 18.17453\n",
      "[673/1000] train loss: 5.11863, val loss: 17.91268\n",
      "[674/1000] train loss: 5.14705, val loss: 18.11478\n",
      "[675/1000] train loss: 4.80755, val loss: 18.12479\n",
      "[676/1000] train loss: 5.06695, val loss: 17.85341\n",
      "[677/1000] train loss: 5.10593, val loss: 17.83432\n",
      "[678/1000] train loss: 5.11817, val loss: 17.77122\n",
      "[679/1000] train loss: 5.13732, val loss: 17.88854\n",
      "[680/1000] train loss: 5.12019, val loss: 17.84969\n",
      "[681/1000] train loss: 5.11592, val loss: 17.88625\n",
      "[682/1000] train loss: 5.01370, val loss: 17.82626\n",
      "[683/1000] train loss: 5.05537, val loss: 17.68132\n",
      "[684/1000] train loss: 5.08472, val loss: 18.01386\n",
      "[685/1000] train loss: 5.08636, val loss: 17.91498\n",
      "[686/1000] train loss: 5.01023, val loss: 17.90287\n",
      "[687/1000] train loss: 5.05623, val loss: 17.69408\n",
      "[688/1000] train loss: 5.06735, val loss: 17.86787\n",
      "[689/1000] train loss: 5.08040, val loss: 17.74174\n",
      "[690/1000] train loss: 5.07576, val loss: 17.94609\n",
      "[691/1000] train loss: 5.15728, val loss: 17.67277\n",
      "[692/1000] train loss: 4.99753, val loss: 17.90091\n",
      "[693/1000] train loss: 5.07293, val loss: 17.79246\n",
      "[694/1000] train loss: 5.10173, val loss: 17.63481\n",
      "[695/1000] train loss: 4.95682, val loss: 17.72771\n",
      "[696/1000] train loss: 5.02899, val loss: 17.62552\n",
      "[697/1000] train loss: 4.98485, val loss: 17.77989\n",
      "[698/1000] train loss: 4.91448, val loss: 17.92190\n",
      "[699/1000] train loss: 4.92826, val loss: 17.61358\n",
      "[700/1000] train loss: 5.04941, val loss: 17.86889\n",
      "[701/1000] train loss: 5.00443, val loss: 17.62248\n",
      "[702/1000] train loss: 4.99873, val loss: 17.75734\n",
      "[703/1000] train loss: 5.04836, val loss: 17.93784\n",
      "[704/1000] train loss: 4.98641, val loss: 17.94217\n",
      "[705/1000] train loss: 4.98358, val loss: 17.65630\n",
      "[706/1000] train loss: 4.94756, val loss: 17.82416\n",
      "[707/1000] train loss: 4.98278, val loss: 17.79272\n",
      "[708/1000] train loss: 5.01445, val loss: 17.79188\n",
      "[709/1000] train loss: 4.99022, val loss: 17.77203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[710/1000] train loss: 4.93367, val loss: 17.54746\n",
      "[711/1000] train loss: 4.95557, val loss: 17.91472\n",
      "[712/1000] train loss: 4.82547, val loss: 17.86888\n",
      "[713/1000] train loss: 4.88253, val loss: 17.55740\n",
      "[714/1000] train loss: 4.95272, val loss: 17.84825\n",
      "[715/1000] train loss: 5.03406, val loss: 17.60475\n",
      "[716/1000] train loss: 4.97201, val loss: 17.72867\n",
      "[717/1000] train loss: 4.92281, val loss: 17.84910\n",
      "[718/1000] train loss: 4.93288, val loss: 17.90830\n",
      "[719/1000] train loss: 4.98112, val loss: 17.80557\n",
      "[720/1000] train loss: 4.92837, val loss: 17.80976\n",
      "[721/1000] train loss: 5.02506, val loss: 18.15514\n",
      "[722/1000] train loss: 4.99658, val loss: 17.48354\n",
      "[723/1000] train loss: 5.00025, val loss: 17.58223\n",
      "[724/1000] train loss: 4.98544, val loss: 17.52023\n",
      "[725/1000] train loss: 4.87955, val loss: 17.66625\n",
      "[726/1000] train loss: 4.80672, val loss: 17.69999\n",
      "[727/1000] train loss: 4.89564, val loss: 17.67380\n",
      "[728/1000] train loss: 4.91108, val loss: 17.89026\n",
      "[729/1000] train loss: 4.87545, val loss: 17.65607\n",
      "[730/1000] train loss: 4.82215, val loss: 17.76332\n",
      "[731/1000] train loss: 4.86589, val loss: 17.78528\n",
      "[732/1000] train loss: 4.93674, val loss: 17.61440\n",
      "[733/1000] train loss: 4.89198, val loss: 17.48914\n",
      "[734/1000] train loss: 4.92297, val loss: 17.94012\n",
      "[735/1000] train loss: 4.84515, val loss: 17.81979\n",
      "[736/1000] train loss: 4.83260, val loss: 17.82316\n",
      "[737/1000] train loss: 4.83930, val loss: 17.80915\n",
      "[738/1000] train loss: 4.85810, val loss: 17.79885\n",
      "[739/1000] train loss: 4.84959, val loss: 17.70741\n",
      "[740/1000] train loss: 4.80120, val loss: 17.59663\n",
      "[741/1000] train loss: 4.83816, val loss: 17.69639\n",
      "[742/1000] train loss: 4.89915, val loss: 17.92085\n",
      "[743/1000] train loss: 4.83493, val loss: 17.62638\n",
      "[744/1000] train loss: 4.78751, val loss: 17.78695\n",
      "[745/1000] train loss: 4.81083, val loss: 17.68205\n",
      "[746/1000] train loss: 4.78127, val loss: 17.68538\n",
      "[747/1000] train loss: 4.80092, val loss: 17.46751\n",
      "[748/1000] train loss: 4.88378, val loss: 17.70602\n",
      "[749/1000] train loss: 4.81315, val loss: 17.87695\n",
      "[750/1000] train loss: 4.77725, val loss: 18.03201\n",
      "[751/1000] train loss: 4.83206, val loss: 17.60703\n",
      "[752/1000] train loss: 4.80567, val loss: 17.62939\n",
      "[753/1000] train loss: 4.80614, val loss: 17.88754\n",
      "[754/1000] train loss: 4.84336, val loss: 17.88457\n",
      "[755/1000] train loss: 4.76941, val loss: 17.70149\n",
      "[756/1000] train loss: 4.95352, val loss: 17.56896\n",
      "[757/1000] train loss: 4.83481, val loss: 17.62230\n",
      "[758/1000] train loss: 4.71708, val loss: 17.77273\n",
      "[759/1000] train loss: 4.72514, val loss: 17.73138\n",
      "[760/1000] train loss: 4.74121, val loss: 17.80836\n",
      "[761/1000] train loss: 4.73363, val loss: 17.69826\n",
      "[762/1000] train loss: 4.65590, val loss: 17.71832\n",
      "[763/1000] train loss: 4.73701, val loss: 17.56152\n",
      "[764/1000] train loss: 4.71358, val loss: 17.63234\n",
      "[765/1000] train loss: 4.73151, val loss: 17.78262\n",
      "[766/1000] train loss: 4.66236, val loss: 17.70266\n",
      "[767/1000] train loss: 4.81211, val loss: 17.40523\n",
      "[768/1000] train loss: 4.79122, val loss: 17.52001\n",
      "[769/1000] train loss: 4.71927, val loss: 17.79243\n",
      "[770/1000] train loss: 4.65496, val loss: 17.64997\n",
      "[771/1000] train loss: 4.73250, val loss: 17.64743\n",
      "[772/1000] train loss: 4.69519, val loss: 17.70371\n",
      "[773/1000] train loss: 4.71860, val loss: 17.79539\n",
      "[774/1000] train loss: 4.67921, val loss: 17.50995\n",
      "[775/1000] train loss: 4.65168, val loss: 17.68614\n",
      "[776/1000] train loss: 4.67261, val loss: 17.82836\n",
      "[777/1000] train loss: 4.72058, val loss: 17.94631\n",
      "[778/1000] train loss: 4.65010, val loss: 17.86185\n",
      "[779/1000] train loss: 4.66869, val loss: 17.62220\n",
      "[780/1000] train loss: 4.58634, val loss: 17.67006\n",
      "[781/1000] train loss: 4.71165, val loss: 17.46535\n",
      "[782/1000] train loss: 4.64286, val loss: 17.54884\n",
      "[783/1000] train loss: 4.62839, val loss: 17.66105\n",
      "[784/1000] train loss: 4.70628, val loss: 17.84343\n",
      "[785/1000] train loss: 4.62091, val loss: 17.78068\n",
      "[786/1000] train loss: 4.65456, val loss: 17.65153\n",
      "[787/1000] train loss: 4.48076, val loss: 17.56242\n",
      "[788/1000] train loss: 4.67885, val loss: 17.75738\n",
      "[789/1000] train loss: 4.60827, val loss: 17.81047\n",
      "[790/1000] train loss: 4.64711, val loss: 17.73442\n",
      "[791/1000] train loss: 4.64108, val loss: 17.53100\n",
      "[792/1000] train loss: 4.66791, val loss: 17.76111\n",
      "[793/1000] train loss: 4.66357, val loss: 17.70878\n",
      "[794/1000] train loss: 4.63239, val loss: 17.53627\n",
      "[795/1000] train loss: 4.66269, val loss: 17.85027\n",
      "[796/1000] train loss: 4.66921, val loss: 17.51970\n",
      "[797/1000] train loss: 4.64253, val loss: 17.67467\n",
      "[798/1000] train loss: 4.58386, val loss: 17.68568\n",
      "[799/1000] train loss: 4.44295, val loss: 17.79232\n",
      "[800/1000] train loss: 4.55233, val loss: 17.81133\n",
      "[801/1000] train loss: 4.42890, val loss: 17.84349\n",
      "[802/1000] train loss: 4.61964, val loss: 17.48864\n",
      "[803/1000] train loss: 4.60724, val loss: 17.82631\n",
      "[804/1000] train loss: 4.62820, val loss: 17.75220\n",
      "[805/1000] train loss: 4.62202, val loss: 17.70524\n",
      "[806/1000] train loss: 4.54754, val loss: 17.65514\n",
      "[807/1000] train loss: 4.63956, val loss: 17.50342\n",
      "[808/1000] train loss: 4.60069, val loss: 17.84945\n",
      "[809/1000] train loss: 4.60844, val loss: 17.49821\n",
      "[810/1000] train loss: 4.65997, val loss: 17.74426\n",
      "[811/1000] train loss: 4.54660, val loss: 17.59329\n",
      "[812/1000] train loss: 4.59782, val loss: 17.56271\n",
      "[813/1000] train loss: 4.60455, val loss: 17.55295\n",
      "[814/1000] train loss: 4.60113, val loss: 17.50947\n",
      "[815/1000] train loss: 4.60879, val loss: 17.80885\n",
      "[816/1000] train loss: 4.55947, val loss: 17.63306\n",
      "[817/1000] train loss: 4.36170, val loss: 17.75795\n",
      "[818/1000] train loss: 4.51820, val loss: 17.77651\n",
      "[819/1000] train loss: 4.48338, val loss: 17.68606\n",
      "[820/1000] train loss: 4.61932, val loss: 17.74327\n",
      "[821/1000] train loss: 4.56315, val loss: 17.69482\n",
      "[822/1000] train loss: 4.61104, val loss: 17.39098\n",
      "[823/1000] train loss: 4.56363, val loss: 17.82313\n",
      "[824/1000] train loss: 4.55921, val loss: 17.70696\n",
      "[825/1000] train loss: 4.54999, val loss: 17.55684\n",
      "[826/1000] train loss: 4.47698, val loss: 17.60564\n",
      "[827/1000] train loss: 4.52692, val loss: 17.70858\n",
      "[828/1000] train loss: 4.49216, val loss: 17.51486\n",
      "[829/1000] train loss: 4.60009, val loss: 17.40115\n",
      "[830/1000] train loss: 4.51128, val loss: 17.89579\n",
      "[831/1000] train loss: 4.49387, val loss: 17.44302\n",
      "[832/1000] train loss: 4.57470, val loss: 17.89956\n",
      "[833/1000] train loss: 4.51737, val loss: 17.36453\n",
      "[834/1000] train loss: 4.48506, val loss: 17.52345\n",
      "[835/1000] train loss: 4.41646, val loss: 17.53775\n",
      "[836/1000] train loss: 4.52305, val loss: 17.38843\n",
      "[837/1000] train loss: 4.45703, val loss: 17.59238\n",
      "[838/1000] train loss: 4.48203, val loss: 17.49628\n",
      "[839/1000] train loss: 4.49058, val loss: 17.56834\n",
      "[840/1000] train loss: 4.50168, val loss: 17.68667\n",
      "[841/1000] train loss: 4.51033, val loss: 17.37983\n",
      "[842/1000] train loss: 4.42695, val loss: 17.54416\n",
      "[843/1000] train loss: 4.46701, val loss: 17.72339\n",
      "[844/1000] train loss: 4.53360, val loss: 17.81060\n",
      "[845/1000] train loss: 4.45864, val loss: 17.42581\n",
      "[846/1000] train loss: 4.48461, val loss: 17.45023\n",
      "[847/1000] train loss: 4.52690, val loss: 17.94336\n",
      "[848/1000] train loss: 4.53263, val loss: 17.47811\n",
      "[849/1000] train loss: 4.42339, val loss: 17.27651\n",
      "[850/1000] train loss: 4.43722, val loss: 17.33469\n",
      "[851/1000] train loss: 4.50964, val loss: 17.41430\n",
      "[852/1000] train loss: 4.49311, val loss: 17.27442\n",
      "[853/1000] train loss: 4.38960, val loss: 17.64108\n",
      "[854/1000] train loss: 4.41898, val loss: 17.32981\n",
      "[855/1000] train loss: 4.46272, val loss: 17.31522\n",
      "[856/1000] train loss: 4.34495, val loss: 17.45226\n",
      "[857/1000] train loss: 4.36311, val loss: 17.43002\n",
      "[858/1000] train loss: 4.42484, val loss: 17.53046\n",
      "[859/1000] train loss: 4.44638, val loss: 17.38484\n",
      "[860/1000] train loss: 4.47641, val loss: 17.32433\n",
      "[861/1000] train loss: 4.45369, val loss: 17.75296\n",
      "[862/1000] train loss: 4.34855, val loss: 17.40249\n",
      "[863/1000] train loss: 4.43777, val loss: 17.38478\n",
      "[864/1000] train loss: 4.32405, val loss: 17.34369\n",
      "[865/1000] train loss: 4.35579, val loss: 17.56507\n",
      "[866/1000] train loss: 4.37021, val loss: 17.47236\n",
      "[867/1000] train loss: 4.39689, val loss: 17.34957\n",
      "[868/1000] train loss: 4.43813, val loss: 17.63183\n",
      "[869/1000] train loss: 4.41302, val loss: 17.45322\n",
      "[870/1000] train loss: 4.41509, val loss: 17.74134\n",
      "[871/1000] train loss: 4.43190, val loss: 17.83105\n",
      "[872/1000] train loss: 4.46603, val loss: 17.50732\n",
      "[873/1000] train loss: 4.35433, val loss: 17.32966\n",
      "[874/1000] train loss: 4.55901, val loss: 18.03333\n",
      "[875/1000] train loss: 4.50817, val loss: 17.70666\n",
      "[876/1000] train loss: 4.36448, val loss: 17.33658\n",
      "[877/1000] train loss: 4.37500, val loss: 17.46766\n",
      "[878/1000] train loss: 4.38032, val loss: 17.42952\n",
      "[879/1000] train loss: 4.37246, val loss: 17.30589\n",
      "[880/1000] train loss: 4.40628, val loss: 17.23524\n",
      "[881/1000] train loss: 4.36530, val loss: 17.58831\n",
      "[882/1000] train loss: 4.32307, val loss: 17.39831\n",
      "[883/1000] train loss: 4.35887, val loss: 17.53236\n",
      "[884/1000] train loss: 4.32846, val loss: 17.56370\n",
      "[885/1000] train loss: 4.28947, val loss: 17.47880\n",
      "[886/1000] train loss: 4.49070, val loss: 17.94684\n",
      "[887/1000] train loss: 4.34696, val loss: 17.50569\n",
      "[888/1000] train loss: 4.34121, val loss: 17.64598\n",
      "[889/1000] train loss: 4.31787, val loss: 17.47826\n",
      "[890/1000] train loss: 4.31914, val loss: 17.63145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[891/1000] train loss: 4.38095, val loss: 17.55646\n",
      "[892/1000] train loss: 4.34498, val loss: 17.37203\n",
      "[893/1000] train loss: 4.32068, val loss: 17.44959\n",
      "[894/1000] train loss: 4.34863, val loss: 17.59552\n",
      "[895/1000] train loss: 4.37120, val loss: 17.32354\n",
      "[896/1000] train loss: 4.33419, val loss: 17.32356\n",
      "[897/1000] train loss: 4.37318, val loss: 17.46570\n",
      "[898/1000] train loss: 4.26696, val loss: 17.63577\n",
      "[899/1000] train loss: 4.25540, val loss: 17.56383\n",
      "[900/1000] train loss: 4.30632, val loss: 17.27955\n",
      "[901/1000] train loss: 4.40951, val loss: 17.46954\n",
      "[902/1000] train loss: 4.31255, val loss: 17.55950\n",
      "[903/1000] train loss: 4.29405, val loss: 17.42506\n",
      "[904/1000] train loss: 4.30637, val loss: 17.58564\n",
      "[905/1000] train loss: 4.30498, val loss: 17.72968\n",
      "[906/1000] train loss: 4.36319, val loss: 17.61466\n",
      "[907/1000] train loss: 4.18989, val loss: 17.49767\n",
      "[908/1000] train loss: 4.27131, val loss: 17.35843\n",
      "[909/1000] train loss: 4.26067, val loss: 17.62847\n",
      "[910/1000] train loss: 4.14274, val loss: 17.47519\n",
      "[911/1000] train loss: 4.26328, val loss: 17.48304\n",
      "[912/1000] train loss: 4.26785, val loss: 17.52383\n",
      "[913/1000] train loss: 4.23263, val loss: 17.36261\n",
      "[914/1000] train loss: 4.26678, val loss: 17.44504\n",
      "[915/1000] train loss: 4.32384, val loss: 17.23163\n",
      "[916/1000] train loss: 4.31511, val loss: 17.25601\n",
      "[917/1000] train loss: 4.23287, val loss: 17.58570\n",
      "[918/1000] train loss: 4.30357, val loss: 17.45276\n",
      "[919/1000] train loss: 4.25266, val loss: 17.67891\n",
      "[920/1000] train loss: 4.22881, val loss: 17.42930\n",
      "[921/1000] train loss: 4.23090, val loss: 17.54314\n",
      "[922/1000] train loss: 4.26686, val loss: 17.44409\n",
      "[923/1000] train loss: 4.16093, val loss: 17.60541\n",
      "[924/1000] train loss: 4.24906, val loss: 17.63344\n",
      "[925/1000] train loss: 4.22050, val loss: 17.47974\n",
      "[926/1000] train loss: 4.23851, val loss: 17.58315\n",
      "[927/1000] train loss: 4.25506, val loss: 17.50917\n",
      "[928/1000] train loss: 4.21613, val loss: 17.56909\n",
      "[929/1000] train loss: 4.24111, val loss: 17.64659\n",
      "[930/1000] train loss: 4.24134, val loss: 17.33498\n",
      "[931/1000] train loss: 4.16843, val loss: 17.58594\n",
      "[932/1000] train loss: 4.22206, val loss: 17.62230\n",
      "[933/1000] train loss: 4.07672, val loss: 17.75270\n",
      "[934/1000] train loss: 4.19473, val loss: 17.65439\n",
      "[935/1000] train loss: 4.16435, val loss: 17.47747\n",
      "[936/1000] train loss: 4.16620, val loss: 17.38185\n",
      "[937/1000] train loss: 4.18810, val loss: 17.51024\n",
      "[938/1000] train loss: 4.21878, val loss: 17.65056\n",
      "[939/1000] train loss: 4.20249, val loss: 17.62008\n",
      "[940/1000] train loss: 4.19021, val loss: 17.36616\n",
      "[941/1000] train loss: 4.21669, val loss: 17.53196\n",
      "[942/1000] train loss: 4.20909, val loss: 17.71582\n",
      "[943/1000] train loss: 3.92111, val loss: 17.37554\n",
      "[944/1000] train loss: 4.20404, val loss: 17.75006\n",
      "[945/1000] train loss: 4.17538, val loss: 17.37386\n",
      "[946/1000] train loss: 4.17896, val loss: 17.61915\n",
      "[947/1000] train loss: 4.17579, val loss: 17.57938\n",
      "[948/1000] train loss: 4.28764, val loss: 17.73140\n",
      "[949/1000] train loss: 4.13012, val loss: 17.44086\n",
      "[950/1000] train loss: 4.21465, val loss: 17.31596\n",
      "[951/1000] train loss: 4.12219, val loss: 17.67963\n",
      "[952/1000] train loss: 4.16546, val loss: 17.45985\n",
      "[953/1000] train loss: 4.16432, val loss: 17.48217\n",
      "[954/1000] train loss: 4.31490, val loss: 17.35503\n",
      "[955/1000] train loss: 4.24643, val loss: 17.47541\n",
      "[956/1000] train loss: 4.23229, val loss: 17.92266\n",
      "[957/1000] train loss: 4.30001, val loss: 17.92135\n",
      "[958/1000] train loss: 4.15265, val loss: 17.40347\n",
      "[959/1000] train loss: 4.15189, val loss: 17.70381\n",
      "[960/1000] train loss: 4.09041, val loss: 17.79477\n",
      "[961/1000] train loss: 4.11718, val loss: 17.50306\n",
      "[962/1000] train loss: 4.21023, val loss: 17.50095\n",
      "[963/1000] train loss: 4.12708, val loss: 17.54913\n",
      "[964/1000] train loss: 4.14637, val loss: 17.75936\n",
      "[965/1000] train loss: 4.11804, val loss: 17.43821\n",
      "[966/1000] train loss: 4.13803, val loss: 17.51643\n",
      "[967/1000] train loss: 4.10226, val loss: 17.65303\n",
      "[968/1000] train loss: 4.11778, val loss: 17.50975\n",
      "[969/1000] train loss: 4.16378, val loss: 17.43848\n",
      "[970/1000] train loss: 4.13607, val loss: 17.47786\n",
      "[971/1000] train loss: 4.18795, val loss: 17.89352\n",
      "[972/1000] train loss: 4.13352, val loss: 17.59750\n",
      "[973/1000] train loss: 4.20084, val loss: 17.90506\n",
      "[974/1000] train loss: 4.07737, val loss: 17.73431\n",
      "[975/1000] train loss: 4.04271, val loss: 17.51854\n",
      "[976/1000] train loss: 4.12077, val loss: 17.73626\n",
      "[977/1000] train loss: 4.19350, val loss: 17.69159\n",
      "[978/1000] train loss: 4.11701, val loss: 17.36790\n",
      "[979/1000] train loss: 4.10097, val loss: 17.55414\n",
      "[980/1000] train loss: 4.11049, val loss: 17.45720\n",
      "[981/1000] train loss: 4.04272, val loss: 17.82932\n",
      "[982/1000] train loss: 4.13341, val loss: 17.74652\n",
      "[983/1000] train loss: 4.11364, val loss: 17.53071\n",
      "[984/1000] train loss: 4.07205, val loss: 17.62978\n",
      "[985/1000] train loss: 4.05887, val loss: 17.76987\n",
      "[986/1000] train loss: 4.14707, val loss: 17.92488\n",
      "[987/1000] train loss: 4.06845, val loss: 17.53437\n",
      "[988/1000] train loss: 4.07908, val loss: 17.69878\n",
      "[989/1000] train loss: 4.05757, val loss: 17.60165\n",
      "[990/1000] train loss: 4.14090, val loss: 17.50333\n",
      "[991/1000] train loss: 4.11000, val loss: 17.61003\n",
      "[992/1000] train loss: 4.07905, val loss: 17.78813\n",
      "[993/1000] train loss: 4.12483, val loss: 17.68288\n",
      "[994/1000] train loss: 4.12884, val loss: 17.83666\n",
      "[995/1000] train loss: 4.08103, val loss: 17.38956\n",
      "[996/1000] train loss: 4.06160, val loss: 17.61989\n",
      "[997/1000] train loss: 4.05050, val loss: 17.54354\n",
      "[998/1000] train loss: 4.09376, val loss: 17.51266\n",
      "[999/1000] train loss: 4.06640, val loss: 17.89757\n",
      "[1000/1000] train loss: 4.02671, val loss: 17.74675\n"
     ]
    }
   ],
   "source": [
    "# 학습: 두단계 - 학습 + 검증\n",
    "for epoch in range(N_EPOCH):\n",
    "    ################\n",
    "    # 학습 - 모델을 train 모드로 변경\n",
    "    ################\n",
    "    boston_model.train()\n",
    "    train_loss = 0.0\n",
    "    for X, y in boston_train_loader:\n",
    "        # X, y를 device로 이동\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # 1. 모델 추정\n",
    "        pred = boston_model(X) # 순전파(forward propagation)\n",
    "        # 2. loss 계산\n",
    "        loss = loss_fn(pred, y)  # 추정, 정답\n",
    "        # 3. 모델 파라미터를 업데이트\n",
    "        ## 3.1 파라미터들의 기울기를 초기화\n",
    "        optimizer.zero_grad()\n",
    "        ## 3.2 역전파(back propagration)을 해서 파라미터들의 기울기를 계산(grad속성에 저장)\n",
    "        loss.backward()\n",
    "        ## 3.3 파라미터 업데이트 처리.=> 1 step\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    #평균 loss\n",
    "    train_loss /= len(boston_train_loader)\n",
    "    # 1 epoch 학습끝\n",
    "    ###############################\n",
    "    # 검증 - 모델을 평가모드로 변경\n",
    "    ###############################\n",
    "    boston_model.eval() #evalutation mode 로 변환\n",
    "    val_loss = 0.0\n",
    "    # 역전파를 통한 gradient 계산이 필요 없기 때문에 일시적으로 grad_fn 을 구하지 않도록 처리.\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in boston_test_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            # 1.추정\n",
    "            pred_val = boston_model(X_val)\n",
    "            # 2. loss 계산\n",
    "            val_loss += loss_fn(pred_val, y_val).item()\n",
    "        val_loss /= len(boston_test_loader)\n",
    "    # epoch에 대한 검증 완료\n",
    "    # 결과 출력\n",
    "    print(f\"[{epoch+1}/{N_EPOCH}] train loss: {train_loss:.5f}, val loss: {val_loss:.5f}\")\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(N_EPOCH), train_loss_list, label=\"train loss\")\n",
    "plt.plot(range(N_EPOCH), val_loss_list, label=\"validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel('Loss')\n",
    "# plt.ylim(3, 30)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장\n",
    "\n",
    "## 모델 전체 저장 및 불러오기\n",
    "- 모델구조, 파라미터 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'models/boston_model.pt'\n",
    "torch.save(boston_model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_boston_model_1 = torch.load(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BostonModel                              [200, 1]                  --\n",
       "├─Linear: 1-1                            [200, 32]                 448\n",
       "├─Linear: 1-2                            [200, 16]                 528\n",
       "├─Linear: 1-3                            [200, 1]                  17\n",
       "==========================================================================================\n",
       "Total params: 993\n",
       "Trainable params: 993\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.20\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.08\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(load_boston_model_1, (200, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = 0.0\n",
    "load_boston_model_1.to(device)\n",
    "load_boston_model_1.eval()\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in boston_test_loader:\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "        # 1.추정\n",
    "        pred_val = load_boston_model_1(X_val)\n",
    "        # 2. loss 계산\n",
    "        val_loss += loss_fn(pred_val, y_val).item()\n",
    "    val_loss /= len(boston_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.74675178527832"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## state_dict 저장 및 로딩\n",
    "- 모델 파라미터만 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path2 = \"models/boston_state_dict.pt\"\n",
    "model_sd = boston_model.state_dict()\n",
    "\n",
    "torch.save(model_sd, save_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#state_dict 를 로딩 \n",
    "## 1. 모델객체 생성\n",
    "load_boston_model_2 = BostonModel().to(device)\n",
    "## 2. state_dict 불러오기\n",
    "load_sd = torch.load(save_path2)\n",
    "## 3. 불러온 state_dict(파라미터들)을 모델에 덮어 씌우기\n",
    "load_boston_model_2.load_state_dict(load_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.74675178527832"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss = 0.0\n",
    "load_boston_model_2.to(device)\n",
    "load_boston_model_2.eval() #평가모드 변환\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in boston_test_loader:\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "        # 1.추정\n",
    "        pred_val = load_boston_model_2(X_val)\n",
    "        # 2. loss 계산\n",
    "        val_loss += loss_fn(pred_val, y_val).item()\n",
    "    val_loss /= len(boston_test_loader)\n",
    "    \n",
    "val_loss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 분류 (Classification)\n",
    "\n",
    "## Fashion MNIST Dataset - 다중분류(Multi-Class Classification) 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "10개의 범주(category)와 70,000개의 흑백 이미지로 구성된 [패션 MNIST](https://github.com/zalandoresearch/fashion-mnist) 데이터셋. \n",
    "이미지는 해상도(28x28 픽셀)가 낮고 다음처럼 개별 의류 품목을 나타낸다:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>그림</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">패션-MNIST 샘플</a> (Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "이미지는 28x28 크기이며 Gray scale이다. *레이블*(label)은 0에서 9까지의 정수 배열이다. 아래 표는 이미지에 있는 의류의 **클래스**(class)들이다.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>레이블</th>\n",
    "    <th>클래스</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>T-shirt/top</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Trousers</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>2</td>\n",
    "    <td>Pullover</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>3</td>\n",
    "    <td>Dress</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>4</td>\n",
    "    <td>Coat</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>5</td>\n",
    "    <td>Sandal</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>6</td>\n",
    "    <td>Shirt</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>7</td>\n",
    "    <td>Sneaker</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>8</td>\n",
    "    <td>Bag</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>9</td>\n",
    "    <td>Ankle boot</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Dress', 'Pullover', 'Sandal'], dtype='<U11'), 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_class = np.array(['T-shirt/top', 'Trousers', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'])\n",
    "class_to_index = {key:value for value, key in enumerate(index_to_class)}\n",
    "\n",
    "index_to_class[[3, 2, 5]], class_to_index['Pullover']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # channel first 처리. 0 ~ 1 scaling, torch.Tensor 변환\n",
    "#     transforms.Normalize(mean=0.5, std=0.5)  # 표준화((pixcel-mean)/std). (-1 ~ 1)\n",
    "])\n",
    "# Dataset loading\n",
    "fmnist_trainset = datasets.FashionMNIST(root=\"datasets\", train=True, \n",
    "                                        download=True, transform=transform)\n",
    "fmnist_testset = datasets.FashionMNIST(root=\"datasets\", train=False,\n",
    "                                       download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: datasets\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=0.5, std=0.5)\n",
      "           )\n",
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: datasets\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=0.5, std=0.5)\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(fmnist_trainset)\n",
    "print(fmnist_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fmnist_trainset), len(fmnist_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_trainset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T-shirt/top': 0,\n",
       " 'Trouser': 1,\n",
       " 'Pullover': 2,\n",
       " 'Dress': 3,\n",
       " 'Coat': 4,\n",
       " 'Sandal': 5,\n",
       " 'Shirt': 6,\n",
       " 'Sneaker': 7,\n",
       " 'Bag': 8,\n",
       " 'Ankle boot': 9}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_trainset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Ankle boot\n",
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "x, y = fmnist_trainset[0]\n",
    "print(y, index_to_class[y])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DataLoader 생성\n",
    "fmnist_train_loader = DataLoader(fmnist_trainset, batch_size=128, \n",
    "                                 shuffle=True, drop_last=True)\n",
    "fmnist_test_loader = DataLoader(fmnist_testset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(28*28, 2048)\n",
    "        self.lr2 = nn.Linear(2048, 1024)\n",
    "        self.lr3 = nn.Linear(1024, 512)\n",
    "        self.lr4 = nn.Linear(512, 256)\n",
    "        self.lr5 = nn.Linear(256, 128)\n",
    "        self.lr6 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 10) #out_feature: 10 - 10개 class별 확률.\n",
    "    def forward(self, X):\n",
    "        out = nn.Flatten()(X)\n",
    "        out = nn.ReLU()(self.lr1(out))\n",
    "        out = nn.ReLU()(self.lr2(out))\n",
    "        out = nn.ReLU()(self.lr3(out))\n",
    "        out = nn.ReLU()(self.lr4(out))\n",
    "        out = nn.ReLU()(self.lr5(out))\n",
    "        out = nn.ReLU()(self.lr6(out))\n",
    "        out = self.output(out)\n",
    "#         nn.Softmax()(out)\n",
    "        # 다중분류의 output는 Softmax()함수로 계산해서 확률로 만들어서 출력해야 한다.\n",
    "        # 모델에서는 Linear를 통과한 결과를 반환.\n",
    "        # loss함수인 CrossEntropyLoss() 에서 softmax를 적용한다. \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModel(\n",
       "  (lr1): Linear(in_features=784, out_features=2048, bias=True)\n",
       "  (lr2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (lr3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (lr4): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (lr5): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (lr6): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_model = FashionMNISTModel()\n",
    "f_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FashionMNISTModel                        [128, 10]                 --\n",
       "├─Linear: 1-1                            [128, 2048]               1,607,680\n",
       "├─Linear: 1-2                            [128, 1024]               2,098,176\n",
       "├─Linear: 1-3                            [128, 512]                524,800\n",
       "├─Linear: 1-4                            [128, 256]                131,328\n",
       "├─Linear: 1-5                            [128, 128]                32,896\n",
       "├─Linear: 1-6                            [128, 64]                 8,256\n",
       "├─Linear: 1-7                            [128, 10]                 650\n",
       "==========================================================================================\n",
       "Total params: 4,403,786\n",
       "Trainable params: 4,403,786\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 563.68\n",
       "==========================================================================================\n",
       "Input size (MB): 0.40\n",
       "Forward/backward pass size (MB): 4.14\n",
       "Params size (MB): 17.62\n",
       "Estimated Total Size (MB): 22.16\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(f_model, (128, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n",
      "tensor([-0.0953, -0.0415, -0.1023, -0.0777,  0.0733, -0.0241,  0.0092,  0.0143,\n",
      "         0.0351, -0.0144], grad_fn=<SelectBackward0>)\n",
      "-0.2233768105506897 4\n"
     ]
    }
   ],
   "source": [
    "# 추정\n",
    "i = torch.ones((2, 1, 28, 28), dtype=torch.float32)\n",
    "# i.shape\n",
    "y_hat = f_model(i)\n",
    "print(y_hat.shape)\n",
    "print(y_hat[0])\n",
    "print(y_hat[0].sum().item(), y_hat[0].argmax(dim=-1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0928, 0.0980, 0.0922, 0.0945, 0.1099, 0.0997, 0.1031, 0.1036, 0.1058,\n",
      "        0.1006], grad_fn=<SoftmaxBackward0>)\n",
      "0.9999998807907104 4\n"
     ]
    }
   ],
   "source": [
    "# 모델이 추정한 결과의 class를 알고 싶을 경우는 Softmax를 계산할 필요 없다.\n",
    "# 모델의 추정 확률을 알고 싶을 경우 softmax를 계산한다.\n",
    "y_hat2 = nn.Softmax(dim=-1)(y_hat[0])\n",
    "print(y_hat2)\n",
    "print(y_hat2.sum().item(), y_hat2.argmax(dim=-1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 학습\n",
    "import time\n",
    "\n",
    "# 모델 생성 + device 이동\n",
    "fmnist_model = FashionMNISTModel().to(device)\n",
    "# loss -> 다중분류: CrossEntropyLoss()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimzer = torch.optim.Adam(fmnist_model.parameters(), lr=0.001)\n",
    "\n",
    "# 결과저장할 리스트\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 위스콘신 유방암 데이터셋 - 이진분류(Binary Classification) 문제\n",
    "\n",
    "- **이진 분류 문제 처리 모델의 두가지 방법**\n",
    "    1. positive(1)일 확률을 출력하도록 구현\n",
    "        - output layer: units=1, activation='sigmoid'\n",
    "        - loss: binary_crossentropy\n",
    "    2. negative(0)일 확률과 positive(1)일 확률을 출력하도록 구현 => 다중분류 처리 방식으로 해결\n",
    "        - output layer: units=2, activation='softmax', y(정답)은 one hot encoding 처리\n",
    "        - loss: categorical_crossentropy\n",
    "        \n",
    "- 위스콘신 대학교에서 제공한 종양의 악성/양성여부 분류를 위한 데이터셋\n",
    "- Feature\n",
    "    - 종양에 대한 다양한 측정값들\n",
    "- Target의 class\n",
    "    - 0 - malignant(악성종양)\n",
    "    - 1 - benign(양성종양)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
