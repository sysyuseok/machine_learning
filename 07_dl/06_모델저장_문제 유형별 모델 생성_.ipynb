{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장\n",
    "\n",
    "- 학습한 모델을 저장장치에 파일로 저장하고 나중에 불러와 사용(추가 학습, 예측 서비스) 할 수 있도록 한다. \n",
    "- 파이토치는 모델의 파라미터만 저장하는 방법과 모델 구조와 파라미터 모두를 저장하는 두가지 방식을 제공한다.\n",
    "- 저장 함수\n",
    "    - `torch.save(저장할 객체, 저장경로)`\n",
    "- 보통 저장파일의 확장자는 `pt`나 `pth` 를 지정한다.\n",
    "\n",
    "## 모델 전체 저장하기 및 불러오기\n",
    "\n",
    "- 저장하기\n",
    "    - `torch.save(model, 저장경로)`\n",
    "- 불러오기\n",
    "    - `load_model = torch.load(저장경로)`\n",
    "- 저장시 **pickle**을 이용해 직렬화하기 때문에 불어오는 실행환경에도 모델을 저장할 때 사용한 클래스가 있어야 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 모델의 파라미터만 저장\n",
    "- 모델을 구성하는 파라미터만 저장한다.\n",
    "- 모델의 구조는 저장하지 않기 때문에 불러올 때 **모델을 먼저 생성하고 생성한 모델에 불러온 파라미터를 덮어씌운다.**\n",
    "- 모델의 파라미터는 **state_dict** 형식으로 저장한다.\n",
    "\n",
    "### state_dict\n",
    "- 모델의 파라미터 Tensor들을 레이어 단위별로 나누어 저장한 Ordered Dictionary (OrderedDict)\n",
    "- `모델객체.state_dict()` 메소드를 이용해 조회한다.\n",
    "- 모델의 state_dict을 조회 후 저장한다.\n",
    "    - `torch.save(model.state_dict(), \"저장경로\")`\n",
    "- 생성된 모델에 읽어온 state_dict를 덮어씌운다.\n",
    "    - `new_model.load_state_dict(torch.load(\"state_dict저장경로\"))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_layer = nn.Linear(784, 64)\n",
    "        self.out = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = torch.flatten(X, start_dim=1)\n",
    "        X = nn.ReLU()(self.in_layer(X))\n",
    "        X = self.out(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (in_layer): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (out): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model = Network()\n",
    "sample_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['in_layer.weight', 'in_layer.bias', 'out.weight', 'out.bias'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dict 조회\n",
    "sd = sample_model.state_dict()\n",
    "print(type(sd))\n",
    "sd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 784]), torch.Size([64]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd['in_layer.weight'].shape, sd['in_layer.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001FDFEC2F1B0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0101,  0.0306, -0.0164,  0.0321, -0.0119,  0.0323,  0.0212, -0.0171,\n",
       "        -0.0073, -0.0351,  0.0132,  0.0166, -0.0245, -0.0214,  0.0279, -0.0108,\n",
       "        -0.0248,  0.0193, -0.0216, -0.0066, -0.0010, -0.0283, -0.0340,  0.0041,\n",
       "        -0.0054, -0.0333, -0.0097, -0.0122, -0.0289, -0.0154, -0.0025, -0.0081,\n",
       "        -0.0186,  0.0063, -0.0092, -0.0047, -0.0262, -0.0220,  0.0330,  0.0064,\n",
       "        -0.0155, -0.0262,  0.0228, -0.0118, -0.0217,  0.0313, -0.0102,  0.0343,\n",
       "         0.0271,  0.0144,  0.0318,  0.0012, -0.0118,  0.0072, -0.0020,  0.0168,\n",
       "        -0.0297,  0.0301, -0.0074, -0.0079,  0.0264, -0.0033,  0.0261, -0.0161],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model.in_layer.weight\n",
    "sample_model.in_layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint를 저장 및 불러오기\n",
    "- 학습이 끝나지 않은 모델을 저장 후 나중에 이어서 학습시킬 경우에는 모델의 구조, 파라미터 뿐만 아니라 optimizer, loss 함수등 학습에 필요한 객체들을 저장해야 한다.\n",
    "- Dictionary에 필요한 요소들을 key-value 쌍으로 저장후 `torch.save()`를 이용해 저장한다.\n",
    "```python\n",
    "# 저장\n",
    "torch.save({\n",
    "    'epoch':epoch,\n",
    "    'model_state_dict':model.state_dict(),\n",
    "    'optimizer_state_dict':optimizer.state_dict(),\n",
    "    'loss':train_loss\n",
    "}, \"저장경로\")\n",
    "\n",
    "# 불러오기\n",
    "model = MyModel()\n",
    "optimizer = optim.Adam(model.parameter())\n",
    "\n",
    "checkpoint = torch.load(\"저장경로\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "#### 이어학습\n",
    "model.train()\n",
    "#### 추론\n",
    "model.eval()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 문제 유형별 MLP 네트워크\n",
    "- MLP(Multi Layer Perceptron)\n",
    "    - Fully Connected Layer로 구성된 네트워크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchinfo\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Regression(회귀)\n",
    "\n",
    "## Boston Housing Dataset\n",
    "보스턴 주택가격 dataset은 다음과 같은 속성을 바탕으로 해당 타운 주택 가격의 중앙값을 예측하는 문제.\n",
    "- CRIM: 범죄율\n",
    "- ZN: 25,000 평방피트당 주거지역 비율\n",
    "- INDUS: 비소매 상업지구 비율\n",
    "- CHAS: 찰스강에 인접해 있는지 여부(인접:1, 아니면:0)\n",
    "- NOX: 일산화질소 농도(단위: 0.1ppm)\n",
    "- RM: 주택당 방의 수\n",
    "- AGE: 1940년 이전에 건설된 주택의 비율\n",
    "- DIS: 5개의 보스턴 직업고용센터와의 거리(가중 평균)\n",
    "- RAD: 고속도로 접근성\n",
    "- TAX: 재산세율\n",
    "- PTRATIO: 학생/교사 비율\n",
    "- B: 흑인 비율\n",
    "- LSTAT: 하위 계층 비율\n",
    "<br><br>\n",
    "- **Target**\n",
    "    - MEDV: 타운의 주택가격 중앙값(단위: 1,000달러)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset, DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506, 1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston = pd.read_csv('boston_hosing.csv')\n",
    "boston.shape\n",
    "\n",
    "X_boston = boston.drop(columns='MEDV').values\n",
    "y_boston = boston['MEDV'].values.reshape(-1, 1) # 2차원\n",
    "X_boston.shape, y_boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test set 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_boston, y_boston,\n",
    "                                                    test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = torch.tensor(scaler.fit_transform(X_train), dtype=torch.float32)\n",
    "X_test_scaled = torch.tensor(scaler.transform(X_test), dtype=torch.float32)\n",
    "# y를 Tensor 타입으로 변환.\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 102\n",
      "(tensor([-0.3726, -0.4996, -0.7049,  3.6645, -0.4249,  0.9357,  0.6937, -0.4372,\n",
      "        -0.1622, -0.5617, -0.4846,  0.3717, -0.4110]), tensor([26.7000]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset\n",
    "boston_train_set = TensorDataset(X_train_scaled, y_train_tensor)\n",
    "boston_test_set = TensorDataset(X_test_scaled, y_test_tensor)\n",
    "print(len(boston_train_set), len(boston_test_set))\n",
    "print(boston_train_set[0])\n",
    "\n",
    "# DataLoader\n",
    "boston_train_loader = DataLoader(boston_train_set, batch_size=200, \n",
    "                                 shuffle=True, drop_last=True)\n",
    "boston_test_loader = DataLoader(boston_test_set, batch_size=len(boston_test_set))\n",
    "len(boston_train_loader), len(boston_test_loader)  # epoch 당 step 수 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 입력 layer => in_feature: input data의 feature개수에 맞춘다.\n",
    "        self.lr1 = nn.Linear(13, 32)   # input\n",
    "        # Hidden layer => in_feature: 앞 Layer의 out_feature 개수에 맞춘다.\n",
    "        self.lr2 = nn.Linear(32, 16)  \n",
    "        # output layer => out_feature: 모델의 최종 출력 개수에 맞춘다. (집값 1개->1)\n",
    "        self.lr3 = nn.Linear(16, 1)\n",
    "    \n",
    "        \n",
    "    def forward(self, X):\n",
    "        # input layer\n",
    "        out = self.lr1(X)\n",
    "        out = nn.ReLU()(out)\n",
    "        # hidden\n",
    "        out = self.lr2(out)\n",
    "        out = nn.ReLU()(out)\n",
    "        # output -> 회귀처리 모델에서 output layer에서는 활성함수를 적용하지 않는다.\n",
    "        #   예외: 출력결과가 특정 활성함수의 출력과 매칭될 경우.\n",
    "        #        output: 0 ~ 1 => logistic 함수사용.\n",
    "        #        output: -1 ~ 1=> hyperbolic tangent (tanh)\n",
    "        out  = self.lr3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BostonModel                              [200, 1]                  --\n",
       "├─Linear: 1-1                            [200, 32]                 448\n",
       "├─Linear: 1-2                            [200, 16]                 528\n",
       "├─Linear: 1-3                            [200, 1]                  17\n",
       "==========================================================================================\n",
       "Total params: 993\n",
       "Trainable params: 993\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.20\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.08\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_model = BostonModel()\n",
    "torchinfo.summary(boston_model, (200, 13))  #(모델, 입력데이터shape-(batch size, feature) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCH = 1000\n",
    "LR = 0.001\n",
    "# 결과 저장할 리스트\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "# 모델, loss함수(회귀-MSE), optimizer\n",
    "boston_model = BostonModel()\n",
    "boston_model = boston_model.to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.RMSprop(boston_model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1000] train loss: 594.37888, val loss: 570.43408\n",
      "[2/1000] train loss: 588.66394, val loss: 562.88055\n",
      "[3/1000] train loss: 578.88843, val loss: 554.63916\n",
      "[4/1000] train loss: 570.28241, val loss: 545.62061\n",
      "[5/1000] train loss: 566.16193, val loss: 535.91327\n",
      "[6/1000] train loss: 548.04317, val loss: 525.59784\n",
      "[7/1000] train loss: 544.15288, val loss: 514.36932\n",
      "[8/1000] train loss: 533.58899, val loss: 502.53384\n",
      "[9/1000] train loss: 520.16202, val loss: 490.04745\n",
      "[10/1000] train loss: 503.44720, val loss: 477.12799\n",
      "[11/1000] train loss: 491.97881, val loss: 463.80646\n",
      "[12/1000] train loss: 477.20920, val loss: 450.13510\n",
      "[13/1000] train loss: 462.22078, val loss: 436.23578\n",
      "[14/1000] train loss: 444.92764, val loss: 422.03226\n",
      "[15/1000] train loss: 432.14780, val loss: 407.62064\n",
      "[16/1000] train loss: 415.96544, val loss: 393.07483\n",
      "[17/1000] train loss: 398.83139, val loss: 378.47281\n",
      "[18/1000] train loss: 383.42761, val loss: 363.88638\n",
      "[19/1000] train loss: 368.05484, val loss: 349.37817\n",
      "[20/1000] train loss: 353.38503, val loss: 334.92365\n",
      "[21/1000] train loss: 332.67911, val loss: 320.70105\n",
      "[22/1000] train loss: 321.87312, val loss: 306.36954\n",
      "[23/1000] train loss: 301.61053, val loss: 292.33179\n",
      "[24/1000] train loss: 288.17752, val loss: 278.65146\n",
      "[25/1000] train loss: 272.13469, val loss: 265.46292\n",
      "[26/1000] train loss: 259.12704, val loss: 252.69608\n",
      "[27/1000] train loss: 246.58121, val loss: 240.46005\n",
      "[28/1000] train loss: 231.71478, val loss: 228.92163\n",
      "[29/1000] train loss: 215.85726, val loss: 217.97432\n",
      "[30/1000] train loss: 207.66993, val loss: 207.50531\n",
      "[31/1000] train loss: 195.67847, val loss: 197.70747\n",
      "[32/1000] train loss: 184.70064, val loss: 188.51060\n",
      "[33/1000] train loss: 173.69517, val loss: 179.90436\n",
      "[34/1000] train loss: 163.70976, val loss: 171.89708\n",
      "[35/1000] train loss: 155.40461, val loss: 164.39897\n",
      "[36/1000] train loss: 147.08203, val loss: 157.43117\n",
      "[37/1000] train loss: 139.42784, val loss: 150.92471\n",
      "[38/1000] train loss: 132.05980, val loss: 144.90482\n",
      "[39/1000] train loss: 124.56440, val loss: 139.31673\n",
      "[40/1000] train loss: 118.96735, val loss: 134.11900\n",
      "[41/1000] train loss: 112.88076, val loss: 129.32040\n",
      "[42/1000] train loss: 106.91881, val loss: 124.91167\n",
      "[43/1000] train loss: 103.19195, val loss: 120.71027\n",
      "[44/1000] train loss: 95.64942, val loss: 116.93513\n",
      "[45/1000] train loss: 94.00289, val loss: 113.32394\n",
      "[46/1000] train loss: 89.68946, val loss: 109.93806\n",
      "[47/1000] train loss: 86.00298, val loss: 106.78872\n",
      "[48/1000] train loss: 81.77673, val loss: 103.86143\n",
      "[49/1000] train loss: 79.14735, val loss: 101.08900\n",
      "[50/1000] train loss: 76.01100, val loss: 98.45947\n",
      "[51/1000] train loss: 73.04960, val loss: 95.98785\n",
      "[52/1000] train loss: 70.08507, val loss: 93.65334\n",
      "[53/1000] train loss: 65.05212, val loss: 91.48329\n",
      "[54/1000] train loss: 65.53206, val loss: 89.35059\n",
      "[55/1000] train loss: 63.14630, val loss: 87.30978\n",
      "[56/1000] train loss: 61.38455, val loss: 85.41808\n",
      "[57/1000] train loss: 59.28687, val loss: 83.62177\n",
      "[58/1000] train loss: 57.10672, val loss: 81.92748\n",
      "[59/1000] train loss: 55.42293, val loss: 80.28802\n",
      "[60/1000] train loss: 53.60099, val loss: 78.71456\n",
      "[61/1000] train loss: 49.56720, val loss: 77.24380\n",
      "[62/1000] train loss: 50.28346, val loss: 75.69942\n",
      "[63/1000] train loss: 47.28542, val loss: 74.24863\n",
      "[64/1000] train loss: 47.40870, val loss: 72.83549\n",
      "[65/1000] train loss: 45.98719, val loss: 71.46269\n",
      "[66/1000] train loss: 43.60907, val loss: 70.25201\n",
      "[67/1000] train loss: 43.64853, val loss: 69.03398\n",
      "[68/1000] train loss: 41.93509, val loss: 67.89504\n",
      "[69/1000] train loss: 41.34433, val loss: 66.77949\n",
      "[70/1000] train loss: 40.04880, val loss: 65.70341\n",
      "[71/1000] train loss: 38.81893, val loss: 64.73434\n",
      "[72/1000] train loss: 38.34298, val loss: 63.77055\n",
      "[73/1000] train loss: 37.39608, val loss: 62.85468\n",
      "[74/1000] train loss: 36.09349, val loss: 61.95921\n",
      "[75/1000] train loss: 35.71299, val loss: 61.10288\n",
      "[76/1000] train loss: 34.58481, val loss: 60.30463\n",
      "[77/1000] train loss: 34.17209, val loss: 59.54082\n",
      "[78/1000] train loss: 33.60917, val loss: 58.77293\n",
      "[79/1000] train loss: 32.56846, val loss: 58.05486\n",
      "[80/1000] train loss: 31.90950, val loss: 57.33314\n",
      "[81/1000] train loss: 31.31500, val loss: 56.69246\n",
      "[82/1000] train loss: 30.87609, val loss: 56.04202\n",
      "[83/1000] train loss: 30.46867, val loss: 55.41656\n",
      "[84/1000] train loss: 29.85958, val loss: 54.81738\n",
      "[85/1000] train loss: 29.57012, val loss: 54.23070\n",
      "[86/1000] train loss: 28.95261, val loss: 53.61811\n",
      "[87/1000] train loss: 28.43888, val loss: 53.05193\n",
      "[88/1000] train loss: 27.97784, val loss: 52.49604\n",
      "[89/1000] train loss: 27.56157, val loss: 51.98011\n",
      "[90/1000] train loss: 27.21108, val loss: 51.45367\n",
      "[91/1000] train loss: 26.88984, val loss: 50.95412\n",
      "[92/1000] train loss: 25.67509, val loss: 50.47048\n",
      "[93/1000] train loss: 26.13541, val loss: 50.02700\n",
      "[94/1000] train loss: 25.92374, val loss: 49.54753\n",
      "[95/1000] train loss: 25.62069, val loss: 49.09598\n",
      "[96/1000] train loss: 23.23847, val loss: 48.80254\n",
      "[97/1000] train loss: 24.76808, val loss: 48.37637\n",
      "[98/1000] train loss: 24.66081, val loss: 47.91581\n",
      "[99/1000] train loss: 24.28041, val loss: 47.49018\n",
      "[100/1000] train loss: 24.20727, val loss: 47.10630\n",
      "[101/1000] train loss: 23.95191, val loss: 46.69148\n",
      "[102/1000] train loss: 23.59808, val loss: 46.25690\n",
      "[103/1000] train loss: 23.46990, val loss: 45.87302\n",
      "[104/1000] train loss: 23.06732, val loss: 45.52484\n",
      "[105/1000] train loss: 22.97179, val loss: 45.20013\n",
      "[106/1000] train loss: 22.80039, val loss: 44.88390\n",
      "[107/1000] train loss: 22.46075, val loss: 44.53638\n",
      "[108/1000] train loss: 22.44215, val loss: 44.20955\n",
      "[109/1000] train loss: 21.95626, val loss: 43.92160\n",
      "[110/1000] train loss: 21.60365, val loss: 43.63688\n",
      "[111/1000] train loss: 21.36169, val loss: 43.29761\n",
      "[112/1000] train loss: 21.66782, val loss: 43.02995\n",
      "[113/1000] train loss: 21.46171, val loss: 42.74675\n",
      "[114/1000] train loss: 21.11337, val loss: 42.43841\n",
      "[115/1000] train loss: 21.24596, val loss: 42.18327\n",
      "[116/1000] train loss: 20.70803, val loss: 41.93864\n",
      "[117/1000] train loss: 20.90133, val loss: 41.64139\n",
      "[118/1000] train loss: 18.96504, val loss: 41.51876\n",
      "[119/1000] train loss: 20.48494, val loss: 41.22355\n",
      "[120/1000] train loss: 20.50382, val loss: 40.94671\n",
      "[121/1000] train loss: 20.15074, val loss: 40.63837\n",
      "[122/1000] train loss: 20.09080, val loss: 40.37399\n",
      "[123/1000] train loss: 20.02295, val loss: 40.09501\n",
      "[124/1000] train loss: 19.89075, val loss: 39.85528\n",
      "[125/1000] train loss: 19.80662, val loss: 39.65081\n",
      "[126/1000] train loss: 19.68623, val loss: 39.44688\n",
      "[127/1000] train loss: 19.53397, val loss: 39.18604\n",
      "[128/1000] train loss: 19.37829, val loss: 38.99076\n",
      "[129/1000] train loss: 19.08894, val loss: 38.74807\n",
      "[130/1000] train loss: 19.05601, val loss: 38.50860\n",
      "[131/1000] train loss: 18.77457, val loss: 38.29783\n",
      "[132/1000] train loss: 18.93945, val loss: 38.04119\n",
      "[133/1000] train loss: 18.68240, val loss: 37.85474\n",
      "[134/1000] train loss: 18.56606, val loss: 37.65210\n",
      "[135/1000] train loss: 18.61533, val loss: 37.44573\n",
      "[136/1000] train loss: 18.40655, val loss: 37.22993\n",
      "[137/1000] train loss: 18.42374, val loss: 37.06831\n",
      "[138/1000] train loss: 18.21906, val loss: 36.93315\n",
      "[139/1000] train loss: 17.89433, val loss: 36.76494\n",
      "[140/1000] train loss: 18.01266, val loss: 36.57037\n",
      "[141/1000] train loss: 17.65322, val loss: 36.34428\n",
      "[142/1000] train loss: 17.86106, val loss: 36.20615\n",
      "[143/1000] train loss: 17.70987, val loss: 36.01036\n",
      "[144/1000] train loss: 17.40926, val loss: 35.86336\n",
      "[145/1000] train loss: 17.41410, val loss: 35.67381\n",
      "[146/1000] train loss: 17.17671, val loss: 35.46183\n",
      "[147/1000] train loss: 17.30713, val loss: 35.30429\n",
      "[148/1000] train loss: 17.19514, val loss: 35.13166\n",
      "[149/1000] train loss: 17.16928, val loss: 34.94283\n",
      "[150/1000] train loss: 17.08226, val loss: 34.79747\n",
      "[151/1000] train loss: 16.48721, val loss: 34.56900\n",
      "[152/1000] train loss: 16.97955, val loss: 34.38063\n",
      "[153/1000] train loss: 16.72632, val loss: 34.18617\n",
      "[154/1000] train loss: 16.64289, val loss: 34.06793\n",
      "[155/1000] train loss: 16.57964, val loss: 33.92138\n",
      "[156/1000] train loss: 16.42066, val loss: 33.83148\n",
      "[157/1000] train loss: 16.19176, val loss: 33.64802\n",
      "[158/1000] train loss: 16.31036, val loss: 33.49192\n",
      "[159/1000] train loss: 15.98592, val loss: 33.37859\n",
      "[160/1000] train loss: 16.18191, val loss: 33.23966\n",
      "[161/1000] train loss: 16.09659, val loss: 33.05972\n",
      "[162/1000] train loss: 15.63031, val loss: 32.95806\n",
      "[163/1000] train loss: 15.69392, val loss: 32.85466\n",
      "[164/1000] train loss: 15.85941, val loss: 32.74502\n",
      "[165/1000] train loss: 15.69601, val loss: 32.60904\n",
      "[166/1000] train loss: 15.67155, val loss: 32.39585\n",
      "[167/1000] train loss: 15.51041, val loss: 32.33676\n",
      "[168/1000] train loss: 15.08885, val loss: 32.11640\n",
      "[169/1000] train loss: 15.32249, val loss: 32.04140\n",
      "[170/1000] train loss: 15.28005, val loss: 31.93475\n",
      "[171/1000] train loss: 15.30472, val loss: 31.77993\n",
      "[172/1000] train loss: 15.15501, val loss: 31.67103\n",
      "[173/1000] train loss: 15.10671, val loss: 31.55642\n",
      "[174/1000] train loss: 14.72147, val loss: 31.53170\n",
      "[175/1000] train loss: 14.90168, val loss: 31.39972\n",
      "[176/1000] train loss: 14.78180, val loss: 31.21549\n",
      "[177/1000] train loss: 14.43297, val loss: 31.09301\n",
      "[178/1000] train loss: 14.70107, val loss: 31.02994\n",
      "[179/1000] train loss: 14.68054, val loss: 30.87478\n",
      "[180/1000] train loss: 14.57710, val loss: 30.77544\n",
      "[181/1000] train loss: 14.25503, val loss: 30.64750\n",
      "[182/1000] train loss: 14.43558, val loss: 30.48214\n",
      "[183/1000] train loss: 14.34798, val loss: 30.40458\n",
      "[184/1000] train loss: 14.32441, val loss: 30.29029\n",
      "[185/1000] train loss: 14.18427, val loss: 30.15951\n",
      "[186/1000] train loss: 14.07807, val loss: 30.03232\n",
      "[187/1000] train loss: 14.07722, val loss: 29.92075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[188/1000] train loss: 14.03581, val loss: 29.85709\n",
      "[189/1000] train loss: 13.43996, val loss: 29.75305\n",
      "[190/1000] train loss: 13.95107, val loss: 29.69041\n",
      "[191/1000] train loss: 13.42567, val loss: 29.68712\n",
      "[192/1000] train loss: 13.74028, val loss: 29.57350\n",
      "[193/1000] train loss: 13.67047, val loss: 29.48989\n",
      "[194/1000] train loss: 13.55108, val loss: 29.29179\n",
      "[195/1000] train loss: 13.52221, val loss: 29.23738\n",
      "[196/1000] train loss: 13.50324, val loss: 29.07234\n",
      "[197/1000] train loss: 13.39694, val loss: 29.00282\n",
      "[198/1000] train loss: 13.38988, val loss: 28.90586\n",
      "[199/1000] train loss: 13.27199, val loss: 28.76713\n",
      "[200/1000] train loss: 13.24864, val loss: 28.72983\n",
      "[201/1000] train loss: 13.17902, val loss: 28.60137\n",
      "[202/1000] train loss: 13.08836, val loss: 28.50629\n",
      "[203/1000] train loss: 13.09205, val loss: 28.39671\n",
      "[204/1000] train loss: 13.07368, val loss: 28.25850\n",
      "[205/1000] train loss: 12.96353, val loss: 28.17633\n",
      "[206/1000] train loss: 12.82444, val loss: 28.03016\n",
      "[207/1000] train loss: 12.67687, val loss: 27.98951\n",
      "[208/1000] train loss: 12.70986, val loss: 27.87662\n",
      "[209/1000] train loss: 12.66675, val loss: 27.80449\n",
      "[210/1000] train loss: 12.63183, val loss: 27.78686\n",
      "[211/1000] train loss: 12.71218, val loss: 27.69827\n",
      "[212/1000] train loss: 12.36597, val loss: 27.55536\n",
      "[213/1000] train loss: 12.62513, val loss: 27.46457\n",
      "[214/1000] train loss: 12.50241, val loss: 27.46326\n",
      "[215/1000] train loss: 12.41280, val loss: 27.34576\n",
      "[216/1000] train loss: 12.26592, val loss: 27.28527\n",
      "[217/1000] train loss: 12.24279, val loss: 27.10080\n",
      "[218/1000] train loss: 12.25381, val loss: 27.10903\n",
      "[219/1000] train loss: 12.18726, val loss: 27.02219\n",
      "[220/1000] train loss: 12.14659, val loss: 26.97163\n",
      "[221/1000] train loss: 12.02293, val loss: 27.00105\n",
      "[222/1000] train loss: 12.01487, val loss: 26.86224\n",
      "[223/1000] train loss: 12.00609, val loss: 26.68814\n",
      "[224/1000] train loss: 11.93788, val loss: 26.60067\n",
      "[225/1000] train loss: 11.92581, val loss: 26.58886\n",
      "[226/1000] train loss: 11.86964, val loss: 26.47900\n",
      "[227/1000] train loss: 11.73508, val loss: 26.38459\n",
      "[228/1000] train loss: 11.69593, val loss: 26.31735\n",
      "[229/1000] train loss: 11.74489, val loss: 26.20397\n",
      "[230/1000] train loss: 11.64393, val loss: 26.24422\n",
      "[231/1000] train loss: 11.55100, val loss: 26.04883\n",
      "[232/1000] train loss: 11.55110, val loss: 25.99339\n",
      "[233/1000] train loss: 11.60232, val loss: 26.01097\n",
      "[234/1000] train loss: 11.59488, val loss: 25.82929\n",
      "[235/1000] train loss: 11.21060, val loss: 25.77105\n",
      "[236/1000] train loss: 11.39755, val loss: 25.68769\n",
      "[237/1000] train loss: 11.14070, val loss: 25.64801\n",
      "[238/1000] train loss: 11.28085, val loss: 25.51578\n",
      "[239/1000] train loss: 11.26766, val loss: 25.43241\n",
      "[240/1000] train loss: 11.16425, val loss: 25.39799\n",
      "[241/1000] train loss: 10.99222, val loss: 25.25740\n",
      "[242/1000] train loss: 11.05197, val loss: 25.28619\n",
      "[243/1000] train loss: 11.01592, val loss: 25.28972\n",
      "[244/1000] train loss: 10.97615, val loss: 25.14513\n",
      "[245/1000] train loss: 11.01981, val loss: 25.20477\n",
      "[246/1000] train loss: 10.82013, val loss: 25.06525\n",
      "[247/1000] train loss: 10.88403, val loss: 24.95768\n",
      "[248/1000] train loss: 10.76104, val loss: 24.91826\n",
      "[249/1000] train loss: 10.78041, val loss: 24.95745\n",
      "[250/1000] train loss: 10.80693, val loss: 24.92653\n",
      "[251/1000] train loss: 10.66562, val loss: 24.73041\n",
      "[252/1000] train loss: 10.59292, val loss: 24.67128\n",
      "[253/1000] train loss: 10.59006, val loss: 24.62319\n",
      "[254/1000] train loss: 10.56822, val loss: 24.53422\n",
      "[255/1000] train loss: 10.54556, val loss: 24.53687\n",
      "[256/1000] train loss: 10.49152, val loss: 24.41533\n",
      "[257/1000] train loss: 10.57498, val loss: 24.26634\n",
      "[258/1000] train loss: 10.42486, val loss: 24.22130\n",
      "[259/1000] train loss: 10.31366, val loss: 24.26229\n",
      "[260/1000] train loss: 10.28528, val loss: 24.22242\n",
      "[261/1000] train loss: 10.30132, val loss: 24.21204\n",
      "[262/1000] train loss: 10.24222, val loss: 24.01513\n",
      "[263/1000] train loss: 10.19821, val loss: 23.91505\n",
      "[264/1000] train loss: 10.14103, val loss: 23.94556\n",
      "[265/1000] train loss: 10.12702, val loss: 23.88242\n",
      "[266/1000] train loss: 9.66882, val loss: 23.81281\n",
      "[267/1000] train loss: 10.07026, val loss: 23.85555\n",
      "[268/1000] train loss: 10.02216, val loss: 23.73909\n",
      "[269/1000] train loss: 9.85561, val loss: 23.72317\n",
      "[270/1000] train loss: 9.98178, val loss: 23.76594\n",
      "[271/1000] train loss: 9.96686, val loss: 23.53141\n",
      "[272/1000] train loss: 9.96854, val loss: 23.44014\n",
      "[273/1000] train loss: 9.87528, val loss: 23.52809\n",
      "[274/1000] train loss: 9.82330, val loss: 23.47962\n",
      "[275/1000] train loss: 9.72203, val loss: 23.35970\n",
      "[276/1000] train loss: 9.76649, val loss: 23.26747\n",
      "[277/1000] train loss: 9.69699, val loss: 23.24095\n",
      "[278/1000] train loss: 9.65801, val loss: 23.11929\n",
      "[279/1000] train loss: 9.59102, val loss: 23.20731\n",
      "[280/1000] train loss: 9.58087, val loss: 23.16298\n",
      "[281/1000] train loss: 9.62241, val loss: 23.29539\n",
      "[282/1000] train loss: 9.33344, val loss: 23.24485\n",
      "[283/1000] train loss: 9.50953, val loss: 23.03555\n",
      "[284/1000] train loss: 9.41317, val loss: 23.08447\n",
      "[285/1000] train loss: 9.44688, val loss: 23.13357\n",
      "[286/1000] train loss: 9.42465, val loss: 23.04274\n",
      "[287/1000] train loss: 9.36814, val loss: 22.85034\n",
      "[288/1000] train loss: 9.24898, val loss: 22.84921\n",
      "[289/1000] train loss: 9.30204, val loss: 22.83014\n",
      "[290/1000] train loss: 9.32965, val loss: 22.94359\n",
      "[291/1000] train loss: 9.28044, val loss: 22.78734\n",
      "[292/1000] train loss: 9.26466, val loss: 22.65631\n",
      "[293/1000] train loss: 9.22884, val loss: 22.75187\n",
      "[294/1000] train loss: 9.08291, val loss: 22.63802\n",
      "[295/1000] train loss: 9.13667, val loss: 22.59308\n",
      "[296/1000] train loss: 9.13111, val loss: 22.60541\n",
      "[297/1000] train loss: 9.05537, val loss: 22.56880\n",
      "[298/1000] train loss: 9.03827, val loss: 22.49344\n",
      "[299/1000] train loss: 9.05668, val loss: 22.41800\n",
      "[300/1000] train loss: 9.05271, val loss: 22.32274\n",
      "[301/1000] train loss: 9.00753, val loss: 22.27941\n",
      "[302/1000] train loss: 8.89554, val loss: 22.37279\n",
      "[303/1000] train loss: 8.91528, val loss: 22.30659\n",
      "[304/1000] train loss: 8.87551, val loss: 22.20521\n",
      "[305/1000] train loss: 8.85374, val loss: 22.26135\n",
      "[306/1000] train loss: 8.80971, val loss: 22.10411\n",
      "[307/1000] train loss: 8.80294, val loss: 22.11705\n",
      "[308/1000] train loss: 8.61108, val loss: 22.15240\n",
      "[309/1000] train loss: 8.78421, val loss: 22.13099\n",
      "[310/1000] train loss: 8.73566, val loss: 22.16265\n",
      "[311/1000] train loss: 8.62865, val loss: 22.02986\n",
      "[312/1000] train loss: 8.66538, val loss: 21.90084\n",
      "[313/1000] train loss: 8.74907, val loss: 21.93768\n",
      "[314/1000] train loss: 8.63366, val loss: 22.00946\n",
      "[315/1000] train loss: 8.66442, val loss: 21.78962\n",
      "[316/1000] train loss: 8.09188, val loss: 21.69755\n",
      "[317/1000] train loss: 8.65903, val loss: 21.85656\n",
      "[318/1000] train loss: 8.50888, val loss: 21.83022\n",
      "[319/1000] train loss: 8.55217, val loss: 21.84654\n",
      "[320/1000] train loss: 8.33702, val loss: 21.78170\n",
      "[321/1000] train loss: 8.53318, val loss: 21.78749\n",
      "[322/1000] train loss: 8.46816, val loss: 21.80314\n",
      "[323/1000] train loss: 7.84886, val loss: 21.81380\n",
      "[324/1000] train loss: 8.42308, val loss: 21.83437\n",
      "[325/1000] train loss: 8.33379, val loss: 21.62877\n",
      "[326/1000] train loss: 8.28043, val loss: 21.58463\n",
      "[327/1000] train loss: 8.36665, val loss: 21.40585\n",
      "[328/1000] train loss: 8.34686, val loss: 21.56573\n",
      "[329/1000] train loss: 8.27344, val loss: 21.64790\n",
      "[330/1000] train loss: 8.22656, val loss: 21.50201\n",
      "[331/1000] train loss: 8.34385, val loss: 21.38845\n",
      "[332/1000] train loss: 8.25229, val loss: 21.52815\n",
      "[333/1000] train loss: 8.25823, val loss: 21.39484\n",
      "[334/1000] train loss: 8.20572, val loss: 21.29698\n",
      "[335/1000] train loss: 8.18234, val loss: 21.31812\n",
      "[336/1000] train loss: 8.24981, val loss: 21.10219\n",
      "[337/1000] train loss: 8.07305, val loss: 21.41368\n",
      "[338/1000] train loss: 8.07861, val loss: 21.38037\n",
      "[339/1000] train loss: 8.03853, val loss: 21.29380\n",
      "[340/1000] train loss: 8.02716, val loss: 21.34398\n",
      "[341/1000] train loss: 8.07452, val loss: 21.50116\n",
      "[342/1000] train loss: 8.08144, val loss: 21.35050\n",
      "[343/1000] train loss: 7.80700, val loss: 21.28852\n",
      "[344/1000] train loss: 7.96274, val loss: 21.21440\n",
      "[345/1000] train loss: 7.92839, val loss: 21.26912\n",
      "[346/1000] train loss: 7.93607, val loss: 21.08392\n",
      "[347/1000] train loss: 7.91948, val loss: 20.88797\n",
      "[348/1000] train loss: 7.95258, val loss: 20.96720\n",
      "[349/1000] train loss: 7.93101, val loss: 21.01478\n",
      "[350/1000] train loss: 8.01045, val loss: 21.13121\n",
      "[351/1000] train loss: 7.85057, val loss: 21.01485\n",
      "[352/1000] train loss: 7.82201, val loss: 20.77231\n",
      "[353/1000] train loss: 7.88409, val loss: 21.05342\n",
      "[354/1000] train loss: 7.87889, val loss: 21.10302\n",
      "[355/1000] train loss: 7.73251, val loss: 20.87316\n",
      "[356/1000] train loss: 7.76127, val loss: 20.96920\n",
      "[357/1000] train loss: 7.77721, val loss: 20.92745\n",
      "[358/1000] train loss: 7.77487, val loss: 20.80115\n",
      "[359/1000] train loss: 7.70211, val loss: 20.89678\n",
      "[360/1000] train loss: 7.78552, val loss: 20.73849\n",
      "[361/1000] train loss: 7.72262, val loss: 20.69732\n",
      "[362/1000] train loss: 7.71013, val loss: 20.66927\n",
      "[363/1000] train loss: 7.63990, val loss: 20.65397\n",
      "[364/1000] train loss: 7.68150, val loss: 20.74825\n",
      "[365/1000] train loss: 7.69686, val loss: 20.74300\n",
      "[366/1000] train loss: 7.67845, val loss: 20.63718\n",
      "[367/1000] train loss: 7.64220, val loss: 20.77313\n",
      "[368/1000] train loss: 7.59205, val loss: 20.68545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[369/1000] train loss: 7.60804, val loss: 20.66871\n",
      "[370/1000] train loss: 7.54912, val loss: 20.60338\n",
      "[371/1000] train loss: 7.62600, val loss: 20.82917\n",
      "[372/1000] train loss: 7.58908, val loss: 20.41633\n",
      "[373/1000] train loss: 7.51596, val loss: 20.56753\n",
      "[374/1000] train loss: 7.54869, val loss: 20.52883\n",
      "[375/1000] train loss: 7.57290, val loss: 20.34956\n",
      "[376/1000] train loss: 7.47508, val loss: 20.49363\n",
      "[377/1000] train loss: 7.24138, val loss: 20.58097\n",
      "[378/1000] train loss: 7.43049, val loss: 20.48440\n",
      "[379/1000] train loss: 7.44113, val loss: 20.34090\n",
      "[380/1000] train loss: 7.43820, val loss: 20.29401\n",
      "[381/1000] train loss: 7.47553, val loss: 20.45416\n",
      "[382/1000] train loss: 7.42168, val loss: 20.62436\n",
      "[383/1000] train loss: 7.42469, val loss: 20.37425\n",
      "[384/1000] train loss: 7.49128, val loss: 20.38163\n",
      "[385/1000] train loss: 7.48760, val loss: 20.17632\n",
      "[386/1000] train loss: 7.30771, val loss: 20.30191\n",
      "[387/1000] train loss: 7.26711, val loss: 20.40251\n",
      "[388/1000] train loss: 7.39609, val loss: 20.36347\n",
      "[389/1000] train loss: 7.26633, val loss: 20.27132\n",
      "[390/1000] train loss: 7.31863, val loss: 20.37713\n",
      "[391/1000] train loss: 7.27064, val loss: 20.20066\n",
      "[392/1000] train loss: 7.28867, val loss: 20.37342\n",
      "[393/1000] train loss: 7.23167, val loss: 20.22309\n",
      "[394/1000] train loss: 7.20246, val loss: 20.37081\n",
      "[395/1000] train loss: 7.27257, val loss: 20.41336\n",
      "[396/1000] train loss: 7.25073, val loss: 20.31857\n",
      "[397/1000] train loss: 7.25477, val loss: 20.16534\n",
      "[398/1000] train loss: 7.17311, val loss: 20.08847\n",
      "[399/1000] train loss: 7.19676, val loss: 20.17995\n",
      "[400/1000] train loss: 7.23384, val loss: 20.25499\n",
      "[401/1000] train loss: 7.09945, val loss: 20.23890\n",
      "[402/1000] train loss: 7.15184, val loss: 20.03019\n",
      "[403/1000] train loss: 7.14391, val loss: 20.17805\n",
      "[404/1000] train loss: 7.09341, val loss: 20.07927\n",
      "[405/1000] train loss: 7.15191, val loss: 20.04379\n",
      "[406/1000] train loss: 7.10964, val loss: 19.92924\n",
      "[407/1000] train loss: 6.94126, val loss: 20.22589\n",
      "[408/1000] train loss: 7.09306, val loss: 20.03697\n",
      "[409/1000] train loss: 7.09890, val loss: 20.05600\n",
      "[410/1000] train loss: 7.15921, val loss: 19.82458\n",
      "[411/1000] train loss: 7.13786, val loss: 19.98990\n",
      "[412/1000] train loss: 7.03779, val loss: 19.93831\n",
      "[413/1000] train loss: 7.01910, val loss: 20.12156\n",
      "[414/1000] train loss: 6.94541, val loss: 19.87494\n",
      "[415/1000] train loss: 6.95963, val loss: 19.92639\n",
      "[416/1000] train loss: 7.00870, val loss: 19.92414\n",
      "[417/1000] train loss: 6.97601, val loss: 20.02238\n",
      "[418/1000] train loss: 6.97840, val loss: 19.94024\n",
      "[419/1000] train loss: 6.79651, val loss: 19.91020\n",
      "[420/1000] train loss: 6.90743, val loss: 19.78201\n",
      "[421/1000] train loss: 6.89622, val loss: 20.03865\n",
      "[422/1000] train loss: 6.94411, val loss: 19.79696\n",
      "[423/1000] train loss: 6.80466, val loss: 19.82132\n",
      "[424/1000] train loss: 6.88547, val loss: 19.76959\n",
      "[425/1000] train loss: 6.93294, val loss: 19.86389\n",
      "[426/1000] train loss: 6.90907, val loss: 19.92383\n",
      "[427/1000] train loss: 6.83386, val loss: 19.58194\n",
      "[428/1000] train loss: 6.85324, val loss: 19.76365\n",
      "[429/1000] train loss: 6.87000, val loss: 19.64162\n",
      "[430/1000] train loss: 6.80416, val loss: 19.84632\n",
      "[431/1000] train loss: 6.80403, val loss: 19.73771\n",
      "[432/1000] train loss: 6.84377, val loss: 19.49700\n",
      "[433/1000] train loss: 6.84449, val loss: 19.68327\n",
      "[434/1000] train loss: 6.81585, val loss: 19.44563\n",
      "[435/1000] train loss: 6.95308, val loss: 19.51394\n",
      "[436/1000] train loss: 6.81279, val loss: 19.79251\n",
      "[437/1000] train loss: 6.72095, val loss: 19.55271\n",
      "[438/1000] train loss: 6.77447, val loss: 19.71077\n",
      "[439/1000] train loss: 6.75611, val loss: 19.70464\n",
      "[440/1000] train loss: 6.74123, val loss: 19.57368\n",
      "[441/1000] train loss: 6.76719, val loss: 19.53972\n",
      "[442/1000] train loss: 6.68904, val loss: 19.45322\n",
      "[443/1000] train loss: 6.67352, val loss: 19.77830\n",
      "[444/1000] train loss: 6.65000, val loss: 19.56918\n",
      "[445/1000] train loss: 6.68788, val loss: 19.51771\n",
      "[446/1000] train loss: 6.62089, val loss: 19.43771\n",
      "[447/1000] train loss: 6.67991, val loss: 19.39404\n",
      "[448/1000] train loss: 6.63094, val loss: 19.37729\n",
      "[449/1000] train loss: 6.65150, val loss: 19.42646\n",
      "[450/1000] train loss: 6.66067, val loss: 19.58886\n",
      "[451/1000] train loss: 6.59522, val loss: 19.53298\n",
      "[452/1000] train loss: 6.64951, val loss: 19.36420\n",
      "[453/1000] train loss: 6.62371, val loss: 19.22954\n",
      "[454/1000] train loss: 6.67106, val loss: 19.47205\n",
      "[455/1000] train loss: 6.53524, val loss: 19.33268\n",
      "[456/1000] train loss: 6.61441, val loss: 19.16536\n",
      "[457/1000] train loss: 6.57780, val loss: 19.30242\n",
      "[458/1000] train loss: 6.52554, val loss: 19.17310\n",
      "[459/1000] train loss: 6.18849, val loss: 19.51669\n",
      "[460/1000] train loss: 6.58234, val loss: 19.44283\n",
      "[461/1000] train loss: 6.45329, val loss: 19.59193\n",
      "[462/1000] train loss: 6.51535, val loss: 19.30747\n",
      "[463/1000] train loss: 6.49609, val loss: 19.23093\n",
      "[464/1000] train loss: 6.52368, val loss: 19.19041\n",
      "[465/1000] train loss: 6.46662, val loss: 19.44311\n",
      "[466/1000] train loss: 6.51840, val loss: 19.24915\n",
      "[467/1000] train loss: 6.48865, val loss: 19.30582\n",
      "[468/1000] train loss: 6.57864, val loss: 19.64767\n",
      "[469/1000] train loss: 6.48076, val loss: 19.46966\n",
      "[470/1000] train loss: 6.48291, val loss: 19.28246\n",
      "[471/1000] train loss: 6.48136, val loss: 19.30641\n",
      "[472/1000] train loss: 6.51481, val loss: 19.31156\n",
      "[473/1000] train loss: 6.49025, val loss: 19.38817\n",
      "[474/1000] train loss: 6.43089, val loss: 19.35324\n",
      "[475/1000] train loss: 6.32154, val loss: 19.14684\n",
      "[476/1000] train loss: 6.44815, val loss: 19.06578\n",
      "[477/1000] train loss: 6.43782, val loss: 19.38461\n",
      "[478/1000] train loss: 6.41522, val loss: 19.06935\n",
      "[479/1000] train loss: 6.40786, val loss: 19.28513\n",
      "[480/1000] train loss: 6.41170, val loss: 18.96939\n",
      "[481/1000] train loss: 6.39416, val loss: 19.13498\n",
      "[482/1000] train loss: 6.41609, val loss: 19.08166\n",
      "[483/1000] train loss: 6.36028, val loss: 19.03915\n",
      "[484/1000] train loss: 6.29811, val loss: 19.16630\n",
      "[485/1000] train loss: 6.38236, val loss: 19.25828\n",
      "[486/1000] train loss: 6.30281, val loss: 19.21240\n",
      "[487/1000] train loss: 6.19753, val loss: 19.13707\n",
      "[488/1000] train loss: 6.35924, val loss: 19.14866\n",
      "[489/1000] train loss: 6.31556, val loss: 19.25537\n",
      "[490/1000] train loss: 6.35345, val loss: 19.23904\n",
      "[491/1000] train loss: 6.38409, val loss: 19.25571\n",
      "[492/1000] train loss: 6.29234, val loss: 19.10909\n",
      "[493/1000] train loss: 6.23661, val loss: 19.04719\n",
      "[494/1000] train loss: 6.11127, val loss: 19.01125\n",
      "[495/1000] train loss: 6.31826, val loss: 19.05908\n",
      "[496/1000] train loss: 6.17820, val loss: 19.07319\n",
      "[497/1000] train loss: 6.26734, val loss: 19.14851\n",
      "[498/1000] train loss: 6.31817, val loss: 18.91395\n",
      "[499/1000] train loss: 6.30793, val loss: 19.17114\n",
      "[500/1000] train loss: 6.28133, val loss: 18.69771\n",
      "[501/1000] train loss: 6.24413, val loss: 18.79174\n",
      "[502/1000] train loss: 6.18240, val loss: 18.72801\n",
      "[503/1000] train loss: 6.22033, val loss: 18.73559\n",
      "[504/1000] train loss: 6.16849, val loss: 18.81396\n",
      "[505/1000] train loss: 6.17681, val loss: 18.79388\n",
      "[506/1000] train loss: 6.17504, val loss: 18.81786\n",
      "[507/1000] train loss: 6.13789, val loss: 19.05464\n",
      "[508/1000] train loss: 6.17453, val loss: 18.76212\n",
      "[509/1000] train loss: 6.01887, val loss: 18.74762\n",
      "[510/1000] train loss: 6.07165, val loss: 18.93732\n",
      "[511/1000] train loss: 6.15057, val loss: 18.83058\n",
      "[512/1000] train loss: 6.04625, val loss: 18.72140\n",
      "[513/1000] train loss: 6.09071, val loss: 18.56072\n",
      "[514/1000] train loss: 6.04398, val loss: 18.73556\n",
      "[515/1000] train loss: 6.13745, val loss: 18.89410\n",
      "[516/1000] train loss: 5.99383, val loss: 18.83020\n",
      "[517/1000] train loss: 6.14453, val loss: 19.07492\n",
      "[518/1000] train loss: 5.73336, val loss: 18.84731\n",
      "[519/1000] train loss: 5.93926, val loss: 18.90130\n",
      "[520/1000] train loss: 6.04397, val loss: 18.50026\n",
      "[521/1000] train loss: 5.96995, val loss: 18.87834\n",
      "[522/1000] train loss: 5.99996, val loss: 18.79562\n",
      "[523/1000] train loss: 6.07389, val loss: 18.98294\n",
      "[524/1000] train loss: 6.03577, val loss: 18.65989\n",
      "[525/1000] train loss: 6.15874, val loss: 18.79989\n",
      "[526/1000] train loss: 6.11912, val loss: 18.52369\n",
      "[527/1000] train loss: 5.94151, val loss: 18.70560\n",
      "[528/1000] train loss: 5.81692, val loss: 18.69548\n",
      "[529/1000] train loss: 5.94464, val loss: 18.65697\n",
      "[530/1000] train loss: 5.99517, val loss: 18.76291\n",
      "[531/1000] train loss: 5.93256, val loss: 18.76052\n",
      "[532/1000] train loss: 5.95298, val loss: 18.62511\n",
      "[533/1000] train loss: 5.98980, val loss: 18.44343\n",
      "[534/1000] train loss: 5.85027, val loss: 18.72544\n",
      "[535/1000] train loss: 5.91907, val loss: 18.53121\n",
      "[536/1000] train loss: 5.96005, val loss: 18.74119\n",
      "[537/1000] train loss: 5.98989, val loss: 18.59821\n",
      "[538/1000] train loss: 5.95148, val loss: 18.64515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[539/1000] train loss: 5.87677, val loss: 18.62009\n",
      "[540/1000] train loss: 5.92120, val loss: 18.64030\n",
      "[541/1000] train loss: 5.58653, val loss: 18.84785\n",
      "[542/1000] train loss: 5.82917, val loss: 18.63009\n",
      "[543/1000] train loss: 5.80379, val loss: 18.73122\n",
      "[544/1000] train loss: 5.79611, val loss: 18.79180\n",
      "[545/1000] train loss: 5.80332, val loss: 18.60736\n",
      "[546/1000] train loss: 5.78727, val loss: 18.72855\n",
      "[547/1000] train loss: 5.80943, val loss: 18.45576\n",
      "[548/1000] train loss: 5.82752, val loss: 18.43685\n",
      "[549/1000] train loss: 5.76200, val loss: 18.58054\n",
      "[550/1000] train loss: 5.69265, val loss: 18.64350\n",
      "[551/1000] train loss: 5.44366, val loss: 18.65419\n",
      "[552/1000] train loss: 5.74092, val loss: 18.56622\n",
      "[553/1000] train loss: 5.81909, val loss: 18.50211\n",
      "[554/1000] train loss: 5.89867, val loss: 18.33023\n",
      "[555/1000] train loss: 5.79794, val loss: 18.45414\n",
      "[556/1000] train loss: 5.90812, val loss: 18.82593\n",
      "[557/1000] train loss: 5.76738, val loss: 18.68765\n",
      "[558/1000] train loss: 5.78983, val loss: 18.53084\n",
      "[559/1000] train loss: 5.80548, val loss: 18.68179\n",
      "[560/1000] train loss: 5.75840, val loss: 18.54592\n",
      "[561/1000] train loss: 5.73430, val loss: 18.57501\n",
      "[562/1000] train loss: 5.69395, val loss: 18.23940\n",
      "[563/1000] train loss: 5.84572, val loss: 18.64063\n",
      "[564/1000] train loss: 5.69724, val loss: 18.48219\n",
      "[565/1000] train loss: 5.75633, val loss: 18.30762\n",
      "[566/1000] train loss: 5.74690, val loss: 18.54694\n",
      "[567/1000] train loss: 5.70953, val loss: 18.60600\n",
      "[568/1000] train loss: 5.70292, val loss: 18.25219\n",
      "[569/1000] train loss: 5.68621, val loss: 18.48127\n",
      "[570/1000] train loss: 5.68447, val loss: 18.20570\n",
      "[571/1000] train loss: 5.75694, val loss: 18.21968\n",
      "[572/1000] train loss: 5.73328, val loss: 18.25580\n",
      "[573/1000] train loss: 5.61140, val loss: 18.42394\n",
      "[574/1000] train loss: 5.57376, val loss: 18.37617\n",
      "[575/1000] train loss: 5.69676, val loss: 18.30122\n",
      "[576/1000] train loss: 5.67091, val loss: 18.43646\n",
      "[577/1000] train loss: 5.65234, val loss: 18.47188\n",
      "[578/1000] train loss: 5.72865, val loss: 18.07321\n",
      "[579/1000] train loss: 5.61027, val loss: 18.53030\n",
      "[580/1000] train loss: 5.59163, val loss: 18.36886\n",
      "[581/1000] train loss: 5.60146, val loss: 18.39516\n",
      "[582/1000] train loss: 5.75833, val loss: 18.50217\n",
      "[583/1000] train loss: 5.77710, val loss: 18.15526\n",
      "[584/1000] train loss: 5.60665, val loss: 18.30994\n",
      "[585/1000] train loss: 5.62777, val loss: 18.23713\n",
      "[586/1000] train loss: 5.55069, val loss: 18.19283\n",
      "[587/1000] train loss: 5.64157, val loss: 18.46090\n",
      "[588/1000] train loss: 5.61352, val loss: 18.38951\n",
      "[589/1000] train loss: 5.53459, val loss: 18.25416\n",
      "[590/1000] train loss: 5.58541, val loss: 18.28917\n",
      "[591/1000] train loss: 5.60321, val loss: 18.36921\n",
      "[592/1000] train loss: 5.47177, val loss: 18.43038\n",
      "[593/1000] train loss: 5.67374, val loss: 18.57062\n",
      "[594/1000] train loss: 5.58123, val loss: 18.28361\n",
      "[595/1000] train loss: 5.53795, val loss: 18.34734\n",
      "[596/1000] train loss: 5.53615, val loss: 18.27664\n",
      "[597/1000] train loss: 5.54485, val loss: 18.22588\n",
      "[598/1000] train loss: 5.50822, val loss: 18.16627\n",
      "[599/1000] train loss: 5.55533, val loss: 18.08992\n",
      "[600/1000] train loss: 5.53333, val loss: 18.06320\n",
      "[601/1000] train loss: 5.50083, val loss: 18.02261\n",
      "[602/1000] train loss: 5.56221, val loss: 17.92973\n",
      "[603/1000] train loss: 5.44555, val loss: 18.16269\n",
      "[604/1000] train loss: 5.46071, val loss: 18.18912\n",
      "[605/1000] train loss: 5.46491, val loss: 18.21912\n",
      "[606/1000] train loss: 5.42876, val loss: 18.17229\n",
      "[607/1000] train loss: 5.51864, val loss: 17.98678\n",
      "[608/1000] train loss: 5.41803, val loss: 18.04298\n",
      "[609/1000] train loss: 5.46283, val loss: 18.24244\n",
      "[610/1000] train loss: 5.42144, val loss: 18.03875\n",
      "[611/1000] train loss: 5.54043, val loss: 17.94562\n",
      "[612/1000] train loss: 5.54288, val loss: 18.29216\n",
      "[613/1000] train loss: 5.51640, val loss: 17.99017\n",
      "[614/1000] train loss: 5.48892, val loss: 18.16389\n",
      "[615/1000] train loss: 5.43279, val loss: 18.18306\n",
      "[616/1000] train loss: 5.41689, val loss: 18.04863\n",
      "[617/1000] train loss: 5.47754, val loss: 18.00767\n",
      "[618/1000] train loss: 5.43101, val loss: 18.27341\n",
      "[619/1000] train loss: 5.49991, val loss: 18.38128\n",
      "[620/1000] train loss: 5.24582, val loss: 18.06964\n",
      "[621/1000] train loss: 5.50066, val loss: 18.00127\n",
      "[622/1000] train loss: 5.41969, val loss: 17.98110\n",
      "[623/1000] train loss: 5.41185, val loss: 18.19435\n",
      "[624/1000] train loss: 5.41443, val loss: 18.00977\n",
      "[625/1000] train loss: 5.41846, val loss: 18.23511\n",
      "[626/1000] train loss: 5.44167, val loss: 17.82363\n",
      "[627/1000] train loss: 5.45010, val loss: 17.84623\n",
      "[628/1000] train loss: 5.38584, val loss: 18.07594\n",
      "[629/1000] train loss: 5.29906, val loss: 17.96131\n",
      "[630/1000] train loss: 5.37275, val loss: 17.92315\n",
      "[631/1000] train loss: 5.37177, val loss: 18.13328\n",
      "[632/1000] train loss: 5.45305, val loss: 18.25117\n",
      "[633/1000] train loss: 5.34387, val loss: 18.01389\n",
      "[634/1000] train loss: 5.30982, val loss: 17.92278\n",
      "[635/1000] train loss: 5.28205, val loss: 17.93253\n",
      "[636/1000] train loss: 5.30437, val loss: 17.98380\n",
      "[637/1000] train loss: 5.14297, val loss: 17.94767\n",
      "[638/1000] train loss: 5.33805, val loss: 18.25078\n",
      "[639/1000] train loss: 5.34973, val loss: 18.16250\n",
      "[640/1000] train loss: 5.26908, val loss: 17.90148\n",
      "[641/1000] train loss: 5.27317, val loss: 17.96685\n",
      "[642/1000] train loss: 5.31532, val loss: 17.90070\n",
      "[643/1000] train loss: 5.30160, val loss: 18.02154\n",
      "[644/1000] train loss: 5.35894, val loss: 18.05422\n",
      "[645/1000] train loss: 5.30263, val loss: 18.07302\n",
      "[646/1000] train loss: 5.26673, val loss: 17.91038\n",
      "[647/1000] train loss: 5.32628, val loss: 18.22505\n",
      "[648/1000] train loss: 5.30694, val loss: 17.86525\n",
      "[649/1000] train loss: 5.30474, val loss: 17.97499\n",
      "[650/1000] train loss: 5.17368, val loss: 17.84778\n",
      "[651/1000] train loss: 5.21672, val loss: 18.15296\n",
      "[652/1000] train loss: 5.32518, val loss: 18.15705\n",
      "[653/1000] train loss: 5.19082, val loss: 18.20649\n",
      "[654/1000] train loss: 5.22843, val loss: 17.86223\n",
      "[655/1000] train loss: 5.23859, val loss: 17.83887\n",
      "[656/1000] train loss: 5.26582, val loss: 17.90664\n",
      "[657/1000] train loss: 5.21724, val loss: 18.15151\n",
      "[658/1000] train loss: 5.20176, val loss: 17.98979\n",
      "[659/1000] train loss: 5.22956, val loss: 17.69622\n",
      "[660/1000] train loss: 5.21859, val loss: 17.80055\n",
      "[661/1000] train loss: 5.21023, val loss: 18.06671\n",
      "[662/1000] train loss: 5.19602, val loss: 17.99282\n",
      "[663/1000] train loss: 5.15620, val loss: 17.78642\n",
      "[664/1000] train loss: 5.18661, val loss: 17.85160\n",
      "[665/1000] train loss: 5.19643, val loss: 18.19052\n",
      "[666/1000] train loss: 5.37053, val loss: 18.18777\n",
      "[667/1000] train loss: 5.24583, val loss: 17.86997\n",
      "[668/1000] train loss: 5.13088, val loss: 17.77338\n",
      "[669/1000] train loss: 5.17771, val loss: 17.80004\n",
      "[670/1000] train loss: 5.08516, val loss: 17.97489\n",
      "[671/1000] train loss: 5.24016, val loss: 18.14329\n",
      "[672/1000] train loss: 5.24613, val loss: 18.17453\n",
      "[673/1000] train loss: 5.11863, val loss: 17.91268\n",
      "[674/1000] train loss: 5.14705, val loss: 18.11478\n",
      "[675/1000] train loss: 4.80755, val loss: 18.12479\n",
      "[676/1000] train loss: 5.06695, val loss: 17.85341\n",
      "[677/1000] train loss: 5.10593, val loss: 17.83432\n",
      "[678/1000] train loss: 5.11817, val loss: 17.77122\n",
      "[679/1000] train loss: 5.13732, val loss: 17.88854\n",
      "[680/1000] train loss: 5.12019, val loss: 17.84969\n",
      "[681/1000] train loss: 5.11592, val loss: 17.88625\n",
      "[682/1000] train loss: 5.01370, val loss: 17.82626\n",
      "[683/1000] train loss: 5.05537, val loss: 17.68132\n",
      "[684/1000] train loss: 5.08472, val loss: 18.01386\n",
      "[685/1000] train loss: 5.08636, val loss: 17.91498\n",
      "[686/1000] train loss: 5.01023, val loss: 17.90287\n",
      "[687/1000] train loss: 5.05623, val loss: 17.69408\n",
      "[688/1000] train loss: 5.06735, val loss: 17.86787\n",
      "[689/1000] train loss: 5.08040, val loss: 17.74174\n",
      "[690/1000] train loss: 5.07576, val loss: 17.94609\n",
      "[691/1000] train loss: 5.15728, val loss: 17.67277\n",
      "[692/1000] train loss: 4.99753, val loss: 17.90091\n",
      "[693/1000] train loss: 5.07293, val loss: 17.79246\n",
      "[694/1000] train loss: 5.10173, val loss: 17.63481\n",
      "[695/1000] train loss: 4.95682, val loss: 17.72771\n",
      "[696/1000] train loss: 5.02899, val loss: 17.62552\n",
      "[697/1000] train loss: 4.98485, val loss: 17.77989\n",
      "[698/1000] train loss: 4.91448, val loss: 17.92190\n",
      "[699/1000] train loss: 4.92826, val loss: 17.61358\n",
      "[700/1000] train loss: 5.04941, val loss: 17.86889\n",
      "[701/1000] train loss: 5.00443, val loss: 17.62248\n",
      "[702/1000] train loss: 4.99873, val loss: 17.75734\n",
      "[703/1000] train loss: 5.04836, val loss: 17.93784\n",
      "[704/1000] train loss: 4.98641, val loss: 17.94217\n",
      "[705/1000] train loss: 4.98358, val loss: 17.65630\n",
      "[706/1000] train loss: 4.94756, val loss: 17.82416\n",
      "[707/1000] train loss: 4.98278, val loss: 17.79272\n",
      "[708/1000] train loss: 5.01445, val loss: 17.79188\n",
      "[709/1000] train loss: 4.99022, val loss: 17.77203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[710/1000] train loss: 4.93367, val loss: 17.54746\n",
      "[711/1000] train loss: 4.95557, val loss: 17.91472\n",
      "[712/1000] train loss: 4.82547, val loss: 17.86888\n",
      "[713/1000] train loss: 4.88253, val loss: 17.55740\n",
      "[714/1000] train loss: 4.95272, val loss: 17.84825\n",
      "[715/1000] train loss: 5.03406, val loss: 17.60475\n",
      "[716/1000] train loss: 4.97201, val loss: 17.72867\n",
      "[717/1000] train loss: 4.92281, val loss: 17.84910\n",
      "[718/1000] train loss: 4.93288, val loss: 17.90830\n",
      "[719/1000] train loss: 4.98112, val loss: 17.80557\n",
      "[720/1000] train loss: 4.92837, val loss: 17.80976\n",
      "[721/1000] train loss: 5.02506, val loss: 18.15514\n",
      "[722/1000] train loss: 4.99658, val loss: 17.48354\n",
      "[723/1000] train loss: 5.00025, val loss: 17.58223\n",
      "[724/1000] train loss: 4.98544, val loss: 17.52023\n",
      "[725/1000] train loss: 4.87955, val loss: 17.66625\n",
      "[726/1000] train loss: 4.80672, val loss: 17.69999\n",
      "[727/1000] train loss: 4.89564, val loss: 17.67380\n",
      "[728/1000] train loss: 4.91108, val loss: 17.89026\n",
      "[729/1000] train loss: 4.87545, val loss: 17.65607\n",
      "[730/1000] train loss: 4.82215, val loss: 17.76332\n",
      "[731/1000] train loss: 4.86589, val loss: 17.78528\n",
      "[732/1000] train loss: 4.93674, val loss: 17.61440\n",
      "[733/1000] train loss: 4.89198, val loss: 17.48914\n",
      "[734/1000] train loss: 4.92297, val loss: 17.94012\n",
      "[735/1000] train loss: 4.84515, val loss: 17.81979\n",
      "[736/1000] train loss: 4.83260, val loss: 17.82316\n",
      "[737/1000] train loss: 4.83930, val loss: 17.80915\n",
      "[738/1000] train loss: 4.85810, val loss: 17.79885\n",
      "[739/1000] train loss: 4.84959, val loss: 17.70741\n",
      "[740/1000] train loss: 4.80120, val loss: 17.59663\n",
      "[741/1000] train loss: 4.83816, val loss: 17.69639\n",
      "[742/1000] train loss: 4.89915, val loss: 17.92085\n",
      "[743/1000] train loss: 4.83493, val loss: 17.62638\n",
      "[744/1000] train loss: 4.78751, val loss: 17.78695\n",
      "[745/1000] train loss: 4.81083, val loss: 17.68205\n",
      "[746/1000] train loss: 4.78127, val loss: 17.68538\n",
      "[747/1000] train loss: 4.80092, val loss: 17.46751\n",
      "[748/1000] train loss: 4.88378, val loss: 17.70602\n",
      "[749/1000] train loss: 4.81315, val loss: 17.87695\n",
      "[750/1000] train loss: 4.77725, val loss: 18.03201\n",
      "[751/1000] train loss: 4.83206, val loss: 17.60703\n",
      "[752/1000] train loss: 4.80567, val loss: 17.62939\n",
      "[753/1000] train loss: 4.80614, val loss: 17.88754\n",
      "[754/1000] train loss: 4.84336, val loss: 17.88457\n",
      "[755/1000] train loss: 4.76941, val loss: 17.70149\n",
      "[756/1000] train loss: 4.95352, val loss: 17.56896\n",
      "[757/1000] train loss: 4.83481, val loss: 17.62230\n",
      "[758/1000] train loss: 4.71708, val loss: 17.77273\n",
      "[759/1000] train loss: 4.72514, val loss: 17.73138\n",
      "[760/1000] train loss: 4.74121, val loss: 17.80836\n",
      "[761/1000] train loss: 4.73363, val loss: 17.69826\n",
      "[762/1000] train loss: 4.65590, val loss: 17.71832\n",
      "[763/1000] train loss: 4.73701, val loss: 17.56152\n",
      "[764/1000] train loss: 4.71358, val loss: 17.63234\n",
      "[765/1000] train loss: 4.73151, val loss: 17.78262\n",
      "[766/1000] train loss: 4.66236, val loss: 17.70266\n",
      "[767/1000] train loss: 4.81211, val loss: 17.40523\n",
      "[768/1000] train loss: 4.79122, val loss: 17.52001\n",
      "[769/1000] train loss: 4.71927, val loss: 17.79243\n",
      "[770/1000] train loss: 4.65496, val loss: 17.64997\n",
      "[771/1000] train loss: 4.73250, val loss: 17.64743\n",
      "[772/1000] train loss: 4.69519, val loss: 17.70371\n",
      "[773/1000] train loss: 4.71860, val loss: 17.79539\n",
      "[774/1000] train loss: 4.67921, val loss: 17.50995\n",
      "[775/1000] train loss: 4.65168, val loss: 17.68614\n",
      "[776/1000] train loss: 4.67261, val loss: 17.82836\n",
      "[777/1000] train loss: 4.72058, val loss: 17.94631\n",
      "[778/1000] train loss: 4.65010, val loss: 17.86185\n",
      "[779/1000] train loss: 4.66869, val loss: 17.62220\n",
      "[780/1000] train loss: 4.58634, val loss: 17.67006\n",
      "[781/1000] train loss: 4.71165, val loss: 17.46535\n",
      "[782/1000] train loss: 4.64286, val loss: 17.54884\n",
      "[783/1000] train loss: 4.62839, val loss: 17.66105\n",
      "[784/1000] train loss: 4.70628, val loss: 17.84343\n",
      "[785/1000] train loss: 4.62091, val loss: 17.78068\n",
      "[786/1000] train loss: 4.65456, val loss: 17.65153\n",
      "[787/1000] train loss: 4.48076, val loss: 17.56242\n",
      "[788/1000] train loss: 4.67885, val loss: 17.75738\n",
      "[789/1000] train loss: 4.60827, val loss: 17.81047\n",
      "[790/1000] train loss: 4.64711, val loss: 17.73442\n",
      "[791/1000] train loss: 4.64108, val loss: 17.53100\n",
      "[792/1000] train loss: 4.66791, val loss: 17.76111\n",
      "[793/1000] train loss: 4.66357, val loss: 17.70878\n",
      "[794/1000] train loss: 4.63239, val loss: 17.53627\n",
      "[795/1000] train loss: 4.66269, val loss: 17.85027\n",
      "[796/1000] train loss: 4.66921, val loss: 17.51970\n",
      "[797/1000] train loss: 4.64253, val loss: 17.67467\n",
      "[798/1000] train loss: 4.58386, val loss: 17.68568\n",
      "[799/1000] train loss: 4.44295, val loss: 17.79232\n",
      "[800/1000] train loss: 4.55233, val loss: 17.81133\n",
      "[801/1000] train loss: 4.42890, val loss: 17.84349\n",
      "[802/1000] train loss: 4.61964, val loss: 17.48864\n",
      "[803/1000] train loss: 4.60724, val loss: 17.82631\n",
      "[804/1000] train loss: 4.62820, val loss: 17.75220\n",
      "[805/1000] train loss: 4.62202, val loss: 17.70524\n",
      "[806/1000] train loss: 4.54754, val loss: 17.65514\n",
      "[807/1000] train loss: 4.63956, val loss: 17.50342\n",
      "[808/1000] train loss: 4.60069, val loss: 17.84945\n",
      "[809/1000] train loss: 4.60844, val loss: 17.49821\n",
      "[810/1000] train loss: 4.65997, val loss: 17.74426\n",
      "[811/1000] train loss: 4.54660, val loss: 17.59329\n",
      "[812/1000] train loss: 4.59782, val loss: 17.56271\n",
      "[813/1000] train loss: 4.60455, val loss: 17.55295\n",
      "[814/1000] train loss: 4.60113, val loss: 17.50947\n",
      "[815/1000] train loss: 4.60879, val loss: 17.80885\n",
      "[816/1000] train loss: 4.55947, val loss: 17.63306\n",
      "[817/1000] train loss: 4.36170, val loss: 17.75795\n",
      "[818/1000] train loss: 4.51820, val loss: 17.77651\n",
      "[819/1000] train loss: 4.48338, val loss: 17.68606\n",
      "[820/1000] train loss: 4.61932, val loss: 17.74327\n",
      "[821/1000] train loss: 4.56315, val loss: 17.69482\n",
      "[822/1000] train loss: 4.61104, val loss: 17.39098\n",
      "[823/1000] train loss: 4.56363, val loss: 17.82313\n",
      "[824/1000] train loss: 4.55921, val loss: 17.70696\n",
      "[825/1000] train loss: 4.54999, val loss: 17.55684\n",
      "[826/1000] train loss: 4.47698, val loss: 17.60564\n",
      "[827/1000] train loss: 4.52692, val loss: 17.70858\n",
      "[828/1000] train loss: 4.49216, val loss: 17.51486\n",
      "[829/1000] train loss: 4.60009, val loss: 17.40115\n",
      "[830/1000] train loss: 4.51128, val loss: 17.89579\n",
      "[831/1000] train loss: 4.49387, val loss: 17.44302\n",
      "[832/1000] train loss: 4.57470, val loss: 17.89956\n",
      "[833/1000] train loss: 4.51737, val loss: 17.36453\n",
      "[834/1000] train loss: 4.48506, val loss: 17.52345\n",
      "[835/1000] train loss: 4.41646, val loss: 17.53775\n",
      "[836/1000] train loss: 4.52305, val loss: 17.38843\n",
      "[837/1000] train loss: 4.45703, val loss: 17.59238\n",
      "[838/1000] train loss: 4.48203, val loss: 17.49628\n",
      "[839/1000] train loss: 4.49058, val loss: 17.56834\n",
      "[840/1000] train loss: 4.50168, val loss: 17.68667\n",
      "[841/1000] train loss: 4.51033, val loss: 17.37983\n",
      "[842/1000] train loss: 4.42695, val loss: 17.54416\n",
      "[843/1000] train loss: 4.46701, val loss: 17.72339\n",
      "[844/1000] train loss: 4.53360, val loss: 17.81060\n",
      "[845/1000] train loss: 4.45864, val loss: 17.42581\n",
      "[846/1000] train loss: 4.48461, val loss: 17.45023\n",
      "[847/1000] train loss: 4.52690, val loss: 17.94336\n",
      "[848/1000] train loss: 4.53263, val loss: 17.47811\n",
      "[849/1000] train loss: 4.42339, val loss: 17.27651\n",
      "[850/1000] train loss: 4.43722, val loss: 17.33469\n",
      "[851/1000] train loss: 4.50964, val loss: 17.41430\n",
      "[852/1000] train loss: 4.49311, val loss: 17.27442\n",
      "[853/1000] train loss: 4.38960, val loss: 17.64108\n",
      "[854/1000] train loss: 4.41898, val loss: 17.32981\n",
      "[855/1000] train loss: 4.46272, val loss: 17.31522\n",
      "[856/1000] train loss: 4.34495, val loss: 17.45226\n",
      "[857/1000] train loss: 4.36311, val loss: 17.43002\n",
      "[858/1000] train loss: 4.42484, val loss: 17.53046\n",
      "[859/1000] train loss: 4.44638, val loss: 17.38484\n",
      "[860/1000] train loss: 4.47641, val loss: 17.32433\n",
      "[861/1000] train loss: 4.45369, val loss: 17.75296\n",
      "[862/1000] train loss: 4.34855, val loss: 17.40249\n",
      "[863/1000] train loss: 4.43777, val loss: 17.38478\n",
      "[864/1000] train loss: 4.32405, val loss: 17.34369\n",
      "[865/1000] train loss: 4.35579, val loss: 17.56507\n",
      "[866/1000] train loss: 4.37021, val loss: 17.47236\n",
      "[867/1000] train loss: 4.39689, val loss: 17.34957\n",
      "[868/1000] train loss: 4.43813, val loss: 17.63183\n",
      "[869/1000] train loss: 4.41302, val loss: 17.45322\n",
      "[870/1000] train loss: 4.41509, val loss: 17.74134\n",
      "[871/1000] train loss: 4.43190, val loss: 17.83105\n",
      "[872/1000] train loss: 4.46603, val loss: 17.50732\n",
      "[873/1000] train loss: 4.35433, val loss: 17.32966\n",
      "[874/1000] train loss: 4.55901, val loss: 18.03333\n",
      "[875/1000] train loss: 4.50817, val loss: 17.70666\n",
      "[876/1000] train loss: 4.36448, val loss: 17.33658\n",
      "[877/1000] train loss: 4.37500, val loss: 17.46766\n",
      "[878/1000] train loss: 4.38032, val loss: 17.42952\n",
      "[879/1000] train loss: 4.37246, val loss: 17.30589\n",
      "[880/1000] train loss: 4.40628, val loss: 17.23524\n",
      "[881/1000] train loss: 4.36530, val loss: 17.58831\n",
      "[882/1000] train loss: 4.32307, val loss: 17.39831\n",
      "[883/1000] train loss: 4.35887, val loss: 17.53236\n",
      "[884/1000] train loss: 4.32846, val loss: 17.56370\n",
      "[885/1000] train loss: 4.28947, val loss: 17.47880\n",
      "[886/1000] train loss: 4.49070, val loss: 17.94684\n",
      "[887/1000] train loss: 4.34696, val loss: 17.50569\n",
      "[888/1000] train loss: 4.34121, val loss: 17.64598\n",
      "[889/1000] train loss: 4.31787, val loss: 17.47826\n",
      "[890/1000] train loss: 4.31914, val loss: 17.63145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[891/1000] train loss: 4.38095, val loss: 17.55646\n",
      "[892/1000] train loss: 4.34498, val loss: 17.37203\n",
      "[893/1000] train loss: 4.32068, val loss: 17.44959\n",
      "[894/1000] train loss: 4.34863, val loss: 17.59552\n",
      "[895/1000] train loss: 4.37120, val loss: 17.32354\n",
      "[896/1000] train loss: 4.33419, val loss: 17.32356\n",
      "[897/1000] train loss: 4.37318, val loss: 17.46570\n",
      "[898/1000] train loss: 4.26696, val loss: 17.63577\n",
      "[899/1000] train loss: 4.25540, val loss: 17.56383\n",
      "[900/1000] train loss: 4.30632, val loss: 17.27955\n",
      "[901/1000] train loss: 4.40951, val loss: 17.46954\n",
      "[902/1000] train loss: 4.31255, val loss: 17.55950\n",
      "[903/1000] train loss: 4.29405, val loss: 17.42506\n",
      "[904/1000] train loss: 4.30637, val loss: 17.58564\n",
      "[905/1000] train loss: 4.30498, val loss: 17.72968\n",
      "[906/1000] train loss: 4.36319, val loss: 17.61466\n",
      "[907/1000] train loss: 4.18989, val loss: 17.49767\n",
      "[908/1000] train loss: 4.27131, val loss: 17.35843\n",
      "[909/1000] train loss: 4.26067, val loss: 17.62847\n",
      "[910/1000] train loss: 4.14274, val loss: 17.47519\n",
      "[911/1000] train loss: 4.26328, val loss: 17.48304\n",
      "[912/1000] train loss: 4.26785, val loss: 17.52383\n",
      "[913/1000] train loss: 4.23263, val loss: 17.36261\n",
      "[914/1000] train loss: 4.26678, val loss: 17.44504\n",
      "[915/1000] train loss: 4.32384, val loss: 17.23163\n",
      "[916/1000] train loss: 4.31511, val loss: 17.25601\n",
      "[917/1000] train loss: 4.23287, val loss: 17.58570\n",
      "[918/1000] train loss: 4.30357, val loss: 17.45276\n",
      "[919/1000] train loss: 4.25266, val loss: 17.67891\n",
      "[920/1000] train loss: 4.22881, val loss: 17.42930\n",
      "[921/1000] train loss: 4.23090, val loss: 17.54314\n",
      "[922/1000] train loss: 4.26686, val loss: 17.44409\n",
      "[923/1000] train loss: 4.16093, val loss: 17.60541\n",
      "[924/1000] train loss: 4.24906, val loss: 17.63344\n",
      "[925/1000] train loss: 4.22050, val loss: 17.47974\n",
      "[926/1000] train loss: 4.23851, val loss: 17.58315\n",
      "[927/1000] train loss: 4.25506, val loss: 17.50917\n",
      "[928/1000] train loss: 4.21613, val loss: 17.56909\n",
      "[929/1000] train loss: 4.24111, val loss: 17.64659\n",
      "[930/1000] train loss: 4.24134, val loss: 17.33498\n",
      "[931/1000] train loss: 4.16843, val loss: 17.58594\n",
      "[932/1000] train loss: 4.22206, val loss: 17.62230\n",
      "[933/1000] train loss: 4.07672, val loss: 17.75270\n",
      "[934/1000] train loss: 4.19473, val loss: 17.65439\n",
      "[935/1000] train loss: 4.16435, val loss: 17.47747\n",
      "[936/1000] train loss: 4.16620, val loss: 17.38185\n",
      "[937/1000] train loss: 4.18810, val loss: 17.51024\n",
      "[938/1000] train loss: 4.21878, val loss: 17.65056\n",
      "[939/1000] train loss: 4.20249, val loss: 17.62008\n",
      "[940/1000] train loss: 4.19021, val loss: 17.36616\n",
      "[941/1000] train loss: 4.21669, val loss: 17.53196\n",
      "[942/1000] train loss: 4.20909, val loss: 17.71582\n",
      "[943/1000] train loss: 3.92111, val loss: 17.37554\n",
      "[944/1000] train loss: 4.20404, val loss: 17.75006\n",
      "[945/1000] train loss: 4.17538, val loss: 17.37386\n",
      "[946/1000] train loss: 4.17896, val loss: 17.61915\n",
      "[947/1000] train loss: 4.17579, val loss: 17.57938\n",
      "[948/1000] train loss: 4.28764, val loss: 17.73140\n",
      "[949/1000] train loss: 4.13012, val loss: 17.44086\n",
      "[950/1000] train loss: 4.21465, val loss: 17.31596\n",
      "[951/1000] train loss: 4.12219, val loss: 17.67963\n",
      "[952/1000] train loss: 4.16546, val loss: 17.45985\n",
      "[953/1000] train loss: 4.16432, val loss: 17.48217\n",
      "[954/1000] train loss: 4.31490, val loss: 17.35503\n",
      "[955/1000] train loss: 4.24643, val loss: 17.47541\n",
      "[956/1000] train loss: 4.23229, val loss: 17.92266\n",
      "[957/1000] train loss: 4.30001, val loss: 17.92135\n",
      "[958/1000] train loss: 4.15265, val loss: 17.40347\n",
      "[959/1000] train loss: 4.15189, val loss: 17.70381\n",
      "[960/1000] train loss: 4.09041, val loss: 17.79477\n",
      "[961/1000] train loss: 4.11718, val loss: 17.50306\n",
      "[962/1000] train loss: 4.21023, val loss: 17.50095\n",
      "[963/1000] train loss: 4.12708, val loss: 17.54913\n",
      "[964/1000] train loss: 4.14637, val loss: 17.75936\n",
      "[965/1000] train loss: 4.11804, val loss: 17.43821\n",
      "[966/1000] train loss: 4.13803, val loss: 17.51643\n",
      "[967/1000] train loss: 4.10226, val loss: 17.65303\n",
      "[968/1000] train loss: 4.11778, val loss: 17.50975\n",
      "[969/1000] train loss: 4.16378, val loss: 17.43848\n",
      "[970/1000] train loss: 4.13607, val loss: 17.47786\n",
      "[971/1000] train loss: 4.18795, val loss: 17.89352\n",
      "[972/1000] train loss: 4.13352, val loss: 17.59750\n",
      "[973/1000] train loss: 4.20084, val loss: 17.90506\n",
      "[974/1000] train loss: 4.07737, val loss: 17.73431\n",
      "[975/1000] train loss: 4.04271, val loss: 17.51854\n",
      "[976/1000] train loss: 4.12077, val loss: 17.73626\n",
      "[977/1000] train loss: 4.19350, val loss: 17.69159\n",
      "[978/1000] train loss: 4.11701, val loss: 17.36790\n",
      "[979/1000] train loss: 4.10097, val loss: 17.55414\n",
      "[980/1000] train loss: 4.11049, val loss: 17.45720\n",
      "[981/1000] train loss: 4.04272, val loss: 17.82932\n",
      "[982/1000] train loss: 4.13341, val loss: 17.74652\n",
      "[983/1000] train loss: 4.11364, val loss: 17.53071\n",
      "[984/1000] train loss: 4.07205, val loss: 17.62978\n",
      "[985/1000] train loss: 4.05887, val loss: 17.76987\n",
      "[986/1000] train loss: 4.14707, val loss: 17.92488\n",
      "[987/1000] train loss: 4.06845, val loss: 17.53437\n",
      "[988/1000] train loss: 4.07908, val loss: 17.69878\n",
      "[989/1000] train loss: 4.05757, val loss: 17.60165\n",
      "[990/1000] train loss: 4.14090, val loss: 17.50333\n",
      "[991/1000] train loss: 4.11000, val loss: 17.61003\n",
      "[992/1000] train loss: 4.07905, val loss: 17.78813\n",
      "[993/1000] train loss: 4.12483, val loss: 17.68288\n",
      "[994/1000] train loss: 4.12884, val loss: 17.83666\n",
      "[995/1000] train loss: 4.08103, val loss: 17.38956\n",
      "[996/1000] train loss: 4.06160, val loss: 17.61989\n",
      "[997/1000] train loss: 4.05050, val loss: 17.54354\n",
      "[998/1000] train loss: 4.09376, val loss: 17.51266\n",
      "[999/1000] train loss: 4.06640, val loss: 17.89757\n",
      "[1000/1000] train loss: 4.02671, val loss: 17.74675\n"
     ]
    }
   ],
   "source": [
    "# 학습: 두단계 - 학습 + 검증\n",
    "for epoch in range(N_EPOCH):\n",
    "    ################\n",
    "    # 학습 - 모델을 train 모드로 변경\n",
    "    ################\n",
    "    boston_model.train()\n",
    "    train_loss = 0.0\n",
    "    for X, y in boston_train_loader:\n",
    "        # X, y를 device로 이동\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # 1. 모델 추정\n",
    "        pred = boston_model(X) # 순전파(forward propagation)\n",
    "        # 2. loss 계산\n",
    "        loss = loss_fn(pred, y)  # 추정, 정답\n",
    "        # 3. 모델 파라미터를 업데이트\n",
    "        ## 3.1 파라미터들의 기울기를 초기화\n",
    "        optimizer.zero_grad()\n",
    "        ## 3.2 역전파(back propagration)을 해서 파라미터들의 기울기를 계산(grad속성에 저장)\n",
    "        loss.backward()\n",
    "        ## 3.3 파라미터 업데이트 처리.=> 1 step\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    #평균 loss\n",
    "    train_loss /= len(boston_train_loader)\n",
    "    # 1 epoch 학습끝\n",
    "    ###############################\n",
    "    # 검증 - 모델을 평가모드로 변경\n",
    "    ###############################\n",
    "    boston_model.eval() #evalutation mode 로 변환\n",
    "    val_loss = 0.0\n",
    "    # 역전파를 통한 gradient 계산이 필요 없기 때문에 일시적으로 grad_fn 을 구하지 않도록 처리.\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in boston_test_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            # 1.추정\n",
    "            pred_val = boston_model(X_val)\n",
    "            # 2. loss 계산\n",
    "            val_loss += loss_fn(pred_val, y_val).item()\n",
    "        val_loss /= len(boston_test_loader)\n",
    "    # epoch에 대한 검증 완료\n",
    "    # 결과 출력\n",
    "    print(f\"[{epoch+1}/{N_EPOCH}] train loss: {train_loss:.5f}, val loss: {val_loss:.5f}\")\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(N_EPOCH), train_loss_list, label=\"train loss\")\n",
    "plt.plot(range(N_EPOCH), val_loss_list, label=\"validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel('Loss')\n",
    "# plt.ylim(3, 30)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장\n",
    "\n",
    "## 모델 전체 저장 및 불러오기\n",
    "- 모델구조, 파라미터 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'models/boston_model.pt'\n",
    "torch.save(boston_model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_boston_model_1 = torch.load(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BostonModel                              [200, 1]                  --\n",
       "├─Linear: 1-1                            [200, 32]                 448\n",
       "├─Linear: 1-2                            [200, 16]                 528\n",
       "├─Linear: 1-3                            [200, 1]                  17\n",
       "==========================================================================================\n",
       "Total params: 993\n",
       "Trainable params: 993\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.20\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.08\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(load_boston_model_1, (200, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = 0.0\n",
    "load_boston_model_1.to(device)\n",
    "load_boston_model_1.eval()\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in boston_test_loader:\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "        # 1.추정\n",
    "        pred_val = load_boston_model_1(X_val)\n",
    "        # 2. loss 계산\n",
    "        val_loss += loss_fn(pred_val, y_val).item()\n",
    "    val_loss /= len(boston_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.74675178527832"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## state_dict 저장 및 로딩\n",
    "- 모델 파라미터만 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path2 = \"models/boston_state_dict.pt\"\n",
    "model_sd = boston_model.state_dict()\n",
    "\n",
    "torch.save(model_sd, save_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#state_dict 를 로딩 \n",
    "## 1. 모델객체 생성\n",
    "load_boston_model_2 = BostonModel().to(device)\n",
    "## 2. state_dict 불러오기\n",
    "load_sd = torch.load(save_path2)\n",
    "## 3. 불러온 state_dict(파라미터들)을 모델에 덮어 씌우기\n",
    "load_boston_model_2.load_state_dict(load_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.74675178527832"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss = 0.0\n",
    "load_boston_model_2.to(device)\n",
    "load_boston_model_2.eval() #평가모드 변환\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in boston_test_loader:\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "        # 1.추정\n",
    "        pred_val = load_boston_model_2(X_val)\n",
    "        # 2. loss 계산\n",
    "        val_loss += loss_fn(pred_val, y_val).item()\n",
    "    val_loss /= len(boston_test_loader)\n",
    "    \n",
    "val_loss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 분류 (Classification)\n",
    "\n",
    "## Fashion MNIST Dataset - 다중분류(Multi-Class Classification) 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "10개의 범주(category)와 70,000개의 흑백 이미지로 구성된 [패션 MNIST](https://github.com/zalandoresearch/fashion-mnist) 데이터셋. \n",
    "이미지는 해상도(28x28 픽셀)가 낮고 다음처럼 개별 의류 품목을 나타낸다:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>그림</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">패션-MNIST 샘플</a> (Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "이미지는 28x28 크기이며 Gray scale이다. *레이블*(label)은 0에서 9까지의 정수 배열이다. 아래 표는 이미지에 있는 의류의 **클래스**(class)들이다.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>레이블</th>\n",
    "    <th>클래스</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>T-shirt/top</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Trousers</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>2</td>\n",
    "    <td>Pullover</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>3</td>\n",
    "    <td>Dress</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>4</td>\n",
    "    <td>Coat</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>5</td>\n",
    "    <td>Sandal</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>6</td>\n",
    "    <td>Shirt</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>7</td>\n",
    "    <td>Sneaker</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>8</td>\n",
    "    <td>Bag</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>9</td>\n",
    "    <td>Ankle boot</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Dress', 'Pullover', 'Sandal'], dtype='<U11'), 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_class=np.array(['T-shirt/top','Trousers','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot'])\n",
    "class_to_index={key:value for value, key in enumerate(index_to_class)}\n",
    "index_to_class[[3,2,5]],class_to_index['Pullover']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to datasets\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 26421880/26421880 [00:05<00:00, 5176684.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to datasets\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to datasets\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 29515/29515 [00:00<00:00, 110443.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to datasets\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to datasets\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 4422102/4422102 [00:02<00:00, 1901393.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to datasets\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to datasets\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 5148/5148 [00:00<00:00, 5152058.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to datasets\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 전처리 하는 것을 합쳐준다.\n",
    "transform= transforms.Compose([\n",
    "    transforms.ToTensor(), # channel first 처리 0 ~ 1 scaling, torch.tensor 변환\n",
    "    transforms.Normalize(mean=0.5,std=0.5) # 표준화((pixcel-mean)/std) (-1~1)\n",
    "])\n",
    "\n",
    "# Dataset Loading\n",
    "fmnist_trainset=datasets.FashionMNIST(root='datasets',train=True,download=True,transform=transform)\n",
    "fmnist_testset=datasets.FashionMNIST(root='datasets',train=False,download=True,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: datasets\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=0.5, std=0.5)\n",
      "           )\n",
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: datasets\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=0.5, std=0.5)\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(fmnist_trainset)\n",
    "print(fmnist_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fmnist_trainset),len(fmnist_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_trainset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T-shirt/top': 0,\n",
       " 'Trouser': 1,\n",
       " 'Pullover': 2,\n",
       " 'Dress': 3,\n",
       " 'Coat': 4,\n",
       " 'Sandal': 5,\n",
       " 'Shirt': 6,\n",
       " 'Sneaker': 7,\n",
       " 'Bag': 8,\n",
       " 'Ankle boot': 9}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_trainset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "x,y=fmnist_trainset[0]\n",
    "print(y)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[0],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DataLoader 생성\n",
    "fmnist_train_loader = DataLoader(fmnist_trainset,batch_size=128,shuffle=True,drop_last=True)\n",
    "fmnist_test_loader = DataLoader(fmnist_testset,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1=nn.Linear(28*28,2048)\n",
    "        self.lr2=nn.Linear(2048,1024)\n",
    "        self.lr3=nn.Linear(1024,512)\n",
    "        self.lr4=nn.Linear(512,256)\n",
    "        self.lr5=nn.Linear(256,128)\n",
    "        self.lr6=nn.Linear(128,64)\n",
    "        self.output=nn.Linear(64,10) # out_feature : 10 - 10개 class별 확률\n",
    "        \n",
    "    def forward(self,X):\n",
    "        out=nn.Flatten()(X)\n",
    "        # torch.flatten하면 dim 지정해줘야함 - 그냥 flatten은 layer형이라 자동적으로 1번부터 진행\n",
    "        \n",
    "        out=nn.ReLU()(self.lr1(out))\n",
    "        out=nn.ReLU()(self.lr2(out))\n",
    "        out=nn.ReLU()(self.lr3(out))\n",
    "        out=nn.ReLU()(self.lr4(out))\n",
    "        out=nn.ReLU()(self.lr5(out))\n",
    "        out=nn.ReLU()(self.lr6(out))\n",
    "        out=self.output(out)\n",
    "        # nn.Softmax()(out)\n",
    "        # 다중 분류의 output은 Softmax()함수로 계산해서 확률로 만들어서 출력해야한다.\n",
    "        # 모델에서는 Linear를 통과한 결과를 반환\n",
    "        # Loss함수인 CrossEntropyLoss()에서 softmax를 적용한다.\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModel(\n",
       "  (lr1): Linear(in_features=784, out_features=2048, bias=True)\n",
       "  (lr2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (lr3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (lr4): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (lr5): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (lr6): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_model = FashionMNISTModel()\n",
    "f_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FashionMNISTModel                        [128, 10]                 --\n",
       "├─Linear: 1-1                            [128, 2048]               1,607,680\n",
       "├─Linear: 1-2                            [128, 1024]               2,098,176\n",
       "├─Linear: 1-3                            [128, 512]                524,800\n",
       "├─Linear: 1-4                            [128, 256]                131,328\n",
       "├─Linear: 1-5                            [128, 128]                32,896\n",
       "├─Linear: 1-6                            [128, 64]                 8,256\n",
       "├─Linear: 1-7                            [128, 10]                 650\n",
       "==========================================================================================\n",
       "Total params: 4,403,786\n",
       "Trainable params: 4,403,786\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 563.68\n",
       "==========================================================================================\n",
       "Input size (MB): 0.40\n",
       "Forward/backward pass size (MB): 4.14\n",
       "Params size (MB): 17.62\n",
       "Estimated Total Size (MB): 22.16\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(f_model,(128,1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n",
      "tensor([-0.0286,  0.1127,  0.0936,  0.0831,  0.0533, -0.0096,  0.0443, -0.0369,\n",
      "         0.0226, -0.0096], grad_fn=<SelectBackward0>)\n",
      "0.324945867061615 1\n"
     ]
    }
   ],
   "source": [
    "# 추정\n",
    "i=torch.ones((2,1,28,28),dtype=torch.float32) # 2는 batch size, 사진 2개를 넣어서 추론해달라는 것\n",
    "\n",
    "#i.shape\n",
    "y_hat=f_model(i)\n",
    "print(y_hat.shape)\n",
    "print(y_hat[0])\n",
    "print(y_hat[0].sum().item(),y_hat[0].argmax(dim=-1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0940, 0.1082, 0.1062, 0.1051, 0.1020, 0.0958, 0.1011, 0.0932, 0.0989,\n",
      "        0.0958], grad_fn=<SoftmaxBackward0>)\n",
      "0.9999998807907104 1\n"
     ]
    }
   ],
   "source": [
    "# 모델이 추정한 결과의 class를 알고 싶을 경우는 Softmax를 계산할 필요 없다.\n",
    "# 모델의 추정 확률을 알고 싶을 경우 Softmax를 계산한다.\n",
    "y_hat2=nn.Softmax(dim=-1)(y_hat[0])\n",
    "print(y_hat2)\n",
    "print(y_hat2.sum().item(), y_hat2.argmax(dim=-1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1000] train loss: 0.58358 val loss : 0.49169 val accuracy : 0.83460\n",
      "저장 : 1 epoch - prev_best_score: inf,present_best_score : 0.49168987817402127\n",
      "[2/1000] train loss: 0.40287 val loss : 0.42493 val accuracy : 0.84610\n",
      "저장 : 2 epoch - prev_best_score: 0.49168987817402127,present_best_score : 0.4249264986454686\n",
      "[3/1000] train loss: 0.36198 val loss : 0.38489 val accuracy : 0.86240\n",
      "저장 : 3 epoch - prev_best_score: 0.4249264986454686,present_best_score : 0.3848868478325349\n",
      "[4/1000] train loss: 0.32793 val loss : 0.37190 val accuracy : 0.86790\n",
      "저장 : 4 epoch - prev_best_score: 0.3848868478325349,present_best_score : 0.3718956767380992\n",
      "[5/1000] train loss: 0.30652 val loss : 0.38435 val accuracy : 0.86710\n",
      "[6/1000] train loss: 0.29065 val loss : 0.34660 val accuracy : 0.87580\n",
      "저장 : 6 epoch - prev_best_score: 0.3718956767380992,present_best_score : 0.3465968377982514\n",
      "[7/1000] train loss: 0.27307 val loss : 0.34996 val accuracy : 0.87510\n",
      "[8/1000] train loss: 0.25691 val loss : 0.37740 val accuracy : 0.87070\n",
      "[9/1000] train loss: 0.24756 val loss : 0.37205 val accuracy : 0.87170\n",
      "[10/1000] train loss: 0.23535 val loss : 0.35140 val accuracy : 0.88100\n",
      "[11/1000] train loss: 0.22158 val loss : 0.33305 val accuracy : 0.88850\n",
      "저장 : 11 epoch - prev_best_score: 0.3465968377982514,present_best_score : 0.33304676460691646\n",
      "[12/1000] train loss: 0.21121 val loss : 0.34061 val accuracy : 0.88550\n",
      "[13/1000] train loss: 0.20166 val loss : 0.35670 val accuracy : 0.88690\n",
      "[14/1000] train loss: 0.19113 val loss : 0.33319 val accuracy : 0.89360\n",
      "[15/1000] train loss: 0.18166 val loss : 0.35210 val accuracy : 0.88190\n",
      "[16/1000] train loss: 0.17398 val loss : 0.37441 val accuracy : 0.88560\n",
      "[17/1000] train loss: 0.16276 val loss : 0.35605 val accuracy : 0.89250\n",
      "[18/1000] train loss: 0.15962 val loss : 0.37083 val accuracy : 0.89350\n",
      "[19/1000] train loss: 0.14714 val loss : 0.38837 val accuracy : 0.88960\n",
      "[20/1000] train loss: 0.14029 val loss : 0.40328 val accuracy : 0.88720\n",
      "[21/1000] train loss: 0.13707 val loss : 0.40629 val accuracy : 0.88780\n",
      "조기종료 : epoch-21. 0.33305에서 개선x\n",
      "학습에 걸린시간 :  804.109589099884\n"
     ]
    }
   ],
   "source": [
    "#### 학습\n",
    "import time\n",
    "\n",
    "# 모델 생성 + device 이동\n",
    "fmnist_model=FashionMNISTModel().to(device)\n",
    "# loss -> 다중분류: CrossEntropyLoss()\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer=torch.optim.Adam(fmnist_model.parameters(),lr=0.001)\n",
    "\n",
    "# 결과저장할 list\n",
    "train_loss_list=[]\n",
    "val_loss_list=[]\n",
    "val_acc_list=[]\n",
    "\n",
    "##################\n",
    "# 가장 성능 좋은 epoch의 모델을 학습도중 저장. -> 성능이 개선될때 마다 저장\n",
    "# 필요한 변수들 정의\n",
    "##################\n",
    "\n",
    "best_score=torch.inf # 학습 중 가장 좋은 평가지표(val_loss)를 저장\n",
    "save_model_path= \"models/fashion_mnist_best_model.pt\"\n",
    "\n",
    "##################\n",
    "# 조기종료 (Early Stopping) - 특정 epoch동안 성능 개선이 없으면 학습 종료\n",
    "##################\n",
    "patiencee = 10  # 성능 개선 여부를 몇 epoch동안 확인 할 것인지\n",
    "trigger_cnt=0 # 몇 epoch째 성능개선을 기다리는지 저장할 변수\n",
    "\n",
    "\n",
    "N_EPOCH=1000\n",
    "s=time.time()\n",
    "\n",
    "for epoch in range(N_EPOCH): # 10번만 확인\n",
    "    ####### 학습\n",
    "    fmnist_model.train()\n",
    "    train_loss=0.0\n",
    "    for X,y in fmnist_train_loader:\n",
    "        X,y=X.to(device),y.to(device)\n",
    "        \n",
    "        pred=fmnist_model(X)\n",
    "        loss=loss_fn(pred,y) # pred: Softmax , y: Ohe 처리해준다.\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss/=len(fmnist_train_loader) # 평균 loss 계산\n",
    "    ### 1 epoch 학습종료\n",
    "        \n",
    "    ########## 검증\n",
    "    fmnist_model.eval()\n",
    "    val_loss=0.0\n",
    "    val_acc=0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in fmnist_test_loader: ## 검증한 부분\n",
    "            X_val,y_val = X_val.to(device), y_val.to(device)\n",
    "            \n",
    "            pred_val = fmnist_model(X_val) # Softmax 적용 전. -> loss는 이값으로 계산\n",
    "            pred_label= pred_val.argmax(dim=-1) # accuracy 계산\n",
    "            \n",
    "            \n",
    "            # val-loss\n",
    "            loss_val = loss_fn(pred_val,y_val)\n",
    "            val_loss += loss_val.item()\n",
    "            \n",
    "            # val-accuracy\n",
    "            val_acc += torch.sum(pred_label == y_val).item()\n",
    "            ## 현재 batch에서 맞은 것의 개수 누적계산\n",
    "    \n",
    "    # val_loss, val_acc의 평균\n",
    "    val_loss /= len(fmnist_test_loader) # step수로 나누어준다.\n",
    "    val_acc /= len(fmnist_test_loader.dataset) # 총 data 개수로 나누어준다.\n",
    "    \n",
    "    # 현재 epoch에 대한 학습, 검증 종료\n",
    "    print(f'[{epoch+1}/{N_EPOCH}] train loss: {train_loss:.5f} val loss : {val_loss:.5f} val accuracy : {val_acc:.5f}')\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "    \n",
    "    ##########################################################\n",
    "    ## 조기종료, 모델 저장\n",
    "    ## 현 epoch의 val_loss가 best_score보다 개선된 경우(작은 경우)\n",
    "    ##########################################################\n",
    "    if val_loss < best_score:\n",
    "        # 저장/ 조기종료\n",
    "        print(f\"저장 : {epoch+1} epoch - prev_best_score: {best_score},present_best_score : {val_loss}\")\n",
    "        best_score=val_loss\n",
    "        torch.save(fmnist_model, save_model_path)\n",
    "        trigger_cnt=0\n",
    "        \n",
    "    else:\n",
    "        # 저장 x / trigger_cnt 증가\n",
    "        trigger_cnt+=1\n",
    "        if patience==trigger_cnt:\n",
    "            print(f\"조기종료 : epoch-{epoch+1}. {best_score:.5f}에서 개선x\")\n",
    "            break\n",
    "\n",
    "e=time.time()\n",
    "\n",
    "print(\"학습에 걸린시간 : \",e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 분 24 초\n",
      "val_loss:0.4062930737869649, val_acc:0.8878\n"
     ]
    }
   ],
   "source": [
    "print(int((e-s)//60),\"분\",int((e-s)%60),\"초\")\n",
    "print(f\"val_loss:{val_loss}, val_acc:{val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#결과 시각화\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_loss_list,label='train loss')\n",
    "plt.plot(val_loss_list,label='val loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(val_acc_list)\n",
    "plt.title(\"validation accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set을 따로 만들어야하지만 그렇지 않아서 그냥 같은 dataset으로 간단한 검증\n",
    "fmnist_load_model = torch.load(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = 0.0, 0.0\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in fmnist_test_loader:\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "\n",
    "        pred_val = fmnist_load_model(X_val)  # Softmax 적용 전. -> loss는 이 값으로 계산.\n",
    "        pred_label = pred_val.argmax(dim=-1)  # accuracy  계산은 이 값으로 한다.\n",
    "\n",
    "        # val-loss\n",
    "        loss_val = loss_fn(pred_val, y_val)\n",
    "        val_loss += loss_val.item()\n",
    "        # val-accuracy\n",
    "        val_acc += torch.sum(pred_label == y_val).item()  #현 배치에서 맞은 것의 개수\n",
    "    # val_loss, val_acc의 평균\n",
    "    val_loss /= len(fmnist_test_loader)  # step 수로 나눔.\n",
    "    val_acc /= len(fmnist_test_loader.dataset) # 총 데이터 개수로 나눔.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.33304676460691646, 0.8885)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 위스콘신 유방암 데이터셋 - 이진분류(Binary Classification) 문제\n",
    "\n",
    "- **이진 분류 문제 처리 모델의 두가지 방법**\n",
    "    1. positive(1)일 확률을 출력하도록 구현\n",
    "        - output layer: units=1, activation='sigmoid'\n",
    "        - loss: binary_crossentropy\n",
    "    2. negative(0)일 확률과 positive(1)일 확률을 출력하도록 구현 => 다중분류 처리 방식으로 해결\n",
    "        - output layer: units=2, activation='softmax', y(정답)은 one hot encoding 처리\n",
    "        - loss: categorical_crossentropy\n",
    "        \n",
    "- 위스콘신 대학교에서 제공한 종양의 악성/양성여부 분류를 위한 데이터셋\n",
    "- Feature\n",
    "    - 종양에 대한 다양한 측정값들\n",
    "- Target의 class\n",
    "    - 0 - malignant(악성종양)\n",
    "    - 1 - benign(양성종양)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
