{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장\n",
    "\n",
    "- 학습한 모델을 저장장치에 파일로 저장하고 나중에 불러와 사용(추가 학습, 예측 서비스) 할 수 있도록 한다. \n",
    "- 파이토치는 모델의 파라미터만 저장하는 방법과 모델 구조와 파라미터 모두를 저장하는 두가지 방식을 제공한다.\n",
    "- 저장 함수\n",
    "    - `torch.save(저장할 객체, 저장경로)`\n",
    "- 보통 저장파일의 확장자는 `pt`나 `pth` 를 지정한다.\n",
    "\n",
    "## 모델 전체 저장하기 및 불러오기\n",
    "\n",
    "- 저장하기\n",
    "    - `torch.save(model, 저장경로)`\n",
    "- 불러오기\n",
    "    - `load_model = torch.load(저장경로)`\n",
    "- 저장시 **pickle**을 이용해 직렬화하기 때문에 불어오는 실행환경에도 모델을 저장할 때 사용한 클래스가 있어야 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 모델의 파라미터만 저장\n",
    "- 모델을 구성하는 파라미터만 저장한다.\n",
    "- 모델의 구조는 저장하지 않기 때문에 불러올 때 **모델을 먼저 생성하고 생성한 모델에 불러온 파라미터를 덮어씌운다.**\n",
    "- 모델의 파라미터는 **state_dict** 형식으로 저장한다.\n",
    "\n",
    "### state_dict\n",
    "- 모델의 파라미터 Tensor들을 레이어 단위별로 나누어 저장한 Ordered Dictionary (OrderedDict)\n",
    "- `모델객체.state_dict()` 메소드를 이용해 조회한다.\n",
    "- 모델의 state_dict을 조회 후 저장한다.\n",
    "    - `torch.save(model.state_dict(), \"저장경로\")`\n",
    "- 생성된 모델에 읽어온 state_dict를 덮어씌운다.\n",
    "    - `new_model.load_state_dict(torch.load(\"state_dict저장경로\"))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_layer = nn.Linear(784, 64)\n",
    "        self.out_layer = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = torch.flatten(X, start_dim = 1)\n",
    "        X = nn.ReLU()(self.in_layer(X))\n",
    "        X = self.out(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (in_layer): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (out_layer): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model = Network()\n",
    "sample_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n"
     ]
    }
   ],
   "source": [
    "# 모델의 state_dict 조회\n",
    "sd = sample_model.state_dict()  # sd가 저장된다.\n",
    "print(type(sd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['in_layer.weight', 'in_layer.bias', 'out_layer.weight', 'out_layer.bias'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd.keys()  # 각 layer의 parameter(weight, bias)가 조회된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 784]), torch.Size([64]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd['in_layer.weight'].shape, sd['in_layer.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001925D9D8AC0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=64, bias=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model.in_layer  #  __init__()에 self.in_layer = nn.Linear(784, 64)로 저장된 객체가 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0242,  0.0991, -0.0378,  0.0633, -0.0149,  0.0216,  0.0109, -0.0429,\n",
       "         0.0044,  0.1147], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 개별 조회\n",
    "sample_model.in_layer.weight\n",
    "sample_model.out_layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint를 저장 및 불러오기\n",
    "- 학습이 끝나지 않은 모델을 저장 후 나중에 이어서 학습시킬 경우에는 모델의 구조, 파라미터 뿐만 아니라 optimizer, loss 함수등 학습에 필요한 객체들을 저장해야 한다.\n",
    "- Dictionary에 필요한 요소들을 key-value 쌍으로 저장후 `torch.save()`를 이용해 저장한다.\n",
    "```python\n",
    "# 저장\n",
    "torch.save({\n",
    "    'epoch':epoch,\n",
    "    'model_state_dict':model.state_dict(),\n",
    "    'optimizer_state_dict':optimizer.state_dict(),\n",
    "    'loss':train_loss\n",
    "}, \"저장경로\")\n",
    "\n",
    "# 불러오기\n",
    "model = MyModel()\n",
    "optimizer = optim.Adam(model.parameter())\n",
    "\n",
    "checkpoint = torch.load(\"저장경로\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "#### 이어학습\n",
    "model.train()\n",
    "#### 추론\n",
    "model.eval()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 문제 유형별 MLP 네트워크\n",
    "- MLP(Multi Layer Perceptron)\n",
    "    - Fully Connected Layer로 구성된 네트워크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Regression(회귀)\n",
    "\n",
    "## Boston Housing Dataset\n",
    "보스턴 주택가격 dataset은 다음과 같은 속성을 바탕으로 해당 타운 주택 가격의 중앙값을 예측하는 문제.\n",
    "- CRIM: 범죄율\n",
    "- ZN: 25,000 평방피트당 주거지역 비율\n",
    "- INDUS: 비소매 상업지구 비율\n",
    "- CHAS: 찰스강에 인접해 있는지 여부(인접:1, 아니면:0)\n",
    "- NOX: 일산화질소 농도(단위: 0.1ppm)\n",
    "- RM: 주택당 방의 수\n",
    "- AGE: 1940년 이전에 건설된 주택의 비율\n",
    "- DIS: 5개의 보스턴 직업고용센터와의 거리(가중 평균)\n",
    "- RAD: 고속도로 접근성\n",
    "- TAX: 재산세율\n",
    "- PTRATIO: 학생/교사 비율\n",
    "- B: 흑인 비율\n",
    "- LSTAT: 하위 계층 비율\n",
    "<br><br>\n",
    "- **Target**\n",
    "    - MEDV: 타운의 주택가격 중앙값(단위: 1,000달러)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchinfo\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset, DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston = pd.read_csv('boston_hosing.csv')\n",
    "boston.shape\n",
    "\n",
    "X_boston = boston.drop(columns = 'MEDV').values\n",
    "y_boston = boston['MEDV'].values.reshape(-1, 1)  # 2차원 변경\n",
    "X_boston.shape, y_boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (102, 13))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train / test set 분리 => 회귀임으로 stratify 설정 안함\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_boston, y_boston, test_size = 0.2, random_state = 0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = torch.tensor(scaler.fit_transform(X_train), dtype = torch.float32)\n",
    "X_test_scaled = torch.tensor(scaler.transform(X_test), dtype = torch.float32)\n",
    "\n",
    "# y를 tensor 타입으로 변환\n",
    "y_train_tensor = torch.tensor(y_train, dtype = torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 102\n",
      "(tensor([-0.3726, -0.4996, -0.7049,  3.6645, -0.4249,  0.9357,  0.6937, -0.4372,\n",
      "        -0.1622, -0.5617, -0.4846,  0.3717, -0.4110]), tensor([26.7000]))\n",
      "2 1\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "boston_train_set = TensorDataset(X_train_scaled, y_train_tensor)\n",
    "boston_test_set = TensorDataset(X_test_scaled, y_test_tensor)\n",
    "print(len(boston_train_set), len(boston_test_set))\n",
    "print(boston_train_set[0])\n",
    "\n",
    "# DataLoader\n",
    "boston_train_loader = DataLoader(boston_train_set, batch_size = 200, shuffle = True, drop_last = True)\n",
    "boston_test_loader = DataLoader(boston_test_set, batch_size = len(boston_test_set))\n",
    "print(len(boston_train_loader), len(boston_test_loader))  # 1 epoch당 step 수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 입력 layer => in_feature: input data의 feature개수에 맞춘다.\n",
    "        self.lr1 = nn.Linear(13, 32)\n",
    "        \n",
    "        # hidden layder => in_feature: 앞 layer의 out_feature 개수에 맞춘다.\n",
    "        self.lr2 = nn.Linear(32, 16)\n",
    "        \n",
    "        # output layer => out_feature: 모델의 최종 출력 개수에 맞춘다.(MEDV 1개)\n",
    "        self.lr3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # input layer\n",
    "        out = self.lr1(X)\n",
    "        out = nn.ReLU()(out)\n",
    "        \n",
    "        # hidden layer\n",
    "        out = self.lr2(out)\n",
    "        out = nn.ReLU()(out)\n",
    "        \n",
    "        # output layer -> 회귀처리 모델에서 output layer에서 활성함수 적용 x\n",
    "        # 예외 : 출력결과가 특정 활성함수의 출력과 매칭될 경우.\n",
    "        ## 예를 들면, output의 범위가 0 ~ 1: Logistic 함수 사용\n",
    "        ##            output의 범위가 -1 ~ 1: tanh 사용\n",
    "        out = self.lr3(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 13])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(boston_train_loader))[0].shape  # 200개에 대한 데이터가 MEDV를 추론함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BostonModel                              [200, 1]                  --\n",
       "├─Linear: 1-1                            [200, 32]                 448\n",
       "├─Linear: 1-2                            [200, 16]                 528\n",
       "├─Linear: 1-3                            [200, 1]                  17\n",
       "==========================================================================================\n",
       "Total params: 993\n",
       "Trainable params: 993\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.20\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.08\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_model = BostonModel()\n",
    "torchinfo.summary(boston_model, (200, 13))  # (모델, 입력데이터(batch_size, feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 1000\n",
    "lr = 0.001\n",
    "\n",
    "# 결과 저장할 리스트를 생성\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "# 모델, loss함수(회귀-mse계산), optimizer 정의\n",
    "boston_model = boston_model.to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.RMSprop(boston_model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1000] train loss: 606.24750, val loss: 574.59967\n",
      "[2/1000] train loss: 593.93195, val loss: 564.97314\n",
      "[3/1000] train loss: 582.61966, val loss: 553.65619\n",
      "[4/1000] train loss: 571.54111, val loss: 541.17767\n",
      "[5/1000] train loss: 558.37988, val loss: 527.50879\n",
      "[6/1000] train loss: 534.09482, val loss: 512.95410\n",
      "[7/1000] train loss: 524.59789, val loss: 497.47400\n",
      "[8/1000] train loss: 507.75020, val loss: 481.20105\n",
      "[9/1000] train loss: 488.74863, val loss: 464.33896\n",
      "[10/1000] train loss: 471.60420, val loss: 446.97705\n",
      "[11/1000] train loss: 451.91350, val loss: 429.40009\n",
      "[12/1000] train loss: 431.54109, val loss: 411.67908\n",
      "[13/1000] train loss: 414.43231, val loss: 393.84155\n",
      "[14/1000] train loss: 393.04822, val loss: 376.17020\n",
      "[15/1000] train loss: 375.78958, val loss: 358.65353\n",
      "[16/1000] train loss: 356.89714, val loss: 341.49310\n",
      "[17/1000] train loss: 332.30133, val loss: 325.00479\n",
      "[18/1000] train loss: 317.12538, val loss: 309.00146\n",
      "[19/1000] train loss: 296.52827, val loss: 293.45694\n",
      "[20/1000] train loss: 279.12460, val loss: 278.53598\n",
      "[21/1000] train loss: 263.10225, val loss: 264.34985\n",
      "[22/1000] train loss: 251.40068, val loss: 250.87517\n",
      "[23/1000] train loss: 235.52809, val loss: 238.16927\n",
      "[24/1000] train loss: 220.85283, val loss: 226.33990\n",
      "[25/1000] train loss: 207.79408, val loss: 215.32428\n",
      "[26/1000] train loss: 198.06083, val loss: 204.94608\n",
      "[27/1000] train loss: 187.42814, val loss: 195.25369\n",
      "[28/1000] train loss: 176.33363, val loss: 186.26312\n",
      "[29/1000] train loss: 165.88030, val loss: 177.99449\n",
      "[30/1000] train loss: 158.00389, val loss: 170.34680\n",
      "[31/1000] train loss: 149.14320, val loss: 163.26894\n",
      "[32/1000] train loss: 140.50500, val loss: 156.75119\n",
      "[33/1000] train loss: 133.75209, val loss: 150.74998\n",
      "[34/1000] train loss: 127.47835, val loss: 145.18394\n",
      "[35/1000] train loss: 121.33306, val loss: 140.00998\n",
      "[36/1000] train loss: 116.00809, val loss: 135.21820\n",
      "[37/1000] train loss: 107.73889, val loss: 130.82713\n",
      "[38/1000] train loss: 106.55950, val loss: 126.62605\n",
      "[39/1000] train loss: 101.56965, val loss: 122.71446\n",
      "[40/1000] train loss: 96.60590, val loss: 119.04282\n",
      "[41/1000] train loss: 93.92518, val loss: 115.57637\n",
      "[42/1000] train loss: 90.43144, val loss: 112.28866\n",
      "[43/1000] train loss: 86.98444, val loss: 109.23379\n",
      "[44/1000] train loss: 84.07089, val loss: 106.32939\n",
      "[45/1000] train loss: 80.54770, val loss: 103.61562\n",
      "[46/1000] train loss: 77.74565, val loss: 101.03757\n",
      "[47/1000] train loss: 74.98170, val loss: 98.60738\n",
      "[48/1000] train loss: 72.64338, val loss: 96.29611\n",
      "[49/1000] train loss: 70.58176, val loss: 94.09375\n",
      "[50/1000] train loss: 67.76229, val loss: 92.01594\n",
      "[51/1000] train loss: 65.45694, val loss: 90.06033\n",
      "[52/1000] train loss: 63.84227, val loss: 88.17571\n",
      "[53/1000] train loss: 61.72123, val loss: 86.40530\n",
      "[54/1000] train loss: 60.17866, val loss: 84.71238\n",
      "[55/1000] train loss: 58.79269, val loss: 83.07854\n",
      "[56/1000] train loss: 56.87431, val loss: 81.54206\n",
      "[57/1000] train loss: 55.87511, val loss: 80.04768\n",
      "[58/1000] train loss: 54.07357, val loss: 78.63427\n",
      "[59/1000] train loss: 52.97664, val loss: 77.29250\n",
      "[60/1000] train loss: 51.31939, val loss: 75.98906\n",
      "[61/1000] train loss: 50.02452, val loss: 74.74945\n",
      "[62/1000] train loss: 48.80524, val loss: 73.58179\n",
      "[63/1000] train loss: 47.45614, val loss: 72.46674\n",
      "[64/1000] train loss: 46.73638, val loss: 71.38765\n",
      "[65/1000] train loss: 45.80059, val loss: 70.38008\n",
      "[66/1000] train loss: 44.79721, val loss: 69.40131\n",
      "[67/1000] train loss: 43.47256, val loss: 68.48281\n",
      "[68/1000] train loss: 42.70671, val loss: 67.59125\n",
      "[69/1000] train loss: 41.97274, val loss: 66.71854\n",
      "[70/1000] train loss: 41.17939, val loss: 65.88310\n",
      "[71/1000] train loss: 40.51367, val loss: 65.09350\n",
      "[72/1000] train loss: 39.41307, val loss: 64.34813\n",
      "[73/1000] train loss: 38.61066, val loss: 63.63297\n",
      "[74/1000] train loss: 38.33850, val loss: 62.93079\n",
      "[75/1000] train loss: 37.38294, val loss: 62.24310\n",
      "[76/1000] train loss: 37.00926, val loss: 61.60326\n",
      "[77/1000] train loss: 36.42288, val loss: 60.99078\n",
      "[78/1000] train loss: 35.20528, val loss: 60.40550\n",
      "[79/1000] train loss: 34.93096, val loss: 59.85275\n",
      "[80/1000] train loss: 34.72739, val loss: 59.30464\n",
      "[81/1000] train loss: 34.10870, val loss: 58.78493\n",
      "[82/1000] train loss: 33.78489, val loss: 58.27962\n",
      "[83/1000] train loss: 33.32585, val loss: 57.78491\n",
      "[84/1000] train loss: 32.77081, val loss: 57.32976\n",
      "[85/1000] train loss: 32.40735, val loss: 56.86896\n",
      "[86/1000] train loss: 31.28216, val loss: 56.47302\n",
      "[87/1000] train loss: 31.53684, val loss: 56.04169\n",
      "[88/1000] train loss: 31.22519, val loss: 55.64790\n",
      "[89/1000] train loss: 30.82772, val loss: 55.25589\n",
      "[90/1000] train loss: 30.62204, val loss: 54.87999\n",
      "[91/1000] train loss: 30.24829, val loss: 54.50605\n",
      "[92/1000] train loss: 29.80847, val loss: 54.14950\n",
      "[93/1000] train loss: 29.76494, val loss: 53.80354\n",
      "[94/1000] train loss: 29.21881, val loss: 53.46419\n",
      "[95/1000] train loss: 29.15068, val loss: 53.14396\n",
      "[96/1000] train loss: 28.53856, val loss: 52.82859\n",
      "[97/1000] train loss: 28.59328, val loss: 52.52582\n",
      "[98/1000] train loss: 26.83892, val loss: 52.30788\n",
      "[99/1000] train loss: 27.72670, val loss: 52.02481\n",
      "[100/1000] train loss: 27.90977, val loss: 51.73990\n",
      "[101/1000] train loss: 27.41143, val loss: 51.44967\n",
      "[102/1000] train loss: 26.64643, val loss: 51.15864\n",
      "[103/1000] train loss: 26.93843, val loss: 50.88690\n",
      "[104/1000] train loss: 27.00019, val loss: 50.62650\n",
      "[105/1000] train loss: 26.39050, val loss: 50.36668\n",
      "[106/1000] train loss: 26.05774, val loss: 50.13219\n",
      "[107/1000] train loss: 26.53199, val loss: 49.87810\n",
      "[108/1000] train loss: 26.51623, val loss: 49.65901\n",
      "[109/1000] train loss: 25.93196, val loss: 49.44764\n",
      "[110/1000] train loss: 25.70099, val loss: 49.21254\n",
      "[111/1000] train loss: 25.40692, val loss: 49.01299\n",
      "[112/1000] train loss: 25.75168, val loss: 48.78851\n",
      "[113/1000] train loss: 25.61707, val loss: 48.56553\n",
      "[114/1000] train loss: 25.03910, val loss: 48.35171\n",
      "[115/1000] train loss: 25.27366, val loss: 48.13783\n",
      "[116/1000] train loss: 24.99182, val loss: 47.93168\n",
      "[117/1000] train loss: 25.07118, val loss: 47.72459\n",
      "[118/1000] train loss: 23.60432, val loss: 47.59665\n",
      "[119/1000] train loss: 24.70331, val loss: 47.38569\n",
      "[120/1000] train loss: 24.60638, val loss: 47.19340\n",
      "[121/1000] train loss: 24.56670, val loss: 46.99212\n",
      "[122/1000] train loss: 22.97098, val loss: 46.86749\n",
      "[123/1000] train loss: 24.14659, val loss: 46.67865\n",
      "[124/1000] train loss: 23.76251, val loss: 46.46873\n",
      "[125/1000] train loss: 24.02333, val loss: 46.28073\n",
      "[126/1000] train loss: 23.80903, val loss: 46.08017\n",
      "[127/1000] train loss: 23.64265, val loss: 45.90597\n",
      "[128/1000] train loss: 23.52953, val loss: 45.71401\n",
      "[129/1000] train loss: 23.19102, val loss: 45.54007\n",
      "[130/1000] train loss: 23.18691, val loss: 45.34934\n",
      "[131/1000] train loss: 22.82445, val loss: 45.13885\n",
      "[132/1000] train loss: 22.78472, val loss: 44.94622\n",
      "[133/1000] train loss: 23.12736, val loss: 44.77840\n",
      "[134/1000] train loss: 22.59857, val loss: 44.57012\n",
      "[135/1000] train loss: 22.92071, val loss: 44.40165\n",
      "[136/1000] train loss: 22.79937, val loss: 44.22263\n",
      "[137/1000] train loss: 22.56436, val loss: 44.03410\n",
      "[138/1000] train loss: 22.53641, val loss: 43.87938\n",
      "[139/1000] train loss: 22.16864, val loss: 43.69086\n",
      "[140/1000] train loss: 22.30546, val loss: 43.53591\n",
      "[141/1000] train loss: 22.16156, val loss: 43.39539\n",
      "[142/1000] train loss: 22.09745, val loss: 43.23975\n",
      "[143/1000] train loss: 22.09269, val loss: 43.07001\n",
      "[144/1000] train loss: 21.94459, val loss: 42.91147\n",
      "[145/1000] train loss: 21.79519, val loss: 42.75641\n",
      "[146/1000] train loss: 21.74964, val loss: 42.60952\n",
      "[147/1000] train loss: 21.69753, val loss: 42.46605\n",
      "[148/1000] train loss: 21.23029, val loss: 42.29115\n",
      "[149/1000] train loss: 21.48586, val loss: 42.14651\n",
      "[150/1000] train loss: 21.27023, val loss: 42.00489\n",
      "[151/1000] train loss: 21.03587, val loss: 41.83466\n",
      "[152/1000] train loss: 21.04988, val loss: 41.66769\n",
      "[153/1000] train loss: 20.89246, val loss: 41.53398\n",
      "[154/1000] train loss: 20.97484, val loss: 41.37927\n",
      "[155/1000] train loss: 20.78649, val loss: 41.21783\n",
      "[156/1000] train loss: 20.79954, val loss: 41.08839\n",
      "[157/1000] train loss: 20.24956, val loss: 40.94054\n",
      "[158/1000] train loss: 20.56219, val loss: 40.79952\n",
      "[159/1000] train loss: 20.49263, val loss: 40.65402\n",
      "[160/1000] train loss: 20.43800, val loss: 40.49849\n",
      "[161/1000] train loss: 20.16770, val loss: 40.32351\n",
      "[162/1000] train loss: 19.98713, val loss: 40.19315\n",
      "[163/1000] train loss: 20.02850, val loss: 40.05346\n",
      "[164/1000] train loss: 19.77126, val loss: 39.86154\n",
      "[165/1000] train loss: 19.96685, val loss: 39.74485\n",
      "[166/1000] train loss: 19.94894, val loss: 39.58468\n",
      "[167/1000] train loss: 19.72286, val loss: 39.47104\n",
      "[168/1000] train loss: 19.68348, val loss: 39.35724\n",
      "[169/1000] train loss: 19.53465, val loss: 39.21849\n",
      "[170/1000] train loss: 19.55362, val loss: 39.05839\n",
      "[171/1000] train loss: 19.43451, val loss: 38.92928\n",
      "[172/1000] train loss: 19.31306, val loss: 38.81492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[173/1000] train loss: 19.22946, val loss: 38.68945\n",
      "[174/1000] train loss: 19.26000, val loss: 38.53271\n",
      "[175/1000] train loss: 19.06716, val loss: 38.42432\n",
      "[176/1000] train loss: 19.00059, val loss: 38.30685\n",
      "[177/1000] train loss: 18.92519, val loss: 38.18249\n",
      "[178/1000] train loss: 18.75597, val loss: 38.06811\n",
      "[179/1000] train loss: 18.80000, val loss: 37.96863\n",
      "[180/1000] train loss: 18.72900, val loss: 37.85033\n",
      "[181/1000] train loss: 18.56539, val loss: 37.72112\n",
      "[182/1000] train loss: 18.29455, val loss: 37.55162\n",
      "[183/1000] train loss: 18.37615, val loss: 37.44656\n",
      "[184/1000] train loss: 18.32830, val loss: 37.32858\n",
      "[185/1000] train loss: 18.25981, val loss: 37.19835\n",
      "[186/1000] train loss: 18.23541, val loss: 37.08773\n",
      "[187/1000] train loss: 18.10895, val loss: 36.95984\n",
      "[188/1000] train loss: 17.87949, val loss: 36.83114\n",
      "[189/1000] train loss: 17.98667, val loss: 36.69182\n",
      "[190/1000] train loss: 17.47793, val loss: 36.52340\n",
      "[191/1000] train loss: 17.82166, val loss: 36.40520\n",
      "[192/1000] train loss: 17.53373, val loss: 36.25931\n",
      "[193/1000] train loss: 17.62273, val loss: 36.15616\n",
      "[194/1000] train loss: 17.49141, val loss: 36.03920\n",
      "[195/1000] train loss: 17.47033, val loss: 35.92868\n",
      "[196/1000] train loss: 17.22774, val loss: 35.80193\n",
      "[197/1000] train loss: 17.30820, val loss: 35.71645\n",
      "[198/1000] train loss: 16.87481, val loss: 35.59763\n",
      "[199/1000] train loss: 17.20939, val loss: 35.54439\n",
      "[200/1000] train loss: 17.14676, val loss: 35.41150\n",
      "[201/1000] train loss: 17.05591, val loss: 35.32185\n",
      "[202/1000] train loss: 16.82308, val loss: 35.19101\n",
      "[203/1000] train loss: 16.97726, val loss: 35.13615\n",
      "[204/1000] train loss: 16.72681, val loss: 35.05233\n",
      "[205/1000] train loss: 16.46396, val loss: 34.92338\n",
      "[206/1000] train loss: 16.74531, val loss: 34.78720\n",
      "[207/1000] train loss: 16.57880, val loss: 34.65348\n",
      "[208/1000] train loss: 16.52170, val loss: 34.55369\n",
      "[209/1000] train loss: 16.40168, val loss: 34.45360\n",
      "[210/1000] train loss: 16.46465, val loss: 34.33681\n",
      "[211/1000] train loss: 16.26678, val loss: 34.24241\n",
      "[212/1000] train loss: 16.23818, val loss: 34.17918\n",
      "[213/1000] train loss: 16.17329, val loss: 34.04653\n",
      "[214/1000] train loss: 16.08439, val loss: 33.92573\n",
      "[215/1000] train loss: 16.01853, val loss: 33.83170\n",
      "[216/1000] train loss: 15.92470, val loss: 33.72620\n",
      "[217/1000] train loss: 15.88827, val loss: 33.62123\n",
      "[218/1000] train loss: 15.82902, val loss: 33.52229\n",
      "[219/1000] train loss: 15.65141, val loss: 33.44278\n",
      "[220/1000] train loss: 15.58169, val loss: 33.36118\n",
      "[221/1000] train loss: 15.58457, val loss: 33.29385\n",
      "[222/1000] train loss: 15.45188, val loss: 33.16451\n",
      "[223/1000] train loss: 15.42084, val loss: 33.05200\n",
      "[224/1000] train loss: 15.36866, val loss: 32.95728\n",
      "[225/1000] train loss: 15.37974, val loss: 32.88453\n",
      "[226/1000] train loss: 15.17773, val loss: 32.81433\n",
      "[227/1000] train loss: 15.09544, val loss: 32.72604\n",
      "[228/1000] train loss: 14.93955, val loss: 32.58211\n",
      "[229/1000] train loss: 15.11910, val loss: 32.48734\n",
      "[230/1000] train loss: 14.85135, val loss: 32.38395\n",
      "[231/1000] train loss: 14.84789, val loss: 32.32528\n",
      "[232/1000] train loss: 14.86020, val loss: 32.25168\n",
      "[233/1000] train loss: 14.72542, val loss: 32.08812\n",
      "[234/1000] train loss: 14.68780, val loss: 32.02194\n",
      "[235/1000] train loss: 14.66179, val loss: 31.92342\n",
      "[236/1000] train loss: 14.61332, val loss: 31.86433\n",
      "[237/1000] train loss: 14.45475, val loss: 31.75381\n",
      "[238/1000] train loss: 14.32787, val loss: 31.65558\n",
      "[239/1000] train loss: 14.43561, val loss: 31.48112\n",
      "[240/1000] train loss: 14.30129, val loss: 31.41592\n",
      "[241/1000] train loss: 14.23094, val loss: 31.30079\n",
      "[242/1000] train loss: 14.11209, val loss: 31.24860\n",
      "[243/1000] train loss: 14.13076, val loss: 31.18677\n",
      "[244/1000] train loss: 14.14959, val loss: 31.06912\n",
      "[245/1000] train loss: 13.93102, val loss: 30.99307\n",
      "[246/1000] train loss: 13.96134, val loss: 30.89432\n",
      "[247/1000] train loss: 13.83025, val loss: 30.86837\n",
      "[248/1000] train loss: 13.82717, val loss: 30.73462\n",
      "[249/1000] train loss: 13.70771, val loss: 30.66642\n",
      "[250/1000] train loss: 13.63003, val loss: 30.55347\n",
      "[251/1000] train loss: 13.46895, val loss: 30.59383\n",
      "[252/1000] train loss: 13.41795, val loss: 30.49000\n",
      "[253/1000] train loss: 13.47242, val loss: 30.35037\n",
      "[254/1000] train loss: 13.46978, val loss: 30.18315\n",
      "[255/1000] train loss: 13.38938, val loss: 30.07790\n",
      "[256/1000] train loss: 13.33433, val loss: 29.97564\n",
      "[257/1000] train loss: 13.23622, val loss: 29.94654\n",
      "[258/1000] train loss: 13.11424, val loss: 29.86690\n",
      "[259/1000] train loss: 13.18849, val loss: 29.76692\n",
      "[260/1000] train loss: 13.09051, val loss: 29.76015\n",
      "[261/1000] train loss: 13.07122, val loss: 29.67049\n",
      "[262/1000] train loss: 12.77867, val loss: 29.60277\n",
      "[263/1000] train loss: 12.87369, val loss: 29.49104\n",
      "[264/1000] train loss: 12.91596, val loss: 29.45252\n",
      "[265/1000] train loss: 12.85927, val loss: 29.35316\n",
      "[266/1000] train loss: 12.58404, val loss: 29.25636\n",
      "[267/1000] train loss: 12.08292, val loss: 29.27677\n",
      "[268/1000] train loss: 12.68128, val loss: 29.15380\n",
      "[269/1000] train loss: 12.47788, val loss: 29.03885\n",
      "[270/1000] train loss: 12.55333, val loss: 28.97067\n",
      "[271/1000] train loss: 12.51723, val loss: 28.95840\n",
      "[272/1000] train loss: 12.26635, val loss: 28.90731\n",
      "[273/1000] train loss: 12.33960, val loss: 28.75338\n",
      "[274/1000] train loss: 12.29910, val loss: 28.68360\n",
      "[275/1000] train loss: 12.35242, val loss: 28.53138\n",
      "[276/1000] train loss: 12.26119, val loss: 28.48668\n",
      "[277/1000] train loss: 12.00548, val loss: 28.41221\n",
      "[278/1000] train loss: 12.16989, val loss: 28.33072\n",
      "[279/1000] train loss: 11.57652, val loss: 28.35710\n",
      "[280/1000] train loss: 11.96420, val loss: 28.30869\n",
      "[281/1000] train loss: 12.05507, val loss: 28.16632\n",
      "[282/1000] train loss: 11.99245, val loss: 28.05229\n",
      "[283/1000] train loss: 11.84006, val loss: 28.01019\n",
      "[284/1000] train loss: 11.87802, val loss: 27.88262\n",
      "[285/1000] train loss: 11.94230, val loss: 27.74854\n",
      "[286/1000] train loss: 11.77916, val loss: 27.73664\n",
      "[287/1000] train loss: 11.64153, val loss: 27.70241\n",
      "[288/1000] train loss: 11.65357, val loss: 27.67954\n",
      "[289/1000] train loss: 11.53877, val loss: 27.58375\n",
      "[290/1000] train loss: 11.39691, val loss: 27.49859\n",
      "[291/1000] train loss: 11.32579, val loss: 27.46232\n",
      "[292/1000] train loss: 11.48270, val loss: 27.36124\n",
      "[293/1000] train loss: 11.40767, val loss: 27.28697\n",
      "[294/1000] train loss: 11.41012, val loss: 27.23838\n",
      "[295/1000] train loss: 11.26396, val loss: 27.09192\n",
      "[296/1000] train loss: 11.36934, val loss: 27.01543\n",
      "[297/1000] train loss: 11.31823, val loss: 26.96685\n",
      "[298/1000] train loss: 11.32020, val loss: 26.92998\n",
      "[299/1000] train loss: 11.25767, val loss: 26.88824\n",
      "[300/1000] train loss: 11.22923, val loss: 26.81910\n",
      "[301/1000] train loss: 11.15672, val loss: 26.71585\n",
      "[302/1000] train loss: 11.02861, val loss: 26.63521\n",
      "[303/1000] train loss: 11.06885, val loss: 26.63549\n",
      "[304/1000] train loss: 11.01380, val loss: 26.58902\n",
      "[305/1000] train loss: 10.99592, val loss: 26.45885\n",
      "[306/1000] train loss: 10.91888, val loss: 26.36906\n",
      "[307/1000] train loss: 10.90992, val loss: 26.41866\n",
      "[308/1000] train loss: 10.82586, val loss: 26.33623\n",
      "[309/1000] train loss: 10.32978, val loss: 26.04249\n",
      "[310/1000] train loss: 10.75314, val loss: 26.02694\n",
      "[311/1000] train loss: 10.74966, val loss: 25.95287\n",
      "[312/1000] train loss: 10.71373, val loss: 25.96229\n",
      "[313/1000] train loss: 10.56847, val loss: 25.89513\n",
      "[314/1000] train loss: 10.59077, val loss: 25.93305\n",
      "[315/1000] train loss: 10.43325, val loss: 25.85890\n",
      "[316/1000] train loss: 10.65941, val loss: 25.78199\n",
      "[317/1000] train loss: 10.74661, val loss: 25.84004\n",
      "[318/1000] train loss: 10.48399, val loss: 25.66292\n",
      "[319/1000] train loss: 10.33764, val loss: 25.59869\n",
      "[320/1000] train loss: 10.42138, val loss: 25.52274\n",
      "[321/1000] train loss: 10.41609, val loss: 25.56061\n",
      "[322/1000] train loss: 10.47260, val loss: 25.49952\n",
      "[323/1000] train loss: 10.32020, val loss: 25.44764\n",
      "[324/1000] train loss: 10.28533, val loss: 25.43592\n",
      "[325/1000] train loss: 10.22163, val loss: 25.33597\n",
      "[326/1000] train loss: 10.28240, val loss: 25.32611\n",
      "[327/1000] train loss: 10.32581, val loss: 25.16236\n",
      "[328/1000] train loss: 10.16003, val loss: 25.12367\n",
      "[329/1000] train loss: 10.13463, val loss: 25.00973\n",
      "[330/1000] train loss: 10.14385, val loss: 24.99916\n",
      "[331/1000] train loss: 10.11911, val loss: 24.96786\n",
      "[332/1000] train loss: 10.07765, val loss: 24.80590\n",
      "[333/1000] train loss: 9.93134, val loss: 24.83023\n",
      "[334/1000] train loss: 9.95785, val loss: 24.86899\n",
      "[335/1000] train loss: 10.09466, val loss: 24.76032\n",
      "[336/1000] train loss: 9.94846, val loss: 24.75362\n",
      "[337/1000] train loss: 9.74937, val loss: 24.77712\n",
      "[338/1000] train loss: 9.91279, val loss: 24.68718\n",
      "[339/1000] train loss: 9.84932, val loss: 24.66747\n",
      "[340/1000] train loss: 9.83219, val loss: 24.66496\n",
      "[341/1000] train loss: 9.79483, val loss: 24.72023\n",
      "[342/1000] train loss: 9.80804, val loss: 24.63121\n",
      "[343/1000] train loss: 9.63853, val loss: 24.50908\n",
      "[344/1000] train loss: 9.81554, val loss: 24.43353\n",
      "[345/1000] train loss: 9.74972, val loss: 24.33339\n",
      "[346/1000] train loss: 9.74960, val loss: 24.43005\n",
      "[347/1000] train loss: 9.52639, val loss: 24.46166\n",
      "[348/1000] train loss: 9.59497, val loss: 24.41031\n",
      "[349/1000] train loss: 9.67157, val loss: 24.35101\n",
      "[350/1000] train loss: 9.57047, val loss: 24.36797\n",
      "[351/1000] train loss: 9.24801, val loss: 24.44929\n",
      "[352/1000] train loss: 9.57413, val loss: 24.37564\n",
      "[353/1000] train loss: 9.56255, val loss: 24.34757\n",
      "[354/1000] train loss: 9.40512, val loss: 24.23070\n",
      "[355/1000] train loss: 9.41682, val loss: 24.21801\n",
      "[356/1000] train loss: 9.44754, val loss: 24.22909\n",
      "[357/1000] train loss: 9.36654, val loss: 24.12499\n",
      "[358/1000] train loss: 9.41349, val loss: 24.10064\n",
      "[359/1000] train loss: 9.54147, val loss: 24.02298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[360/1000] train loss: 9.15980, val loss: 23.94502\n",
      "[361/1000] train loss: 9.32865, val loss: 23.99619\n",
      "[362/1000] train loss: 9.21184, val loss: 23.86588\n",
      "[363/1000] train loss: 9.31735, val loss: 23.84670\n",
      "[364/1000] train loss: 9.26867, val loss: 23.79124\n",
      "[365/1000] train loss: 9.25517, val loss: 23.78699\n",
      "[366/1000] train loss: 9.25496, val loss: 23.72299\n",
      "[367/1000] train loss: 9.16207, val loss: 23.64462\n",
      "[368/1000] train loss: 8.96550, val loss: 23.62063\n",
      "[369/1000] train loss: 9.22227, val loss: 23.56380\n",
      "[370/1000] train loss: 9.11016, val loss: 23.56391\n",
      "[371/1000] train loss: 9.10450, val loss: 23.54087\n",
      "[372/1000] train loss: 9.14269, val loss: 23.48371\n",
      "[373/1000] train loss: 9.05178, val loss: 23.43888\n",
      "[374/1000] train loss: 9.03447, val loss: 23.41840\n",
      "[375/1000] train loss: 9.01310, val loss: 23.46703\n",
      "[376/1000] train loss: 9.01831, val loss: 23.38614\n",
      "[377/1000] train loss: 8.93909, val loss: 23.26634\n",
      "[378/1000] train loss: 9.00948, val loss: 23.30107\n",
      "[379/1000] train loss: 8.98365, val loss: 23.18898\n",
      "[380/1000] train loss: 8.93170, val loss: 23.22831\n",
      "[381/1000] train loss: 8.83104, val loss: 23.18176\n",
      "[382/1000] train loss: 8.75265, val loss: 23.13663\n",
      "[383/1000] train loss: 8.82534, val loss: 23.11193\n",
      "[384/1000] train loss: 8.85191, val loss: 23.15050\n",
      "[385/1000] train loss: 8.83496, val loss: 23.07568\n",
      "[386/1000] train loss: 8.79318, val loss: 23.03839\n",
      "[387/1000] train loss: 8.70213, val loss: 22.96918\n",
      "[388/1000] train loss: 8.72298, val loss: 23.03055\n",
      "[389/1000] train loss: 8.76603, val loss: 23.06429\n",
      "[390/1000] train loss: 8.72336, val loss: 22.89021\n",
      "[391/1000] train loss: 8.50101, val loss: 22.87396\n",
      "[392/1000] train loss: 8.63546, val loss: 22.96945\n",
      "[393/1000] train loss: 8.60352, val loss: 22.87932\n",
      "[394/1000] train loss: 8.66903, val loss: 22.97713\n",
      "[395/1000] train loss: 8.76239, val loss: 22.96324\n",
      "[396/1000] train loss: 8.62578, val loss: 22.88998\n",
      "[397/1000] train loss: 8.55740, val loss: 22.82260\n",
      "[398/1000] train loss: 7.72567, val loss: 22.95354\n",
      "[399/1000] train loss: 8.53832, val loss: 22.86572\n",
      "[400/1000] train loss: 8.54962, val loss: 22.67282\n",
      "[401/1000] train loss: 8.50363, val loss: 22.54657\n",
      "[402/1000] train loss: 8.59996, val loss: 22.53141\n",
      "[403/1000] train loss: 8.45655, val loss: 22.66600\n",
      "[404/1000] train loss: 8.45375, val loss: 22.61054\n",
      "[405/1000] train loss: 8.41385, val loss: 22.54559\n",
      "[406/1000] train loss: 8.39389, val loss: 22.45360\n",
      "[407/1000] train loss: 8.46650, val loss: 22.41193\n",
      "[408/1000] train loss: 8.37642, val loss: 22.41110\n",
      "[409/1000] train loss: 8.33197, val loss: 22.55844\n",
      "[410/1000] train loss: 8.23382, val loss: 22.49360\n",
      "[411/1000] train loss: 8.36615, val loss: 22.45385\n",
      "[412/1000] train loss: 8.31036, val loss: 22.51636\n",
      "[413/1000] train loss: 8.29552, val loss: 22.38206\n",
      "[414/1000] train loss: 8.21741, val loss: 22.26862\n",
      "[415/1000] train loss: 8.27124, val loss: 22.30663\n",
      "[416/1000] train loss: 8.27763, val loss: 22.42905\n",
      "[417/1000] train loss: 8.18953, val loss: 22.23520\n",
      "[418/1000] train loss: 8.19825, val loss: 22.29016\n",
      "[419/1000] train loss: 8.05458, val loss: 22.25382\n",
      "[420/1000] train loss: 8.21353, val loss: 22.36347\n",
      "[421/1000] train loss: 7.87436, val loss: 22.28255\n",
      "[422/1000] train loss: 8.15712, val loss: 22.29855\n",
      "[423/1000] train loss: 8.11682, val loss: 22.34326\n",
      "[424/1000] train loss: 8.01684, val loss: 22.26035\n",
      "[425/1000] train loss: 8.02262, val loss: 22.22722\n",
      "[426/1000] train loss: 8.13905, val loss: 22.19188\n",
      "[427/1000] train loss: 8.00104, val loss: 22.02891\n",
      "[428/1000] train loss: 7.86696, val loss: 22.02378\n",
      "[429/1000] train loss: 7.99713, val loss: 22.09325\n",
      "[430/1000] train loss: 7.99899, val loss: 21.95983\n",
      "[431/1000] train loss: 8.01183, val loss: 21.97584\n",
      "[432/1000] train loss: 7.86857, val loss: 21.96830\n",
      "[433/1000] train loss: 8.09440, val loss: 22.09433\n",
      "[434/1000] train loss: 7.94978, val loss: 21.96120\n",
      "[435/1000] train loss: 7.95215, val loss: 21.91748\n",
      "[436/1000] train loss: 7.90868, val loss: 21.86081\n",
      "[437/1000] train loss: 7.94146, val loss: 21.82520\n",
      "[438/1000] train loss: 7.83727, val loss: 21.84731\n",
      "[439/1000] train loss: 7.82624, val loss: 21.86945\n",
      "[440/1000] train loss: 7.83562, val loss: 21.85827\n",
      "[441/1000] train loss: 7.83634, val loss: 21.84837\n",
      "[442/1000] train loss: 7.75010, val loss: 21.83086\n",
      "[443/1000] train loss: 7.13163, val loss: 21.90853\n",
      "[444/1000] train loss: 7.82466, val loss: 21.77514\n",
      "[445/1000] train loss: 7.75032, val loss: 21.75471\n",
      "[446/1000] train loss: 7.76676, val loss: 21.67701\n",
      "[447/1000] train loss: 7.77298, val loss: 21.74132\n",
      "[448/1000] train loss: 7.74893, val loss: 21.71493\n",
      "[449/1000] train loss: 7.58024, val loss: 21.75462\n",
      "[450/1000] train loss: 7.75233, val loss: 21.57314\n",
      "[451/1000] train loss: 7.65058, val loss: 21.64086\n",
      "[452/1000] train loss: 7.62323, val loss: 21.58642\n",
      "[453/1000] train loss: 7.70873, val loss: 21.53490\n",
      "[454/1000] train loss: 7.73005, val loss: 21.63310\n",
      "[455/1000] train loss: 7.67495, val loss: 21.65948\n",
      "[456/1000] train loss: 7.59083, val loss: 21.62709\n",
      "[457/1000] train loss: 7.57208, val loss: 21.63720\n",
      "[458/1000] train loss: 7.82041, val loss: 21.79875\n",
      "[459/1000] train loss: 7.59353, val loss: 21.53861\n",
      "[460/1000] train loss: 7.61575, val loss: 21.67035\n",
      "[461/1000] train loss: 7.50007, val loss: 21.56203\n",
      "[462/1000] train loss: 7.53598, val loss: 21.58347\n",
      "[463/1000] train loss: 7.43067, val loss: 21.50655\n",
      "[464/1000] train loss: 7.47929, val loss: 21.47654\n",
      "[465/1000] train loss: 7.45557, val loss: 21.57380\n",
      "[466/1000] train loss: 7.46945, val loss: 21.59801\n",
      "[467/1000] train loss: 7.44970, val loss: 21.59665\n",
      "[468/1000] train loss: 7.45382, val loss: 21.50528\n",
      "[469/1000] train loss: 7.39933, val loss: 21.62610\n",
      "[470/1000] train loss: 7.36112, val loss: 21.53127\n",
      "[471/1000] train loss: 7.47048, val loss: 21.61604\n",
      "[472/1000] train loss: 7.45361, val loss: 21.67282\n",
      "[473/1000] train loss: 7.39467, val loss: 21.34701\n",
      "[474/1000] train loss: 7.24593, val loss: 21.31022\n",
      "[475/1000] train loss: 7.36823, val loss: 21.27924\n",
      "[476/1000] train loss: 7.40505, val loss: 21.47661\n",
      "[477/1000] train loss: 7.20741, val loss: 21.37317\n",
      "[478/1000] train loss: 7.28682, val loss: 21.33970\n",
      "[479/1000] train loss: 7.21049, val loss: 21.37013\n",
      "[480/1000] train loss: 7.30531, val loss: 21.22882\n",
      "[481/1000] train loss: 7.12203, val loss: 21.30987\n",
      "[482/1000] train loss: 7.20688, val loss: 21.29363\n",
      "[483/1000] train loss: 7.19393, val loss: 21.33646\n",
      "[484/1000] train loss: 7.18733, val loss: 21.32086\n",
      "[485/1000] train loss: 7.07897, val loss: 21.27826\n",
      "[486/1000] train loss: 7.21003, val loss: 21.42430\n",
      "[487/1000] train loss: 7.20459, val loss: 21.27883\n",
      "[488/1000] train loss: 7.10984, val loss: 21.25224\n",
      "[489/1000] train loss: 7.15984, val loss: 21.31389\n",
      "[490/1000] train loss: 7.12075, val loss: 21.32339\n",
      "[491/1000] train loss: 7.19309, val loss: 21.13759\n",
      "[492/1000] train loss: 7.16471, val loss: 21.25710\n",
      "[493/1000] train loss: 6.82169, val loss: 21.19339\n",
      "[494/1000] train loss: 7.11842, val loss: 21.30772\n",
      "[495/1000] train loss: 7.12245, val loss: 21.30568\n",
      "[496/1000] train loss: 7.02750, val loss: 21.14294\n",
      "[497/1000] train loss: 6.92961, val loss: 21.11647\n",
      "[498/1000] train loss: 7.15376, val loss: 20.99298\n",
      "[499/1000] train loss: 7.02320, val loss: 21.19652\n",
      "[500/1000] train loss: 7.03899, val loss: 21.23187\n",
      "[501/1000] train loss: 6.96413, val loss: 21.19083\n",
      "[502/1000] train loss: 7.05233, val loss: 21.30486\n",
      "[503/1000] train loss: 6.98498, val loss: 21.11060\n",
      "[504/1000] train loss: 6.89491, val loss: 21.10877\n",
      "[505/1000] train loss: 6.88181, val loss: 21.07033\n",
      "[506/1000] train loss: 6.94396, val loss: 21.10898\n",
      "[507/1000] train loss: 6.27578, val loss: 21.12798\n",
      "[508/1000] train loss: 6.82249, val loss: 21.04580\n",
      "[509/1000] train loss: 6.90170, val loss: 21.21697\n",
      "[510/1000] train loss: 6.93309, val loss: 21.12906\n",
      "[511/1000] train loss: 6.90610, val loss: 21.05966\n",
      "[512/1000] train loss: 7.14657, val loss: 21.05479\n",
      "[513/1000] train loss: 7.05197, val loss: 20.97027\n",
      "[514/1000] train loss: 6.83186, val loss: 20.99828\n",
      "[515/1000] train loss: 6.81650, val loss: 21.06537\n",
      "[516/1000] train loss: 6.73255, val loss: 20.98549\n",
      "[517/1000] train loss: 6.87282, val loss: 20.76413\n",
      "[518/1000] train loss: 6.74947, val loss: 20.83638\n",
      "[519/1000] train loss: 6.77708, val loss: 20.89996\n",
      "[520/1000] train loss: 6.62107, val loss: 20.88921\n",
      "[521/1000] train loss: 6.75508, val loss: 20.83842\n",
      "[522/1000] train loss: 6.77871, val loss: 20.99731\n",
      "[523/1000] train loss: 6.85223, val loss: 20.87847\n",
      "[524/1000] train loss: 6.72712, val loss: 21.04185\n",
      "[525/1000] train loss: 6.71141, val loss: 20.85209\n",
      "[526/1000] train loss: 6.70584, val loss: 20.83556\n",
      "[527/1000] train loss: 6.66635, val loss: 20.83640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[528/1000] train loss: 6.70335, val loss: 20.92365\n",
      "[529/1000] train loss: 6.72878, val loss: 20.77485\n",
      "[530/1000] train loss: 6.66884, val loss: 20.79158\n",
      "[531/1000] train loss: 6.76523, val loss: 20.83726\n",
      "[532/1000] train loss: 6.61672, val loss: 20.90433\n",
      "[533/1000] train loss: 6.56145, val loss: 20.72625\n",
      "[534/1000] train loss: 6.59812, val loss: 20.69142\n",
      "[535/1000] train loss: 6.70947, val loss: 20.68741\n",
      "[536/1000] train loss: 6.51257, val loss: 20.81234\n",
      "[537/1000] train loss: 6.60051, val loss: 20.82360\n",
      "[538/1000] train loss: 6.58261, val loss: 20.79995\n",
      "[539/1000] train loss: 6.40485, val loss: 20.89916\n",
      "[540/1000] train loss: 6.61753, val loss: 20.71894\n",
      "[541/1000] train loss: 6.56678, val loss: 20.90270\n",
      "[542/1000] train loss: 6.45527, val loss: 20.82573\n",
      "[543/1000] train loss: 6.57304, val loss: 20.81121\n",
      "[544/1000] train loss: 6.49996, val loss: 20.77429\n",
      "[545/1000] train loss: 6.52524, val loss: 20.63718\n",
      "[546/1000] train loss: 6.53854, val loss: 20.57436\n",
      "[547/1000] train loss: 6.39515, val loss: 20.54338\n",
      "[548/1000] train loss: 6.50123, val loss: 20.50389\n",
      "[549/1000] train loss: 6.45815, val loss: 20.55306\n",
      "[550/1000] train loss: 6.48946, val loss: 20.50944\n",
      "[551/1000] train loss: 6.43364, val loss: 20.61733\n",
      "[552/1000] train loss: 6.44240, val loss: 20.60697\n",
      "[553/1000] train loss: 6.39878, val loss: 20.63584\n",
      "[554/1000] train loss: 6.40502, val loss: 20.65015\n",
      "[555/1000] train loss: 6.37362, val loss: 20.56713\n",
      "[556/1000] train loss: 6.48404, val loss: 20.76215\n",
      "[557/1000] train loss: 6.37406, val loss: 20.66045\n",
      "[558/1000] train loss: 6.31995, val loss: 20.64939\n",
      "[559/1000] train loss: 6.35614, val loss: 20.59386\n",
      "[560/1000] train loss: 6.32677, val loss: 20.57012\n",
      "[561/1000] train loss: 6.41080, val loss: 20.51003\n",
      "[562/1000] train loss: 6.35608, val loss: 20.56510\n",
      "[563/1000] train loss: 6.41011, val loss: 20.51852\n",
      "[564/1000] train loss: 6.32430, val loss: 20.48805\n",
      "[565/1000] train loss: 6.33426, val loss: 20.54492\n",
      "[566/1000] train loss: 6.30437, val loss: 20.43595\n",
      "[567/1000] train loss: 6.35568, val loss: 20.41898\n",
      "[568/1000] train loss: 6.34034, val loss: 20.52886\n",
      "[569/1000] train loss: 6.08949, val loss: 20.55820\n",
      "[570/1000] train loss: 6.22977, val loss: 20.39669\n",
      "[571/1000] train loss: 6.36826, val loss: 20.39808\n",
      "[572/1000] train loss: 6.26593, val loss: 20.62753\n",
      "[573/1000] train loss: 6.19011, val loss: 20.44851\n",
      "[574/1000] train loss: 5.74905, val loss: 20.73588\n",
      "[575/1000] train loss: 6.25627, val loss: 20.63175\n",
      "[576/1000] train loss: 6.19266, val loss: 20.42390\n",
      "[577/1000] train loss: 6.12738, val loss: 20.37503\n",
      "[578/1000] train loss: 6.14857, val loss: 20.40697\n",
      "[579/1000] train loss: 6.20055, val loss: 20.39672\n",
      "[580/1000] train loss: 6.13943, val loss: 20.39909\n",
      "[581/1000] train loss: 6.05926, val loss: 20.41698\n",
      "[582/1000] train loss: 6.12714, val loss: 20.57242\n",
      "[583/1000] train loss: 6.18356, val loss: 20.48809\n",
      "[584/1000] train loss: 6.18073, val loss: 20.66524\n",
      "[585/1000] train loss: 6.03063, val loss: 20.37558\n",
      "[586/1000] train loss: 6.12303, val loss: 20.39375\n",
      "[587/1000] train loss: 6.12295, val loss: 20.37229\n",
      "[588/1000] train loss: 6.10713, val loss: 20.43923\n",
      "[589/1000] train loss: 6.05827, val loss: 20.25354\n",
      "[590/1000] train loss: 6.11924, val loss: 20.32056\n",
      "[591/1000] train loss: 5.95639, val loss: 20.38140\n",
      "[592/1000] train loss: 6.08485, val loss: 20.26584\n",
      "[593/1000] train loss: 6.06408, val loss: 20.39066\n",
      "[594/1000] train loss: 6.07183, val loss: 20.42895\n",
      "[595/1000] train loss: 6.09154, val loss: 20.31767\n",
      "[596/1000] train loss: 6.06914, val loss: 20.24191\n",
      "[597/1000] train loss: 6.14778, val loss: 20.63182\n",
      "[598/1000] train loss: 6.08658, val loss: 20.31066\n",
      "[599/1000] train loss: 6.03466, val loss: 20.46884\n",
      "[600/1000] train loss: 5.51434, val loss: 20.48293\n",
      "[601/1000] train loss: 5.95996, val loss: 20.37440\n",
      "[602/1000] train loss: 6.03929, val loss: 20.41653\n",
      "[603/1000] train loss: 5.84900, val loss: 20.27308\n",
      "[604/1000] train loss: 5.99584, val loss: 20.38695\n",
      "[605/1000] train loss: 5.96702, val loss: 20.33251\n",
      "[606/1000] train loss: 5.82494, val loss: 20.35075\n",
      "[607/1000] train loss: 5.77843, val loss: 20.28427\n",
      "[608/1000] train loss: 6.13674, val loss: 20.26454\n",
      "[609/1000] train loss: 5.45457, val loss: 20.46036\n",
      "[610/1000] train loss: 5.95237, val loss: 20.31749\n",
      "[611/1000] train loss: 6.03007, val loss: 20.34164\n",
      "[612/1000] train loss: 5.93051, val loss: 20.21957\n",
      "[613/1000] train loss: 5.90856, val loss: 20.29557\n",
      "[614/1000] train loss: 5.91953, val loss: 20.15791\n",
      "[615/1000] train loss: 5.77004, val loss: 20.26046\n",
      "[616/1000] train loss: 5.89261, val loss: 20.33618\n",
      "[617/1000] train loss: 5.89709, val loss: 20.12020\n",
      "[618/1000] train loss: 5.89678, val loss: 20.23935\n",
      "[619/1000] train loss: 5.87373, val loss: 20.16545\n",
      "[620/1000] train loss: 5.71865, val loss: 20.24852\n",
      "[621/1000] train loss: 5.94845, val loss: 20.24338\n",
      "[622/1000] train loss: 5.90099, val loss: 20.25458\n",
      "[623/1000] train loss: 5.86185, val loss: 20.23467\n",
      "[624/1000] train loss: 5.83433, val loss: 20.18807\n",
      "[625/1000] train loss: 5.85248, val loss: 20.22767\n",
      "[626/1000] train loss: 5.80695, val loss: 20.04805\n",
      "[627/1000] train loss: 5.78839, val loss: 20.06708\n",
      "[628/1000] train loss: 5.86497, val loss: 19.98474\n",
      "[629/1000] train loss: 5.78936, val loss: 20.01101\n",
      "[630/1000] train loss: 5.71433, val loss: 20.11664\n",
      "[631/1000] train loss: 5.79519, val loss: 20.10769\n",
      "[632/1000] train loss: 5.83665, val loss: 20.05638\n",
      "[633/1000] train loss: 5.72959, val loss: 20.12228\n",
      "[634/1000] train loss: 5.76678, val loss: 20.07552\n",
      "[635/1000] train loss: 5.74445, val loss: 20.06506\n",
      "[636/1000] train loss: 5.76154, val loss: 20.04752\n",
      "[637/1000] train loss: 5.59044, val loss: 19.99777\n",
      "[638/1000] train loss: 5.72963, val loss: 20.14083\n",
      "[639/1000] train loss: 5.49173, val loss: 19.88281\n",
      "[640/1000] train loss: 5.83577, val loss: 20.04909\n",
      "[641/1000] train loss: 5.74073, val loss: 20.12890\n",
      "[642/1000] train loss: 5.65395, val loss: 19.91435\n",
      "[643/1000] train loss: 5.67941, val loss: 19.94513\n",
      "[644/1000] train loss: 5.65461, val loss: 19.90963\n",
      "[645/1000] train loss: 5.64204, val loss: 19.94830\n",
      "[646/1000] train loss: 5.68186, val loss: 19.86056\n",
      "[647/1000] train loss: 5.64915, val loss: 19.80404\n",
      "[648/1000] train loss: 5.57251, val loss: 19.93793\n",
      "[649/1000] train loss: 5.63851, val loss: 19.88943\n",
      "[650/1000] train loss: 5.69418, val loss: 19.82384\n",
      "[651/1000] train loss: 5.62957, val loss: 19.98962\n",
      "[652/1000] train loss: 5.64599, val loss: 19.89407\n",
      "[653/1000] train loss: 5.61571, val loss: 19.82175\n",
      "[654/1000] train loss: 5.48252, val loss: 19.84721\n",
      "[655/1000] train loss: 5.62001, val loss: 19.88141\n",
      "[656/1000] train loss: 5.53869, val loss: 19.72995\n",
      "[657/1000] train loss: 5.62401, val loss: 19.75321\n",
      "[658/1000] train loss: 5.55270, val loss: 19.94675\n",
      "[659/1000] train loss: 5.70960, val loss: 19.87364\n",
      "[660/1000] train loss: 5.68011, val loss: 19.97861\n",
      "[661/1000] train loss: 5.57686, val loss: 19.87562\n",
      "[662/1000] train loss: 5.58318, val loss: 19.65420\n",
      "[663/1000] train loss: 5.46081, val loss: 19.81388\n",
      "[664/1000] train loss: 5.52837, val loss: 19.71322\n",
      "[665/1000] train loss: 5.49874, val loss: 19.84418\n",
      "[666/1000] train loss: 5.52058, val loss: 19.74401\n",
      "[667/1000] train loss: 5.50924, val loss: 19.68703\n",
      "[668/1000] train loss: 5.51000, val loss: 19.84211\n",
      "[669/1000] train loss: 5.52125, val loss: 19.92798\n",
      "[670/1000] train loss: 5.56977, val loss: 19.79087\n",
      "[671/1000] train loss: 5.43912, val loss: 19.80069\n",
      "[672/1000] train loss: 5.54365, val loss: 19.70419\n",
      "[673/1000] train loss: 5.49717, val loss: 19.80018\n",
      "[674/1000] train loss: 5.49218, val loss: 19.75013\n",
      "[675/1000] train loss: 5.44060, val loss: 19.69908\n",
      "[676/1000] train loss: 5.25566, val loss: 19.76264\n",
      "[677/1000] train loss: 5.48055, val loss: 19.59358\n",
      "[678/1000] train loss: 5.35335, val loss: 19.84596\n",
      "[679/1000] train loss: 5.46701, val loss: 19.58512\n",
      "[680/1000] train loss: 5.40494, val loss: 19.70996\n",
      "[681/1000] train loss: 4.79087, val loss: 20.00801\n",
      "[682/1000] train loss: 5.38417, val loss: 19.62517\n",
      "[683/1000] train loss: 5.41938, val loss: 19.80513\n",
      "[684/1000] train loss: 5.35133, val loss: 19.72728\n",
      "[685/1000] train loss: 5.43194, val loss: 19.79699\n",
      "[686/1000] train loss: 5.47365, val loss: 19.80025\n",
      "[687/1000] train loss: 5.35281, val loss: 19.55884\n",
      "[688/1000] train loss: 5.38724, val loss: 19.67096\n",
      "[689/1000] train loss: 5.34022, val loss: 19.65313\n",
      "[690/1000] train loss: 5.32811, val loss: 19.52510\n",
      "[691/1000] train loss: 5.30540, val loss: 19.59300\n",
      "[692/1000] train loss: 5.44368, val loss: 19.77230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[693/1000] train loss: 5.32039, val loss: 19.57031\n",
      "[694/1000] train loss: 5.27314, val loss: 19.54573\n",
      "[695/1000] train loss: 5.34596, val loss: 19.37985\n",
      "[696/1000] train loss: 5.30073, val loss: 19.57721\n",
      "[697/1000] train loss: 5.35836, val loss: 19.54816\n",
      "[698/1000] train loss: 5.43387, val loss: 19.61880\n",
      "[699/1000] train loss: 5.32477, val loss: 19.37473\n",
      "[700/1000] train loss: 5.20189, val loss: 19.41683\n",
      "[701/1000] train loss: 5.44439, val loss: 19.49009\n",
      "[702/1000] train loss: 5.37788, val loss: 19.62044\n",
      "[703/1000] train loss: 5.29652, val loss: 19.60528\n",
      "[704/1000] train loss: 5.25965, val loss: 19.75710\n",
      "[705/1000] train loss: 5.26965, val loss: 19.60326\n",
      "[706/1000] train loss: 5.26760, val loss: 19.50073\n",
      "[707/1000] train loss: 5.23493, val loss: 19.42545\n",
      "[708/1000] train loss: 5.25613, val loss: 19.55393\n",
      "[709/1000] train loss: 5.26596, val loss: 19.63674\n",
      "[710/1000] train loss: 5.23888, val loss: 19.64447\n",
      "[711/1000] train loss: 5.19784, val loss: 19.43452\n",
      "[712/1000] train loss: 5.18191, val loss: 19.52979\n",
      "[713/1000] train loss: 5.16155, val loss: 19.50939\n",
      "[714/1000] train loss: 5.20638, val loss: 19.54073\n",
      "[715/1000] train loss: 5.13456, val loss: 19.40842\n",
      "[716/1000] train loss: 5.06969, val loss: 19.52308\n",
      "[717/1000] train loss: 5.13918, val loss: 19.52780\n",
      "[718/1000] train loss: 5.21854, val loss: 19.45250\n",
      "[719/1000] train loss: 5.27611, val loss: 19.46927\n",
      "[720/1000] train loss: 5.15487, val loss: 19.65319\n",
      "[721/1000] train loss: 5.13452, val loss: 19.38559\n",
      "[722/1000] train loss: 5.18339, val loss: 19.55839\n",
      "[723/1000] train loss: 5.14687, val loss: 19.37783\n",
      "[724/1000] train loss: 5.16269, val loss: 19.46597\n",
      "[725/1000] train loss: 5.13844, val loss: 19.46259\n",
      "[726/1000] train loss: 5.16744, val loss: 19.55161\n",
      "[727/1000] train loss: 5.07088, val loss: 19.40005\n",
      "[728/1000] train loss: 5.09358, val loss: 19.35368\n",
      "[729/1000] train loss: 5.15395, val loss: 19.43568\n",
      "[730/1000] train loss: 5.14320, val loss: 19.44811\n",
      "[731/1000] train loss: 5.08585, val loss: 19.34270\n",
      "[732/1000] train loss: 5.13862, val loss: 19.43602\n",
      "[733/1000] train loss: 5.19951, val loss: 19.40270\n",
      "[734/1000] train loss: 5.12720, val loss: 19.52510\n",
      "[735/1000] train loss: 5.07667, val loss: 19.24459\n",
      "[736/1000] train loss: 5.04407, val loss: 19.30512\n",
      "[737/1000] train loss: 5.00051, val loss: 19.41149\n",
      "[738/1000] train loss: 5.08017, val loss: 19.39312\n",
      "[739/1000] train loss: 5.02871, val loss: 19.47948\n",
      "[740/1000] train loss: 4.99588, val loss: 19.49274\n",
      "[741/1000] train loss: 5.05613, val loss: 19.45150\n",
      "[742/1000] train loss: 5.08337, val loss: 19.49369\n",
      "[743/1000] train loss: 4.97484, val loss: 19.37373\n",
      "[744/1000] train loss: 5.02776, val loss: 19.35825\n",
      "[745/1000] train loss: 5.07749, val loss: 19.19632\n",
      "[746/1000] train loss: 5.07659, val loss: 19.19792\n",
      "[747/1000] train loss: 4.96202, val loss: 19.44541\n",
      "[748/1000] train loss: 4.96852, val loss: 19.41953\n",
      "[749/1000] train loss: 4.97040, val loss: 19.34694\n",
      "[750/1000] train loss: 5.03417, val loss: 19.19656\n",
      "[751/1000] train loss: 4.99053, val loss: 19.42932\n",
      "[752/1000] train loss: 5.01166, val loss: 19.22026\n",
      "[753/1000] train loss: 4.98938, val loss: 19.35529\n",
      "[754/1000] train loss: 4.97857, val loss: 19.35972\n",
      "[755/1000] train loss: 5.01818, val loss: 19.20795\n",
      "[756/1000] train loss: 4.99044, val loss: 19.32917\n",
      "[757/1000] train loss: 4.96503, val loss: 19.20382\n",
      "[758/1000] train loss: 5.02681, val loss: 19.37646\n",
      "[759/1000] train loss: 4.98428, val loss: 19.09173\n",
      "[760/1000] train loss: 4.96639, val loss: 19.25429\n",
      "[761/1000] train loss: 4.86439, val loss: 19.25420\n",
      "[762/1000] train loss: 4.87200, val loss: 19.19468\n",
      "[763/1000] train loss: 4.90482, val loss: 19.02221\n",
      "[764/1000] train loss: 5.00595, val loss: 19.05107\n",
      "[765/1000] train loss: 4.97137, val loss: 19.14630\n",
      "[766/1000] train loss: 4.86151, val loss: 19.11530\n",
      "[767/1000] train loss: 4.82305, val loss: 19.26703\n",
      "[768/1000] train loss: 4.84673, val loss: 19.29173\n",
      "[769/1000] train loss: 4.89347, val loss: 19.24956\n",
      "[770/1000] train loss: 4.86390, val loss: 19.27404\n",
      "[771/1000] train loss: 4.84798, val loss: 19.06921\n",
      "[772/1000] train loss: 4.83986, val loss: 19.37864\n",
      "[773/1000] train loss: 4.84599, val loss: 19.22170\n",
      "[774/1000] train loss: 4.84974, val loss: 19.35305\n",
      "[775/1000] train loss: 4.84718, val loss: 19.20714\n",
      "[776/1000] train loss: 4.83675, val loss: 19.21585\n",
      "[777/1000] train loss: 4.94215, val loss: 19.42742\n",
      "[778/1000] train loss: 4.86655, val loss: 19.22912\n",
      "[779/1000] train loss: 4.85364, val loss: 19.24479\n",
      "[780/1000] train loss: 4.85125, val loss: 19.31853\n",
      "[781/1000] train loss: 4.83073, val loss: 19.28433\n",
      "[782/1000] train loss: 4.72257, val loss: 19.09160\n",
      "[783/1000] train loss: 4.79667, val loss: 19.13261\n",
      "[784/1000] train loss: 4.91817, val loss: 19.09640\n",
      "[785/1000] train loss: 4.70050, val loss: 19.26314\n",
      "[786/1000] train loss: 4.78368, val loss: 19.16522\n",
      "[787/1000] train loss: 4.73732, val loss: 19.20438\n",
      "[788/1000] train loss: 4.77569, val loss: 19.43924\n",
      "[789/1000] train loss: 4.83845, val loss: 19.37810\n",
      "[790/1000] train loss: 4.79903, val loss: 19.15568\n",
      "[791/1000] train loss: 4.74485, val loss: 19.13422\n",
      "[792/1000] train loss: 4.76876, val loss: 19.22328\n",
      "[793/1000] train loss: 4.81686, val loss: 19.25875\n",
      "[794/1000] train loss: 4.72466, val loss: 19.25533\n",
      "[795/1000] train loss: 4.75315, val loss: 19.22107\n",
      "[796/1000] train loss: 4.77718, val loss: 19.40552\n",
      "[797/1000] train loss: 4.77080, val loss: 19.12552\n",
      "[798/1000] train loss: 4.71313, val loss: 19.17266\n",
      "[799/1000] train loss: 4.80270, val loss: 19.33709\n",
      "[800/1000] train loss: 4.74447, val loss: 19.20725\n",
      "[801/1000] train loss: 4.74025, val loss: 19.02212\n",
      "[802/1000] train loss: 4.66888, val loss: 19.22413\n",
      "[803/1000] train loss: 4.57668, val loss: 19.03832\n",
      "[804/1000] train loss: 4.73182, val loss: 19.21552\n",
      "[805/1000] train loss: 4.71060, val loss: 19.09826\n",
      "[806/1000] train loss: 4.67136, val loss: 19.13112\n",
      "[807/1000] train loss: 4.69081, val loss: 19.23293\n",
      "[808/1000] train loss: 4.65433, val loss: 19.20475\n",
      "[809/1000] train loss: 4.70391, val loss: 19.28105\n",
      "[810/1000] train loss: 4.77341, val loss: 19.01277\n",
      "[811/1000] train loss: 4.69230, val loss: 18.95903\n",
      "[812/1000] train loss: 4.73951, val loss: 19.20256\n",
      "[813/1000] train loss: 4.69624, val loss: 19.28811\n",
      "[814/1000] train loss: 4.70148, val loss: 19.11891\n",
      "[815/1000] train loss: 4.71337, val loss: 19.21624\n",
      "[816/1000] train loss: 4.33708, val loss: 19.11718\n",
      "[817/1000] train loss: 4.65735, val loss: 19.09689\n",
      "[818/1000] train loss: 4.68848, val loss: 19.22399\n",
      "[819/1000] train loss: 4.83019, val loss: 19.19819\n",
      "[820/1000] train loss: 4.68175, val loss: 19.13430\n",
      "[821/1000] train loss: 4.63963, val loss: 19.14794\n",
      "[822/1000] train loss: 4.69522, val loss: 19.06083\n",
      "[823/1000] train loss: 4.62368, val loss: 18.95052\n",
      "[824/1000] train loss: 4.64024, val loss: 19.05812\n",
      "[825/1000] train loss: 4.58732, val loss: 19.09640\n",
      "[826/1000] train loss: 4.60592, val loss: 19.07383\n",
      "[827/1000] train loss: 4.72104, val loss: 19.32629\n",
      "[828/1000] train loss: 4.62256, val loss: 19.18546\n",
      "[829/1000] train loss: 4.60224, val loss: 19.15664\n",
      "[830/1000] train loss: 4.64902, val loss: 18.94994\n",
      "[831/1000] train loss: 4.69431, val loss: 19.00387\n",
      "[832/1000] train loss: 4.67643, val loss: 19.09080\n",
      "[833/1000] train loss: 4.54614, val loss: 19.03001\n",
      "[834/1000] train loss: 4.58748, val loss: 19.01873\n",
      "[835/1000] train loss: 4.58698, val loss: 19.06982\n",
      "[836/1000] train loss: 4.53603, val loss: 19.00315\n",
      "[837/1000] train loss: 4.56453, val loss: 19.05971\n",
      "[838/1000] train loss: 4.58201, val loss: 19.07119\n",
      "[839/1000] train loss: 4.54752, val loss: 19.08931\n",
      "[840/1000] train loss: 4.69292, val loss: 19.02783\n",
      "[841/1000] train loss: 4.66955, val loss: 18.97518\n",
      "[842/1000] train loss: 4.56135, val loss: 18.95872\n",
      "[843/1000] train loss: 4.60912, val loss: 19.09892\n",
      "[844/1000] train loss: 4.53498, val loss: 19.11144\n",
      "[845/1000] train loss: 4.56471, val loss: 18.91287\n",
      "[846/1000] train loss: 4.59331, val loss: 18.93967\n",
      "[847/1000] train loss: 4.56383, val loss: 18.85841\n",
      "[848/1000] train loss: 4.58447, val loss: 19.11792\n",
      "[849/1000] train loss: 4.50902, val loss: 19.04822\n",
      "[850/1000] train loss: 4.55117, val loss: 19.27812\n",
      "[851/1000] train loss: 4.62822, val loss: 19.03504\n",
      "[852/1000] train loss: 4.49340, val loss: 19.11036\n",
      "[853/1000] train loss: 4.46986, val loss: 19.17562\n",
      "[854/1000] train loss: 4.47896, val loss: 19.09027\n",
      "[855/1000] train loss: 4.43729, val loss: 18.89134\n",
      "[856/1000] train loss: 4.48775, val loss: 19.06202\n",
      "[857/1000] train loss: 4.35515, val loss: 18.95515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[858/1000] train loss: 4.46637, val loss: 18.95109\n",
      "[859/1000] train loss: 4.41232, val loss: 18.85729\n",
      "[860/1000] train loss: 4.45692, val loss: 19.00338\n",
      "[861/1000] train loss: 4.40956, val loss: 19.14867\n",
      "[862/1000] train loss: 4.44918, val loss: 19.06706\n",
      "[863/1000] train loss: 4.41903, val loss: 18.92680\n",
      "[864/1000] train loss: 4.51095, val loss: 18.96242\n",
      "[865/1000] train loss: 4.03824, val loss: 19.02380\n",
      "[866/1000] train loss: 4.41245, val loss: 18.92746\n",
      "[867/1000] train loss: 4.44646, val loss: 18.97765\n",
      "[868/1000] train loss: 4.37227, val loss: 19.09720\n",
      "[869/1000] train loss: 4.40131, val loss: 18.94051\n",
      "[870/1000] train loss: 4.39616, val loss: 18.84769\n",
      "[871/1000] train loss: 4.44092, val loss: 18.91760\n",
      "[872/1000] train loss: 4.39228, val loss: 18.95735\n",
      "[873/1000] train loss: 4.40894, val loss: 18.88721\n",
      "[874/1000] train loss: 4.34209, val loss: 18.91818\n",
      "[875/1000] train loss: 4.40184, val loss: 18.99997\n",
      "[876/1000] train loss: 4.35567, val loss: 19.07567\n",
      "[877/1000] train loss: 4.43374, val loss: 19.16016\n",
      "[878/1000] train loss: 4.39841, val loss: 19.07493\n",
      "[879/1000] train loss: 4.33314, val loss: 18.97519\n",
      "[880/1000] train loss: 4.35131, val loss: 19.03761\n",
      "[881/1000] train loss: 4.40844, val loss: 18.74823\n",
      "[882/1000] train loss: 4.31197, val loss: 18.90081\n",
      "[883/1000] train loss: 4.37114, val loss: 18.89776\n",
      "[884/1000] train loss: 4.34831, val loss: 18.81790\n",
      "[885/1000] train loss: 4.26618, val loss: 18.82512\n",
      "[886/1000] train loss: 4.29012, val loss: 18.98139\n",
      "[887/1000] train loss: 4.27075, val loss: 19.02700\n",
      "[888/1000] train loss: 4.42366, val loss: 18.93634\n",
      "[889/1000] train loss: 4.30160, val loss: 18.97293\n",
      "[890/1000] train loss: 4.27052, val loss: 18.96602\n",
      "[891/1000] train loss: 4.34167, val loss: 19.04486\n",
      "[892/1000] train loss: 4.28464, val loss: 18.73910\n",
      "[893/1000] train loss: 4.33000, val loss: 18.91395\n",
      "[894/1000] train loss: 4.28799, val loss: 18.96462\n",
      "[895/1000] train loss: 4.28083, val loss: 19.08877\n",
      "[896/1000] train loss: 4.30669, val loss: 18.92116\n",
      "[897/1000] train loss: 4.34102, val loss: 19.00313\n",
      "[898/1000] train loss: 4.32014, val loss: 18.95439\n",
      "[899/1000] train loss: 4.29540, val loss: 18.80012\n",
      "[900/1000] train loss: 4.14311, val loss: 18.88166\n",
      "[901/1000] train loss: 4.24738, val loss: 18.85250\n",
      "[902/1000] train loss: 4.26131, val loss: 18.92624\n",
      "[903/1000] train loss: 4.21332, val loss: 18.91063\n",
      "[904/1000] train loss: 4.23523, val loss: 18.91887\n",
      "[905/1000] train loss: 4.24667, val loss: 19.04962\n",
      "[906/1000] train loss: 4.33295, val loss: 19.00861\n",
      "[907/1000] train loss: 4.61208, val loss: 19.24354\n",
      "[908/1000] train loss: 4.32997, val loss: 18.94936\n",
      "[909/1000] train loss: 4.26077, val loss: 19.04930\n",
      "[910/1000] train loss: 4.23259, val loss: 18.82463\n",
      "[911/1000] train loss: 4.28941, val loss: 18.84149\n",
      "[912/1000] train loss: 4.22736, val loss: 18.87519\n",
      "[913/1000] train loss: 4.28844, val loss: 18.97106\n",
      "[914/1000] train loss: 4.25796, val loss: 18.90342\n",
      "[915/1000] train loss: 4.22179, val loss: 18.81037\n",
      "[916/1000] train loss: 4.23024, val loss: 18.70662\n",
      "[917/1000] train loss: 4.15598, val loss: 18.96693\n",
      "[918/1000] train loss: 4.17158, val loss: 18.76964\n",
      "[919/1000] train loss: 4.25799, val loss: 19.05044\n",
      "[920/1000] train loss: 4.16991, val loss: 18.79415\n",
      "[921/1000] train loss: 4.12954, val loss: 18.90582\n",
      "[922/1000] train loss: 4.18068, val loss: 18.87246\n",
      "[923/1000] train loss: 4.21137, val loss: 19.00282\n",
      "[924/1000] train loss: 4.23249, val loss: 18.87229\n",
      "[925/1000] train loss: 4.08100, val loss: 18.78569\n",
      "[926/1000] train loss: 4.18981, val loss: 19.17107\n",
      "[927/1000] train loss: 4.18971, val loss: 18.83928\n",
      "[928/1000] train loss: 4.25066, val loss: 18.98785\n",
      "[929/1000] train loss: 4.21300, val loss: 18.83485\n",
      "[930/1000] train loss: 4.07068, val loss: 18.80126\n",
      "[931/1000] train loss: 4.16819, val loss: 18.97231\n",
      "[932/1000] train loss: 4.16190, val loss: 18.80708\n",
      "[933/1000] train loss: 4.14708, val loss: 18.74503\n",
      "[934/1000] train loss: 4.10336, val loss: 18.83164\n",
      "[935/1000] train loss: 4.16636, val loss: 18.71857\n",
      "[936/1000] train loss: 4.09663, val loss: 18.86251\n",
      "[937/1000] train loss: 3.87677, val loss: 18.67707\n",
      "[938/1000] train loss: 4.13447, val loss: 18.88616\n",
      "[939/1000] train loss: 4.23930, val loss: 18.87364\n",
      "[940/1000] train loss: 4.18117, val loss: 18.80052\n",
      "[941/1000] train loss: 4.05143, val loss: 18.70054\n",
      "[942/1000] train loss: 4.13237, val loss: 18.79354\n",
      "[943/1000] train loss: 4.05371, val loss: 18.68492\n",
      "[944/1000] train loss: 4.15948, val loss: 18.73507\n",
      "[945/1000] train loss: 4.14887, val loss: 18.81403\n",
      "[946/1000] train loss: 4.02916, val loss: 18.80215\n",
      "[947/1000] train loss: 4.08691, val loss: 18.85962\n",
      "[948/1000] train loss: 4.13097, val loss: 18.66689\n",
      "[949/1000] train loss: 4.11304, val loss: 18.86025\n",
      "[950/1000] train loss: 4.00992, val loss: 18.84937\n",
      "[951/1000] train loss: 4.07679, val loss: 18.73893\n",
      "[952/1000] train loss: 4.05660, val loss: 18.76049\n",
      "[953/1000] train loss: 4.10541, val loss: 18.86764\n",
      "[954/1000] train loss: 4.08694, val loss: 18.80291\n",
      "[955/1000] train loss: 4.09046, val loss: 18.80378\n",
      "[956/1000] train loss: 4.07122, val loss: 18.76432\n",
      "[957/1000] train loss: 4.06336, val loss: 18.72904\n",
      "[958/1000] train loss: 4.17724, val loss: 18.63361\n",
      "[959/1000] train loss: 4.05991, val loss: 18.75096\n",
      "[960/1000] train loss: 4.07736, val loss: 18.61893\n",
      "[961/1000] train loss: 4.08502, val loss: 18.97375\n",
      "[962/1000] train loss: 4.23392, val loss: 18.70020\n",
      "[963/1000] train loss: 4.03675, val loss: 18.63345\n",
      "[964/1000] train loss: 4.06135, val loss: 18.71102\n",
      "[965/1000] train loss: 3.99992, val loss: 18.84142\n",
      "[966/1000] train loss: 4.00122, val loss: 18.72152\n",
      "[967/1000] train loss: 3.98440, val loss: 18.82846\n",
      "[968/1000] train loss: 3.99648, val loss: 18.79004\n",
      "[969/1000] train loss: 4.01871, val loss: 18.81845\n",
      "[970/1000] train loss: 4.01152, val loss: 18.63645\n",
      "[971/1000] train loss: 4.03093, val loss: 18.72933\n",
      "[972/1000] train loss: 4.07137, val loss: 18.77313\n",
      "[973/1000] train loss: 4.04989, val loss: 18.63814\n",
      "[974/1000] train loss: 3.95827, val loss: 18.63761\n",
      "[975/1000] train loss: 4.01146, val loss: 18.73398\n",
      "[976/1000] train loss: 4.03464, val loss: 18.56674\n",
      "[977/1000] train loss: 4.07480, val loss: 18.71301\n",
      "[978/1000] train loss: 4.00997, val loss: 18.61578\n",
      "[979/1000] train loss: 4.00624, val loss: 18.75076\n",
      "[980/1000] train loss: 4.03091, val loss: 18.89625\n",
      "[981/1000] train loss: 3.95540, val loss: 18.62217\n",
      "[982/1000] train loss: 4.00347, val loss: 18.84100\n",
      "[983/1000] train loss: 4.04559, val loss: 18.66368\n",
      "[984/1000] train loss: 3.97723, val loss: 18.86254\n",
      "[985/1000] train loss: 3.96390, val loss: 18.62995\n",
      "[986/1000] train loss: 3.98584, val loss: 18.72016\n",
      "[987/1000] train loss: 3.97137, val loss: 18.86807\n",
      "[988/1000] train loss: 3.97810, val loss: 18.68199\n",
      "[989/1000] train loss: 3.98086, val loss: 18.75476\n",
      "[990/1000] train loss: 3.92490, val loss: 18.62593\n",
      "[991/1000] train loss: 3.95046, val loss: 18.76452\n",
      "[992/1000] train loss: 3.95162, val loss: 18.70442\n",
      "[993/1000] train loss: 3.98341, val loss: 18.81176\n",
      "[994/1000] train loss: 3.98608, val loss: 18.58790\n",
      "[995/1000] train loss: 3.99581, val loss: 18.63240\n",
      "[996/1000] train loss: 3.88652, val loss: 18.77193\n",
      "[997/1000] train loss: 3.90313, val loss: 18.69317\n",
      "[998/1000] train loss: 3.94661, val loss: 18.65957\n",
      "[999/1000] train loss: 3.92739, val loss: 18.64804\n",
      "[1000/1000] train loss: 3.82375, val loss: 18.58533\n"
     ]
    }
   ],
   "source": [
    "# 학습: 두단계 - 학습 + 검증\n",
    "for epoch in range(n_epoch):\n",
    "    ###################################################\n",
    "    # 학습단계 - 모델을 train 모드로 변경\n",
    "    ###################################################\n",
    "    boston_model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for X, y in boston_train_loader:\n",
    "        # X, y를 device로 이동\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # 1. 모델 추정\n",
    "        pred = boston_model(X)  # 순전파(forward propagation) - forward() 메서드 실행\n",
    "        \n",
    "        # 2. loss 계산\n",
    "        loss = loss_fn(pred, y)  # 추정값이 '먼저' 그리고 정답이 '나중'\n",
    "        \n",
    "        # 3. 모델의 파라미터를 업데이트\n",
    "        ## 3.1 파라미터들의 기울기를 초기화\n",
    "        optimizer.zero_grad()\n",
    "        ## 3.2 역전파(back propagation) - 파라미터 기울기를 계산(각각의 grad속성에 저장)\n",
    "        loss.backward()\n",
    "        ## 3.3 파라미터 업데이트 => 1step\n",
    "        optimizer.step()\n",
    "    \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    # 평균 loss 구하기\n",
    "    train_loss /= len(boston_train_loader)  # step 수로 나누기\n",
    "    \n",
    "    # 1 epoch 학습 끝. => 1 epoch 학습한 것을 가지고 아래의 검증단계에 들어감\n",
    "    \n",
    "    ###################################################\n",
    "    # 검증단계 - 모델을 평가모드로 변경\n",
    "    ###################################################\n",
    "    boston_model.eval()  # evaluation mode로 변환\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    # 역전파를 통한 gradient 계산이 필요없기 때문에 일시적으로 grad_fn을 구하지 않도록 처리.\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in boston_test_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            # 1. 추정\n",
    "            pred_val = boston_model(X_val)\n",
    "            # 2. loss 계산\n",
    "            val_loss += loss_fn(pred_val, y_val).item()\n",
    "            \n",
    "        val_loss /= len(boston_test_loader)\n",
    "        \n",
    "    # 1 epoch 검증 끝\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(f'[{epoch + 1}/{n_epoch}] train loss: {train_loss:.5f}, val loss: {val_loss:.5f}')\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAGzCAYAAADnmPfhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABjA0lEQVR4nO3dd3gU5drH8W96JY0QSCANQpXeEaUoiKJgAwuiohwVK1gPWPFYUOzie+TYsKCgWBEUpAjSe1E6ofeQkEJ6svP+MWTDEgIBkkx28/tc117MPFP23gGyd57qZhiGgYiIiIiTcLc6ABEREZFzoeRFREREnIqSFxEREXEqSl5ERETEqSh5EREREaei5EVEREScipIXERERcSpKXkRERMSpKHkRERERp2Jp8jJ27FgaNWpETEwMLVq0YOrUqfZja9asoXPnzsTGxtKsWTNmzZplYaQiIiJSVbhZuTzA/Pnzufjii/Hy8uKvv/6iT58+7Nu3D29vb5o2bcrnn39Or169mD9/Ptdeey2bN2+mTp06VoUrIiIiVYClycupatasyaJFi/jrr7/4/fff+emnn+zH+vfvz+WXX87w4cMtjFBERESs5ml1AAA5OTmMHz+eDh060KRJE15//XW6du3qcE6nTp1Yu3btaa/Pzc0lNzfXvm+z2UhJSaFmzZq4ublVZOgiIiJSTgzDICMjg6ioKNzdS+/ZYmnykpiYSI8ePdi/fz8dO3bkm2++AeDgwYNcdtllDudGRESwbNmy095nzJgxvPjiixUer4iIiFS8vXv3Uq9evVKPW5q8NGjQgL1795KTk8OPP/5Ily5dWLhwIQUFBZzamlVYWFhqLcqoUaN47LHH7PtpaWnExMSwd+9egoKCKvQzVDW9357PwbQcvv5XR1pFh1odTtW1fzV82R9whweXQVCk1RGJiFR76enpREdHU6NGjTOeVyWajXx9fRk0aBBz5szhiy++ICwsjKNHjzqck5SUVGpnXR8fH3x8fEqUBwUFVbvkpXZ4KIdz0tiSUsilF1Wvz35OgnpAw4thzxKY/Tjc+SuoiVFEpEo4W5ePKjXPi4+PD35+frRr147Fixc7HFu8eDFdunSxKDLncXmT2gDM+OeQxZE4gTa3m3/uWgC7F1kbi4iIlJllycv+/fuZNGkSBQUFAPz111/89NNPDBw4kNtuu405c+Ywd+5cAH777Tc2bdrEwIEDrQrXaXSIN5uKUrPzLY7ECbS6Fep1NLfXTbY2FhERKTPLmo18fHz49NNPGT58ODVq1CAuLo6ffvqJRo0aATB58mQeeOABUlJSSEhI4NdffyUgIMCqcJ1GkK8XABk5Sl7Oyt0deo2Gz/vChp/hqrHg7W91VCIichaWJS/h4eHMnj271ON9+vRh8+bNlRiRayhKXtKzCyyOxEnEdIGQGEjdA+smQYehVkck4vQKCwvJz9cvUFKSl5cXHh4eF3yfKtFhV8pPkJ/5V5qdX0h+oQ0vjyrVranqcXeH9nfD7NEw50VoMxg8S3b+FpGzMwyDQ4cOkZqaanUoUoWFhIRQp06dC5qHTcmLiwn0Kf4rzcgpICzA28JonESXh2HpeDh+CBLnQuOrrI5IxCkVJS4RERH4+/trklBxYBgGWVlZHDlyBIDIyPOfokLJi4vx9HAnwNuDzLxCMnLylbyUhYcnXHQ9LPsQJt0CI/6BkGiroxJxKoWFhfbEpWbNmlaHI1WUn58fAEeOHCEiIuK8m5DUpuCCgvzMfi8HUnMsjsSJtLq5ePvv76yLQ8RJFfVx8fdXp3c5s6J/IxfSL0rJiwvqEBcGwNR1ByyOxIlEtTH7vgBsn2ttLCJOTE1Fcjbl8W9EyYsL6ppgVtkeTMu2OBInc/HD5p97l0JuhrWxiIhIqZS8uKCi4dKZuRoufU7C6kPNhmArgGmPwSnra4mInKqwsJA+ffqwc+fO875Hjx49mDxZE2WeCyUvLijQ1+yHnZGj5OWcXfKo+eff38HfU6yNRUQq3IQJE3jiiSfO+3oPDw9mzpxJfHx8OUYlZ6PkxQUVDZc+rpqXc9fmNujwL3N706/WxiIiFW737t0cP3681OM2m60So5GyUvLigpS8XKCLrjf/3LdSTUci58kwDLLyCix5GWX8fzt48GDeffddvv76a+Li4vj222/ZtWsXvr6+fPPNNyQkJPDss8+Sn5/PfffdR1xcHNHR0XTv3p0dO3bY7+Pm5sahQ+ZiuEOGDOG5557j9ttvJzY2lri4OKZMObda3GnTptGxY0fi4+NJSEjgmWeeITc3FzCbqZ588kkaNWpEZGQkN9100xnLXZXmeXFBRc1Gmbnmf2L1/j9HUW3A0w8yDsCOedCgp9URiTid7PxCmj0/05L33vifPvh7n/3rbeLEiYwePZpDhw4xfvx4AHbt2kVBQQHr169n27ZtGIZBTk4OnTp14oMPPsDLy4tHHnmEZ555hkmTJp32vp999hnTp0/nq6++4pdffmHw4MH06dOHoKCgs8Y0d+5chg0bxrRp02jdujWpqancfPPNPPvss7zxxht88cUXrFixgg0bNuDl5cXWrVsBSi13Vap5cUFFNS/5hQbbj5ReHSql8A6Alid+a9lqzQ9fEbFOYWEhw4cPx83NDXd3d/z9/bn77rs5fvw4y5YtIzAwkA0bNpR6/Y033kjr1q0BuPbaa/H392fLli1leu93332XZ555xn59SEgIb7/9Nh9//DFgLmp8+PBhewfhosWMSyt3Vap5cUEBJ/3GMXbmFj6+o72F0TipuEtg9RewZ7HVkYg4JT8vDzb+p49l730hvLy8HKau37lzJ3fccQc2m42mTZtSUFBAXl5eqddHRUU57IeGhpKZmVmm905MTKRJkyYOZfXr1yctLY2MjAwGDRpESkoKV1xxBRdddBFjxoyhZcuWpZa7KtW8uCB39+JmovxCdTY7L7EXg5s7HFyn2heR8+Dm5oa/t6clrwttKnd3d/xqfOGFF+jTpw+LFi3ik08+oX///hd0/zOJjo5m27ZtDmU7d+4kPDycGjVq4ObmxsMPP0xiYiLXXnstPXr0ICcnp9RyV6XkxUU92susMqxdw9fiSJxUcD3o/IC5/c1NkHnU2nhEpEKEhYXZO98WFJx+kENubi7Hjh0D4OjRo7zzzjsVFs+DDz7ISy+9xLp16wBITU3liSee4NFHzWkcVq1aRUpKCh4eHlxxxRVkZWVhs9lKLXdVSl5clK+X+Veb78L/eCtcx3uLtz++DArPfx0OEamabr75ZlJSUoiLi2Pq1KmnPWf06NEsWLCAevXq0a9fP2655ZYKi6dfv368+eab3HnnncTGxtK1a1d69erFyJEjAdiyZQstW7YkPj6eG2+8ke+++87ep+Z05a7KzSjrmDInkp6eTnBwMGlpaWXq3e2KPl24k5embaR/qyjev7WN1eE4r9//DcvMUQjc8QvU72FpOCJVVU5ODjt37iQ+Ph5fX9X4SunO9G+lrN/fqnlxUV4eZptvgWpeLsxVr0PLEytO71xgbSwiIgIoeXFZHic67eYXulzFWuWL72b+ufMva+MQERFAyYvL8jrRW77QpuTlghUlL/tWwJHN1sYiIiJKXlyV54lmoy2HMkjKyLU4GicXEgP1ewIGLP+f1dGIiFR7Sl5clKeH+Ve7PzWbK96Zb3E0LqDjPeaf6vciImI5JS8uyvOkieqOZeWXeaEyKUXsxYAbJG+DjENWRyMiUq0peXFRJycvYC6SJhfALxTqtDC3Jw+CvLJN9S0iIuVPyYuL8vJw/KtNy9YEaxes/V2AG+xfBb88aHU0IiLVlpIXF1XUYbeIkpdy0P5uuPPEDJwbp8LxI9bGIyJSTSl5cVGepywslpal5KVcxHeDuu3AKIR1k62ORkQssGvXLoeZYR977DF+/vnnUs9/7bXXGDJkyHm9V0pKCj179iQjI+O8ri+LIUOG8Nprr1XY/SuCkhcXpZqXCtR6kPnnvNcgM9naWETEcm+//TbXXXddudxrwoQJPPHEE/b9sLAw/vzzT2rUqFEu93cVSl5c1KkddtNzTr9aqpyHtndCrSaQnwmbTr+Qm4jI+di9ezfHjx+3OowqT8mLizq1w25WnpKXcuPhBa1uNbdXTQCtHyVSkmGYo/KseJVxaoh+/frxxhtvOJQNGTKEV155heTkZAYNGkRsbCzR0dH069eP5OTT17T26NGDyZOLm5EnTZpE8+bNiY6OpkePHuzZs8fh/KeffpqEhARiYmJo164dq1atAmDw4MG8++67fP3118TFxfHtt9+WaKLKzs5m1KhRNGnShNjYWDp06MDMmTPtx0ePHs0999zD8OHDqV+/PnXr1uX9998v0/MosnjxYnr06EH9+vWJj4/n/vvvJz093X587NixNG3alLp169K5c+ezllcEzwq9u1jm1GajrDwNlS5XbW6H+a/DwXXwz/fQ8iarIxKpWvKz4NUoa9776QPgHXDW04YOHcoLL7zAk08+CcDx48eZOnUqGzdu5Pjx49x000189dVXAAwYMIA333yTMWPGnPGes2bNYuTIkfzxxx80btyYdevW0atXL66++mr7OdHR0axfvx5/f3/efvttHnroIZYsWcLEiRMZPXo0hw4dYvx4czX7Xbt2Odz/vvvuIzc3l5UrVxIYGMiSJUvo168fc+bMoVWrVgBMmTKF7777jvfee49Vq1Zx8cUX07dvXxISEs76TDZt2kT//v2ZMmUKPXv2JDs7m2HDhjF06FCmTJnC3Llz+fTTT1m9ejUBAQFs3boVoNTyiqKaFxd1aoddJS/lLKAmdLzX3N72h7WxiMh5ueaaazh8+DD//PMPAN9//z29evWiTp06xMbGct1115GcnMzSpUsJCwtjw4YNZ73nuHHjGDlyJI0bNwagVatW3H333Q7n3H///dhsNlatWoW7u3uZ7guQnJzM5MmT+eijjwgMDASgS5cu3HXXXUyYMMF+Xrdu3bjiiisAaNeuHa1bt2bNmjVleo8PP/yQoUOH0rNnTwD8/PwYN24cP/74I6mpqfj4+JCamsrmzeY6b40aNQIotbyiqObFRZWYpE7NRuWvfndY9C7sXmI2HbnrdwEROy9/swbEqvcuA09PT+644w4mTpzIa6+9xueff84LL7wAwOrVq7nnnnsIDg6mUaNGHDt2jLy8vLPeMzExkaZNmzqUhYaGcvjwYcAcPXT77bdz+PBhWrRoQVBQUJnuC7Bjxw4iIyMJDg52KK9fvz6zZ8+270dFOdZ4hYaGkplZtok1ExMTGTBggENZUFAQ4eHh7N27l65du/LOO+8wePBgwsPDeeWVV+jWrVup5RVFP21d1KnNRh8v2Mlvfx+0KBoXFd0JvGtA+j7Y9IvV0YhULW5uZtONFS83t7PHd8Ldd9/NpEmT2LFjB0eOHLHXOIwYMYJHH32UuXPnMn78eC655JIy3S88PLxEH5cdO3bYt999910iIyNZuXIlEyZM4M477yxzrNHR0Rw6dKhEh96dO3dSv379Mt/nbO+xbds2h7KMjAxSUlKIj48HYNCgQWzatIknnniCvn37sm/fvjOWVwQlLy7q1GYjgCemrLMgEhfmHQCdTjQdTR0OKTvOfL6IVDlNmjQhOjqakSNHcu+999rLc3NzOXbsGGD2O/n444/LdL+bbrqJMWPGsHfvXgD+/PNPhzlgcnNzSUtLw2azkZmZyauvvupwfVhYmD3ZKShwrDGvU6cO11xzDffee689gVm2bBlff/01w4YNO7cPXor77ruP8ePHM2/ePABycnIYPnw4d911F4GBgWzatIn9+/cDZvOUj48POTk5pZZXFCUvLsp2mt726vdSAbqPhHodITcNfnnY6mhE5DwMHTqU6dOnO9SCvPXWW4wfP56YmBjuueceBg8eXKZ7DRs2jBtvvJGLL76YuLg4vvjiCx58sHg5kUcffZTk5GSio6Pp2rUr1157rcP1N998MykpKcTFxTF1asmpGD7//HPCw8Np2bIl9evXZ+TIkfz00080aNDgPD+9ozZt2jBlyhRGjhxJTEwMrVu3JjIy0j5i6eDBg1xyySXExMTQvXt3xo4dS0JCQqnlFcXNcMHlhtPT0wkODiYtLY2goCCrw7FERk4+LUY7diT18nBj2yt9LYrIhaXugffbgi0f7p0HUW2sjkik0uXk5LBz507i4+MdhvaKnOpM/1bK+v2tmhcXVcPXix/uv5gB7erZywJ81D+7QoTEQNN+5va6b62NRUSkGlDy4sLaxYbSoFagfT8nX81GFablzeafyz6E7bPPfK6IiFwQJS8u7sZ2de3bOfk2ZvxzCBdsKbRewuUQWNvc/ul+KNTQdBGRiqLkxcVF1PBl80tX2veHTVzFpwt3WhiRi/LwgvsWmNuZR+CQRnaJiFQUJS/VgK+Xh8N+YpIW/aoQNWpD4xNTgH98GRzbbW08IhZQza6cTXn8G1HyUg3l5mshwQrT9Jri7QVvWheHSCXz8vICICsry+JIpKor+jdS9G/mfGj4STWUkav+GBXmouvhlwfBsMGepVZHI1JpPDw8CAkJ4ciRIwD4+/vjdg4z3YrrMwyDrKwsjhw5QkhICB4eHme/qBRKXqoJdzewnaipy8jJtzYYV+blBw+ugA/aQdp+MIxzmqpcxJnVqVMHwJ7AiJxOSEiI/d/K+VLyUk14e7qTc6K56LhqXipW0IlF0fIzIScN/EIsDUeksri5uREZGUlERAT5+folSUry8vK6oBqXIkpeqgkfTw978pKRo+SlQnn7g18YZKfA7kXQ5GqrIxKpVB4eHuXyBSVSGnXYrSbeu6W1ffu4kpeKV6uJ+efkQbDhZ7P5SEREyoWSl2qiR+MIZj3aDYD0nHwNZ6xo/ccBJ/q6TLkTlnxgaTgiIq5EyUs1ElPTH3c3yC80SMrItToc1xaeADd+Urz/x7Pw21PWxSMi4kKUvFQjPp4eRIX4AXD1uIWkZalDXYVqMQDuX1y8v/x/YNP6UiIiF0rJSzUTHx4AQFJGLq/8ttHiaKqB2hfBDR8X76fssC4WEREXoeSlmokO87dvz9uSZGEk1UjLm6BeB3N7/bdg0wzHIiIXQslLNRMZ5GvfztR8L5Wn9SDzz7/egN8etzYWEREnp+SlmqnhWzy1j5+35mGoNK0GgZfZZMfKz2Dzb9bGIyLixJS8VDMnrzCdX6jh0pXGyxfumQvegeb+D0MhK8XamEREnJSSl2qmb8tIPN3N+UfSsvMpKFT/i0oT0QQe3wI+QZCfBVOGWB2RiIhTsix5mTt3Ll27diUhIYEGDRowbtw4+7HmzZtTu3Zt4uLiiIuLo0uXLlaF6XKCfL1Y83xv+/5T36+3MJpqyCcQer1gbu9aALnHrY1HRMQJWZa8/PLLL3z22Wds376dWbNm8frrrzNjxgz78cmTJ7Nr1y527drFkiVLrArTJQX6FPd7+XHNfgsjqaY6/AsCIsCwwYHVVkcjIuJ0LEte3nvvPRo3bgxA/fr1uemmm5g7d679eEhIiEWRuT43NzeH/bRsTVZX6WIvNv9c+I61cYiIOKEq0+clKSmJ4OBg+76Sl8rzzqytVodQ/fQYZf6ZOBf2r7I2FhERJ1Mlkpfly5czbdo0Bg0y58Jwc3OjR48e9hqZrVvP/OWam5tLenq6w0vO7Ob20fbtr5butjCSaiqiCdRuYW5PvBGyj1kbj4iIE7E8eZk8eTL9+/fniy++ID4+HoB169axe/duNmzYQJs2bejVqxfHj5fesXHMmDEEBwfbX9HR0aWeK6bXB7Skce0aAFzdItLiaKqpGz8Gv1Azcfm/TpCy0+qIREScgmXJS2FhIQ888AAvvvgiM2fOpH///sVBuZth+fn5MWrUKAICAli2bFmp9xo1ahRpaWn21969eys8flcwqFMMAIWG5nuxRERTaHy1uX38MHzSS2sfiYiUgefZT6kYI0aMYMeOHaxcuZKAgIAznltQUIC3t3epx318fPDx8SnvEF2el4eZJM785xCFNgMPd7ezXCHlrkFPWDvR3M46CuO7Qbs7zRWpo9pYG5uISBVlSc1LTk4OH374IRMmTCiRuBw5coTVq83ho4WFhbz66qu4u7vToUMHK0J1aZ4eZrJSYDN4Yso6i6OppprfCPctgLhLzf28DFjygSawExE5A0uSlx07dmCz2ejSpYt9Irq4uDj69OlDTk4Od9xxB5GRkSQkJLB27VpmzpyJr6/v2W8s58Tbo/iv/6c1+zmuhRorn5sbRLaEIdPg6reLy4/tgiSNAhMROR1Lmo2aNWuGzVb6tPT//PNPJUZTfXl5OOauf2w4xA1t61kUjdBhKDS5Br7oB0e3QOIcCI0Dz9KbTEVEqiPLRxuJdbw8HPu4ZOSo5sVyNWpD29vN7Rkj4fVY+G8X+PQKLeQoInKCkpdq7NSal3wt0lg1tLq1eDs/C45shL3LYNbz1sUkIlKFKHmpxkomLxoyXSUEhENofMnyNV/B0W2VH4+ISBWj5KUaO7XZqEA1L1XHrZOh5c1w+8/QfxxEtjbLP2gPB9ZYGZmIiOWUvFRjnqfUvLw1a6tGHFUVEU3gho/MeWDa3gGXPlZ8bMHbpV8nIlINKHmpxrw9Sv71v/XHFgsikbNqdi30e9/c3jTVXNBRRKSaUvJSjXl5lpxRd8KiXRw9nmtBNHJWbe+A5gPM7a+uh8Q/rY1HRMQiSl6qsVM77Bb5afX+So5EysTNDS4/acTRt4Oti0VExEJKXqoxL/fT//WnZudVciRSZqGx5kR2AHnHYdcia+MREbGAkpdq7HTNRgA7j2ZWciRyTm75Gur3MLc/7wu7FloajohIZVPyUo2V1my05VBGJUci52zABAisbW7/8RwYmqNHRKoPJS/VmKd76TUvOfmFlRyNnBP/MLjnxIijA6thzURr4xERqURKXqqxU+d5KWIzYPXuY5UcjZyz4Hpw8cPm9tSHYM5LcIYFT0VEXIWSl2os0MeTZ69uSmxNf3vZ5U0iAFiw/ahVYcm56PUixF1qbi94E767HXbMtzYmEZEKpuSlmvvXpfX516X17fst6gUDcCA126qQ5Fy4e5hLCRTVwGyeBl9dB8d2WxqWiEhFUvIi2GzFnT3rhZq1MEfSNVGd0/AJhF7/gYTe5r5hg7/GWhuTiEgFUvIi2E4aqRIV7AvAkYwcq8KR8+HuDoO+g84PmPtrJsLqL62NSUSkgih5EYdRthFBZvKSmJSpBMbZuLtDl4eK96c+DH9/D7nHrYtJRKQCKHkRx5qXEF/7dsdX5jBp+R4rQpLzFVwX+n9QvP/DUHi7GWQmWxeTiEg5U/IiDjUv/t6eXHZixBHAqB//tiAiuSBtb4fBP0BgHXM/Nw3eqA/vNIej262NTUSkHCh5EYeaF4BLEsItikTKTUIveGQ1XHRDcVnaXvigHbzREP56w2xSsmkyQhFxPkpehA7xYQ774TV8HPY/XbizMsOR8uIdAAMnOCYwAJlHYO7LZpPSn69aE5uIyAVQ8iK0jQnlu/u6sGTUZQCEB3o7HH9p2kYMrZ3jvPq/XzwK6VQL3qzcWEREyoGSFwGgY3wYkcF+AMTWDChxfFdyVmWHJOXFpwZcOQauegPCGkDLmx2P52tUmYg4FyUvUkLdED+e7tvEoWxHkobbOr1O95r9YG74CP41t7j8t8chda91cYmInCMlL3Ja93Zr4LC/WzUvrqVeO4i9xNxeMxE+7Gp23lXzoIg4ASUvUiYzNxyyOgQpb7EXF2/npsH/usNrMTD9Ca2NJCJVmpIXKdXsx7oz9JJ4AJbtTCEpQ+sduZSLrgN3z+L9w39Dbjqs+Bh+vh8O/aOh1CJSJSl5kVIlRATy3DXN7KOPtFyAi6l9EQxbCDedZg2k3YtgfFeY/zrsmA/52WpSEpEqQ8mLnFV4oDnvy/er9lkciZS7iKbQ7Fq48jXw8IaaCY7H578OX/aHV+rAH89aE6OIyCmUvMhZZeQUADBh0S6y89SM4JI63w+j9sNDK6H14NOfs+QDSNGEhSJiPSUvclb7U7Pt2+v2pVoXiFQsT29wc4MOd4O71+nPmXgjFBZUblwiIqdQ8iJnNax78bDpkT+sJzNXX14urW47eP4o3DUDbvsenj4Ij28BnyBISYQJV8Hab2DOf9ShV0Qs4Xn2U6S6G9GrId6e7rw/Zxu7krOYtv4AN3eIsTosqWixXYq3vf0hqjXs/Av2LTdfALWaQsuBloQnItWXal7krHy9PHisdyO6NaoFwOrdqdYGJNaI71aybO/S4u2cdNXEiEilUM2LlNngTjH8tTWJ1XuOWR2KWKHroxAQAb8+Uly24hPIODGB4eZp0PFe6PuGNfGJSLWhmhcps7axoQBsO3KctOx8i6ORSufhCe3uhCcT4Ylt0OQas3zzNPMFsPwjsNnMV5HkRPh9JKTtr/yYRcQlKXmRMgsP9CE+3Fxx+vNFu6wNRqwTEA6BEdB/nNnnxS8MwuoXH/9PqPnatcjcn3gDLPsQpo2wJFwRcT1KXuScjOjVEIBxc7dxLDPP4mjEUv5hcP9ieGoHPLIGep4yid2X18K81+DYLnN/2x9mLQxAYT5s/QM+vQIWvlOpYYuI83MzDNeb8zs9PZ3g4GDS0tIICgqyOhyXYhgGHV+dQ1JGLtMevoTmdYOtDkmqisIC+LgnHFp/5vMSesH22Y5lo9MqLi4RcRpl/f5WzYucEzc3N0L9zQnMlu9MsTgaqVI8POHe+XDfgjOfd2riApCyQyOVRKTMlLzIOfPx9ADgP9M2arkAceTuDpEtYeAXEN4YAuvA8HUQe8mZr3u/Dcx9qXh/7wrYNK1iYxURp6Wh0nLOjp80w+7ulEya1FHTnJziouvMBR/d3Mz9u6abI5Dm/geObILOD4BPIEwcANknavAWvgO4QZOr4dNeJ66bAdEdwd3Dik8hIlWUkhc5Z0ULNQLsOqrkRUpRlLgUcXeHXqMdy0JiipMXgIVvm68iE64E3CCoLgRFwe0/wdYZsPxjGPg5BEVWUPAiUpWp2UjOWUZO8RwvO45mWhiJOL3u/y7DSQak7zOXJPjlQfhhqDmz719jzZFM0x6F1D0VHamIVCFKXuScvXxdc/v2LiUvciGa9IUh0+Ha/0K3J82y1oOh21MQ0azk+Rt/Lt7etRDeawUrP4MJV1dKuCJSNWiotJyXD+cl8vqMzQDsHNMXt1ObCETOVWEBZBwwm5KKTOgLuxeV7fouD0FALbj4YbOPTG4GZByG8ISKiVdEyp2GSkuF6ppQ0749euoGCyMRl+Hh6Zi4AAyYADd8DM8ecSyv1aTk9Us+gNkvwPrv4MBaGFMPPmgH+1dVWMgiYg0lL3JeLooqnpzuiyW7WbVbc75IBahRG1reBJ4+ZnMSQNs74f4lMPhHs6mp1SDHaxa8CR91L97/9ApY+iHsWVY8w++x3bDxl+J9EXEqajaS8/bTmn08+u06AB7o0YCnrjzNb8Mi5aUgFzZOhcZXgk8Nx2Mbp8LUhyDnLDP1hsbDzV/B+JPmnXky0VyvSUQsp2YjqXBXNS8epvrfeYkO87+IlDtPH2g5sGTiAtCsv7nO0tkc2+mYuAB8fg3kndTx3DBgw0+QfqD0+xiG48rZIlKplLzIefP18uCH+y+2728+mG5hNFLt1Ygq3m7Yx2xWuu7Ds1+XtMlMVops/BmmDIHxl57+/KwUeLMh/HB3cZlhmB2ERaRSKHmRC9IuNhR/b3P205MnrxOpdO7uMPgH6D8OBn0LCZdD60FmZ98Bn8FF1xefG3Mi6S5KeHYtMpOSaY+aiQtA1lFYM7Hk+3w7GDKTHGtn5rwIr8XC3uWO526dCTOfMUdSiUi5UZ8XuWC3frSUJTuSqR8ewOzHuuPurmHTUgXl58DicVCQDT2eNhOQpM3w1XVnvs432FzOILIVLBsPO+YVH7vhY6hRB77oZ+4n9DITqCKjT3Rs7/8BtL29PD+NiEsq6/e3lgeQC1ZU87LjaCY/rN7HwPbRFkckchpevtD9yeL9oEgIrA11SxlO7RdmLl2Qkwbzxpz+nke3wrTHTnoP/9OfN/UhiLsEwuLNJqb9qyGiKXiXcr6InJGajeSC5RYUd1z877xEXLAyT1yVuzvcPNFMYgB8guHfu+ChlTB8Lficpeb2rzcg76S+LpumwrstYfN0+KS347nvt4b1U+CfH+CTy2Dqw+X4QUSqFzUbyQW7/r+LWLMn1b6/5rnehAZ4WxeQyLmyFZojjnxqOC4oaRiQfQw+6QUpFTAnzH0LYPtsiO4EcV3L//4iTkbNRlJpsnILHfb3p2YreRHn4u4Bvqf5QenmBv5h8NAKOLjWnLn36Da45FFY9qFZkxLfzewYPGMUHP773N73fyeNaOr7Jhg2iLsUajU2YxKR07Ks2Wju3Ll07dqVhIQEGjRowLhx4+zHdu3aRe/evYmNjSUhIYGJE0/T41+qjFbRwQ77+45lWxSJSAVx9zD7xnQYCle9Zs7822s0PLYBrv8Q4i+F+xfCZc+d/vpr/w+8As78Hr89Ab8/BR92gc+10KTImVhW8/LLL7/w2Wef0bhxY3bs2EG3bt1o2LAhvXv3pl+/fjz++OMMGTKEjRs3cskll9C8eXNat25tVbhyBk/3bUpogDfzNiex5XAG+45lWR2SiDU6DTObmiKamMOw130DCb3NWYFju4KXHxw/Aql74NvbSr/PniXw9/fmy7BB10dgxkg4ngTNb4Q+rzg2b4lUM1Wmz8tjjz2Gp6cnvXr14t///jdr1qyxH3vkkUfw8PDgnXfeKdO91OfFGu/M2sp7c7YR7OfFmud6a8i0yJkU5ML3d8PmaeZ+jSi49094q/HZr31wudm0dL5shVCQA95nqQ0SqWROtzxAUlISwcHBLFmyhK5dHTuuderUibVr15Z6bW5uLunp6Q4vqXxNI81p29Oy8xkwfjHpOfkWRyRShXn6wC1fw2ObYMh0eHyTOWdM7CVnv3bFp5C6F/YshY96wrZZZvmuRfDt7bB7yZmv//JaeOeis68FJVJFVYnkZfny5UybNo1BgwZx8OBBateu7XA8IiKC5OTkUq8fM2YMwcHB9ld0tOYZsULb2FD79uo9qTw1Zb2F0Yg4iaAocw6YIgMnFG/3GAWPrIVGVzles/x/8G5z+KwPHFgNXw+AOf+Bz/uaw7UnXGmed2QzvJEA814rvtZWCLsWmKOoTp5wT8SJWJ68TJ48mf79+/PFF18QHx9PQUFBiXlCCgsLcTtD++6oUaNIS0uzv/bu3VvRYctpRNTw5ab29ez7i7YftTAaEScVGAGj9sOjG6HHSHNiu1snnf26BW857o8Ohv92MmcSnjcGdi+GzGR4u2nxOYlzYd9Ks5+OiBOxrMNuYWEhDz/8MH/++SczZ86kVatWAISFhXH0qOOXXlJSEnXq1Cn1Xj4+Pvj4+FRovFI2of7FQ6RzCgoptBl4qO+LyLnxCTRfRdzc4JZJcGANtLwZfroP9q88t3tOuKpk2arPzddNX8HeZWZH4gETwOPEV0PqXnMivm5PQEjM+X4akXJnWfIyYsQIduzYwcqVKwkIKO401q5dO9544w2HcxcvXkyXLl0qO0Q5D7d0jOHjBTuwGZBfaLBw+1G6N6pldVgizq9JX/MFcMcvMOt5c2bgmg3MRSKLOv6ejyl3mqOaAPYth9gTC1dOHgSH1pvLGQydaZ7jU6P4usMbITRWHX+l0lky2ignJ4fAwED27t1LZGSkw7GsrCwSEhIYO3YsgwcPZuXKlfTv35/ly5dTr169Uu7oSKONrJWSmcfoqRuYuu4AV15Uh/G3t7M6JBHXVpgP0x+D0DjYMsNMQIrU6whG4enXbzqdSx6DwxugblvHNZ28a4At35ywLyQGdsyHL/tDg8vh9h/L9eNI9VXW729LkpeNGzfSvHlzYmIcqyEbN27MzJkzWbVqFffccw/79u2jTp06vP/++/To0aPM91fyYr3lO1O46X/miIefH+xK6+gQawMSqS6O7YY/noHuIwEDAuuAf01z+z9hjueGNzIXlzxXp17X4iZI3gbX/hdqN4NdCyE/Gxr2hoI887091bQvZ1elk5eKpuTFejn5hTR5bgYAV7eI5P9ua2txRCLCgbXmCtd125v9WLz84b+d4fjh8rm/hw/0+Lc58gnMGYfnvmRud3vKXL/JJ8jslBxYGzy8zn5Pw4CMQ+Yq4EXyMs3+P7FdNVmfi1HyouTFcp8u3MlL0zYS4u/Fqmd7q+OuSFW1byUc2WQmNqfqNAyWjS//96zdHAZ9a753cD2o197x+IE15rGsFJj3KtzwMbS8yTz2x3Ow+H24+GHo/ZK5QndeJgz4TMmMk1PyouTFcgWFNtr8ZxYZuQX88mBXWqnpSKRq+6S3Y3+Zpv3gug/Nie9CY81EAsy5ZBJ6w/ZZ5ffeo09MmJebYXYE/uyKkudcNx5a32oOAy/y+JbiWYk9/eDeeebyDADZqeaSDMd2Q3Ddkh2L570OBdlw+QtnT3qyU8Ev5Nw/l5wTJS9KXqqEYV+tYsaGQwAsf+ZyImr4WhyRiJQqMxn++R5qNYF9K6DLg+aX/8kMA/KOm6OOjmyCFZ9ATBfITYffR0JhruP5DyyFNRNhyQdnfu+Yi80J+r68DpI2lX5eZCs4uO7M9xr8AyRthZmjistqNYGr3zITmKg25iR9r8ediHFZccJzOgveMpvCbvsBGvY683tfiH9+MCcW7Pl0ta1BUvKi5KVKmLnhEPd9ZY5yePX6FgzqpLkiRFxe0Zd946vh1m8gaQv8X8ezX+cXBtkpFR/fxQ+bC1x+1MPcv+xZ6PZk6eefXNNz05fQ7FpzaYUDayG+W8lEIz8bVk6Ai6537KtzJoYBL4aY20N+M/sHnSovE2a/CM36O87K7EKcbm0jcU29mxYv9bD1cIaFkYhIpen8AAyaAjd9Ye7XbAhRbcHT16z1AAg/zcKSpSUuvsEQUI7zRS0eV5y4AMx9GRa+A3uWwfbZ8NebZm3Rp31g2qOO1353h7m694ynzaHiSz+EjMPw21PmJH8Afzxr1vr8eM/p399mg+Ufm0ldkayTlsA5tQN1UR3DovfMpSE+v/q8PrYrUc2LVLgpK/fy5PfraRUdwi8Pnua3CRFxfYZhzkfj6e1YPq4dJG93LKsRCRkHze2ivjCbp5uT5hW54xdzzaYm15jz20x9uHJqbc4kqi3cPQNejigua3cX9HvXTFZSdsChv821pYqM2mc2wZ3a3+j5FHD3MJdw+G4I9HreTLKyj504fgzcXa/+Qc1GSl6qjENpOVw6di75hQY/PnAxbWNCz36RiFQPqXvMBSLnvQbp+6HrCHMY98/3Q7ProMUA8zybDVZ/YXbobTMY/E+Zs8Yw4J3mkL7P3L/hE/hrbPF8NE8mws755hw14yu5yWXwjzDxhtMfi+5k9sU5NSa/UPPY1hml37fFTXDDR2XvH7N9NvzyMPR/35yDpwpS8qLkpUp5cso6pqzaxw1t6vL2za2tDkdEqpojm8y1lBr2Pv/Oqmn7zAnyEnpBQDhsnQnf3ATx3eHOqcXnvdcaju2EO6bC+u9g7USz9iYkFmI6Q046LPvQ8d43fGKu83R0C2VSs6E5cV9FG/Kb2Qzm5Qu9Rpuf49huM+kJrmf2vwmKMpd8+E9NsBWY143cYzbHner3f5s1Ya1vM8+1FUCrWyutA7GSFyUvVcpfW5O44zOzSnThv3tSL9Tf4ohEpFpI2gqBtcyajCKpe80mnPrdzRFWe5ZA46vMZhowa3GObILfnoDdi8yyRzdC6u5TFrh0MxOlwjyzVqdIjSgYtgDeaOAYi28I5KSWHmvv/5gdco3C8/+87e+GlZ+VLL/xU/hhaMnyqDbmEPTwRma/mzcTSp4zYAIcXGvO1NziprJ3Qj4PSl6UvFQpycdzaffybAA6xYfx7X1aaFNEnMCaiZCXBZ3uNffTD5rDwpf+11yCoeiLfNdCs2am3RCo39OcE+bkUUqefvDgMnivpbkfVBdaDIRF75r7j6yBsPrmKt+/Dq+cz3aygFrg5gHHD5393DummolfBVDyouSlyuk29k/2pGQBsOa53oQGeJ/lChERJ/bnqzD/degzxpwdOCAcvh9qzqVz7X/houvMFcEvut5xAr2sFHN+nVfqFJfdOtns2Lt9jjnUe9F7xYmPFYavM5uoypmSFyUvVY5hGFw69k/2Hcvmy7s70q1ROQ59FBGpagzDHPZc46QkJC8LjmyEuu3O3o/k7+9hz1K48jXw8Cx5PP0g/PoIbPvD7Ny7d5lZ3vIWWD/ZnPjvho/MpOf12JLXRzQzYzkfERfBsIXlPuKprN/fp3kaIhXDzc2N1tEh7DuWzYYD6UpeRMS1ubk5Ji4A3v4l13EqTYsBxaOtTicoEm6bYtbU+IWa71dUH9HpPohoWjxDcv8PzLWr3DygSV+zNsjD25xMMDfdXPKhiH+42YQVGGHOsjz3Fcg8Yo7cimpj9gc6sgG2/g5NrJlzRsmLVKr64WbVaFHzkYiIXKCTh40X1ebUbet4Ttvbzdeprvs/8889S83RWT2fLrna913TzRFY+1ZA/R5mPyDvAGh0VYnbVRYlL1Kp6oaavwVMWr6HB3o0IDpMo45ERCwX09l8lcY3CBIuN7fb3Vk5MZ2B603PJ1VaZHDxIm93f74CF+xyJSIiFUzJi1SqxnVq2Le3HTnOl0t2WxiNiIg4IyUvUqlqB/ky9aGuXNXc7MT26cKdFkckIiLORsmLVLqW9UJ4Y2ArPNzd2JOSxb5j6rwrIiJlp+RFLBHo40nDiEAAvl62x+JoRETEmSh5EcvUquEDwIfzEknLyrc4GhERcRZKXsQyBYXFI412JWdaGImIiDgTJS9imSf6NLZvX/t/izRsWkREykTJi1imXWwo7ict7ZGSmWddMCIi4jSUvIilTq5rmbP5iGVxiIiI81DyIpY6uaXoqe/Xq+lIRETOSsmLVClLdiRbHYKIiFRxSl7EUhc3qOmw/39/brcoEhERcRZKXsRS793ShieuaMR7t7QGYNH2ZGw2NR2JiEjplLyIpWrV8OGhyxrSu1lte9kzP/9jYUQiIlLVKXmRKsHf29O+PWn5Hpaq74uIiJRCyYtUGa2jQ+zbt3y0lIJCm3XBiIhIlaXkRaqMV69v4bC/YPtRiyIREZGqTMmLVBlhAd4O+3dNWEFqlmbdFRERR+eVvDz11FMkJiYCsGfPHpo1a0ZsbCzLli0r1+Ckegnx9ypR9u7sbRZEIiIiVdl5JS9fffUVDRo0AGDUqFEMHDiQL7/8kieeeKJcg5PqxdfLo0TZP/vTLIhERESqMs+zn1JSjRo1ADh48CDz5s3j888/x8vLi0OHDpVrcFL9PNmnMbM2Hmbt3lQAVu4+xu7kTGJrBlgbmIiIVBnnVfNy6aWXcuedd3LLLbdw33334eXlRVZWFsePHy/v+KSaebBnAj8/2JXBnWPsZb3enm9hRCIiUtWcV/Iybtw44uLi6NOnD08//TQA69atU7ORlJu7usbbt/MLDQ2bFhEROzfDBZfxTU9PJzg4mLS0NIKCgqwOR85DTn4hTZ6bYd//flgX2seFWRiRiIhUtLJ+f2u0kVRJp3beHTB+CfuOZVkUjYiIVCUabSRVVg0fx/7kr/62yaJIRESkKtFoI6myfht+Kav3HGP45LUALEnUekciInKeyUvRaKNdu3ZptJFUmOgwf6LD/Anx9+bOz5aTlp1PalYeIf7eZ79YRERc1gWPNho1ahSg0UZScbrUr0lksC82A16fsdnqcERExGIabSROYemOZG75aCneHu4sffryEusgiYiI86vQ0UYAH374IW3btqVWrVp06tSJb7/99nxvJXJWnevXpEXdYPIKbfy4ep/V4YiIiIXOK3l59913+eSTT3j55ZdZsGABL7zwAm+++SbffPNNeccnYndrR3PW3W+W78EFKwxFRKSMzqvZqFmzZsyZM4fIyEh72YEDB7jmmmtYvXp1uQZ4PtRs5JqO5xbQ8ZXZZOUVMnZAS25qH211SCIiUo4qtNkoJyfHIXEBiIqKIjU19XxuJ1ImgT6e3NfNnF/olembyMjJtzgiERGxwnklL0FBQezZs8ehbPfu3fj6+pZLUCKleeiyBOqG+JGWnc+aPalWhyMiIhY4r+RlxIgRXHPNNfzxxx/s3r2b2bNnc9111/HQQw+Vd3wiDjzc3WgdEwLAxoPp1gYjIiKWOK9J6oYMGYLNZuPJJ58kMTGRmJgYHnjgAR544IHyjk+khIuigpi+/iCv/b6ZG9rUJSJINX4iItVJmTvsLl++/LTlhmHg5uZm3+/YsWP5RHYB1GHXte1NyeLSsX8CcGeXWF68trnFEYmISHko6/d3mWtebr755hJlJyctRUnMjh07zjFUkXMTHebPrR1jmLR8D3M2H+G5a2x4epz3lEUiIuJkNMOuOKXUrDw6j5lDTr4NPy8Plj9zOTV8vawOS0RELkCFz7ArYqUQf29ev7ElANn5hfy67qDFEYmISGWxNHkxDIMvv/ySLl26OJQHBgZSt25d4uLiiIuLY+DAgRZFKFXZta3rUr9WAADbjmRYHI2IiFSW8xptVB5mzJjBk08+SXZ2Np6eJcNYuHAh8fHxFkQmzuTeS+sz8se/Wbs3laPHcwkP9LE6JBERqWCW1bxkZmby+uuv88knn5z2eEhISOUGJE6pbWwoAGv2pNL+5dmkZuVZHJGIiFQ0y5KXG2+8kb59+572mLu7O8HBwWW+V25uLunp6Q4vqR4a1a5BVHDxPC+adVdExPVVyQ67bm5uNGjQgEaNGjF06FAOHDhwxvPHjBlDcHCw/RUdrQX7qhN/n+Jmx2OqeRERcXlVMnk5duwYO3fuZMWKFfj7+9OvXz/ONKJ71KhRpKWl2V979+6txGjFao/3bmTffuy7dRzJyLEwGhERqWhVMnlxdzfDCg4O5r333mPLli1nnPzOx8eHoKAgh5dUH1e1iOSalsWrnH84L9HCaEREpKJVyeTlZDabDZvNhre3t9WhSBX2yvUtaFQ7EIBF249is7nc3IsiInJClUteEhMT2bp1K2B2xB0+fDgdOnRQPxY5o2A/Lybd0xlvT3e2Hj7OK79tsjokERGpIFUueUlJSaFv377UrVuXpk2bkpeXx/fff291WOIEagb68MYAc9bdTxfuZNmOZIsjEhGRiqC1jcTlPDJpDVPXHeCRyxJ47IrGVocjIiJlpLWNpNrqXL8mAO/P3c6fm49YHI2IiJQ3JS/ictrHhdq37/p8hYWRiIhIRVDyIi4noVagw35GTr5FkYiISEVQ8iIux93djf/d3s6+P2XlPgujERGR8qbkRVxSn4vq8EK/ZgD8Z9pGnv7pb4sjEhGR8qLkRVzWZU0i7NvfLNtDQaHNwmhERKS8KHkRlxVbM4DXbmhh31+1+5iF0YiISHlR8iIu7ZaOMdQMMJeWuPmjpRZHIyIi5UHJi7i8nic1H63anWJhJCIiUh6UvIjLe2NASyKDfQF4ZbrWPBIRcXZKXsTlubm58dK1zQFYty+NtGzN+yIi4syUvEi10KtZbRrVDqTQZvDL2v1WhyMiIhdAyYtUGwPbRQMwV+sdiYg4NSUvUm0UrXk0b0sSo378m5TMPIsjEhGR86HkRaqNppHFy6tPWr6HD+ZutzAaERE5X0pepNrw9fLgqSsb2/cnr9jD4fQcCyMSEZHzoeRFqpUHeiSw6tletIoOISuvUJ13RUSckJIXqXZqBvpwQ5u6ALz622bmb02yOCIRETkXSl6kWjp50cY7P1uOzWZYGI2IiJwLJS9SLUWH+ZMQEWjfn7hst4XRiIjIuVDyItXWlPu60Do6BIDF25OtDUZERMpMyYtUW6EB3gzr3gCAGRsOcdP/llBQaLM4KhERORslL1KtRYX42reX70xh1sbDFkYjIiJloeRFqrXIYD+H/S2HMyyKREREykrJi1Rr4YHe3Nox2r7/7uxt/LxGc7+IiFRlSl6kWnNzc2PMDS15a2Are9mIb9daF5CIiJyVkhcRIPKkvi8Az/78t0WRiIjI2Sh5EQHCArwd9icu3cPBtGyLohERkTNR8iICBPt5lSj7cfV+JTAiIlWQkhcRIMi3ZPLyxswt9P9gEYVaOkBEpEpR8iIC+Ht7nLY8KSOXZTs0+66ISFWi5EUEc9RRkeZ1g6gd5GPfH/TJMo7nFlgRloiInIaSF5FTtKgbwrKne9mXDgAYO2Ozmo9ERKoIJS8iJ3h7mP8dLm0YDoCPZ/F/jy+X7Ob5X/6xJC4REXHkaXUAIlXFvCd78M/+NHo3qw1Aq+hgh+NfL9vDobQcxg5oSc1An9PdQkREKoFqXkROiArx44qL6tj7v/RsHFHinDmbj/DfeYmVHZqIiJxEyYtIKdzc3Lihbd0S5X/vT7MgGhERKaLkReQMcgts9u2YMH8AthzKwDDUeVdExCpKXkTOIO+k5GX2Y93x9nQnLTtfTUciIhZS8iJyBifXvHh7utOodiAAHy/YodoXERGLKHkROYPrWkcB0KRODQDev6UNAKlZ+bz2+2ay8jR5nYhIZXMzXPDXx/T0dIKDg0lLSyMoKMjqcMSJ2WwGy3am0CwqyL54431frWTmhsMAeLq78c+LffD1Ov3yAiIiUnZl/f5WzYvIGbi7u9GlQU2HVacvb1rbvl1gM2jy3Az+3HzEivBERKolJS8i5+iyJiXnf7nr8xUs1QKOIiKVQsmLyDkKD/Th2aub0j421KF8+OQ1HMvMsygqEZHqQ8mLyHn416X1+f7+i/nPtRcx9saWRIf5cTg9l4lLd1sdmoiIy9PaRiIX4I4uceaGGzz1/XremrWVHo0jaFEv+IzXiYjI+VPNi0g56Nawln170MdL2Z2caWE0IiKuTcmLSDmoE+zLTe3rAZCRW8B9X61ymJ1XRETKj5IXkXIydkArfnmwK25usPlQBhMW7bQ6JBERl6TkRaQctYoO4f7uDQAY8/tmliSaw6f/2Z/G4fQcK0MTEXEZSl5EylmvZsWT2N368VKe/+Ufrhm3kHu/XGlhVCIirkPJi0g5O3k2XoAvl5jDp9ftS8Nmc7nVOEREKp2SF5FydmrycrIjGbmVGImIiGtS8iJSzs6UvHyyYAeTlu+pxGhERFyPJqkTKWdeHsW/E8TV9GdXcpZ9/5OF5gikDnFhJEQEVnpsIiKuQDUvIhXosia16RAXWqJ84tLdvDNrKzn5hRZEJSLi3FTzIlKBejapxfP9mrFyVwoDxi+xl3++eBcAEUE+3NYp1qLoRESck6U1L4Zh8OWXX9KlSxeH8jVr1tC5c2diY2Np1qwZs2bNsihCkfOz4KmefDakPZeeWDagfVwY0x+5hLmPd3c4b9H2o1aEJyLi1NwMw7Bk7OaMGTN48sknyc7OxtPTk82bNwOQkZFB06ZN+fzzz+nVqxfz58/n2muvZfPmzdSpU6dM905PTyc4OJi0tDSCgoIq8mOInLNZGw9zz0lzvnRNqMmDPRII8femWZT+vYpI9VXW72/Lal4yMzN5/fXX+eSTTxzKJ02aRIcOHejVqxcA3bt3p1u3bnz77bdWhClS7no3q82Wl6+kc/0wABZtT2bQJ8vo+/4CcgvUB0ZE5GwsS15uvPFG+vbtW6J8yZIldO3a1aGsU6dOrF27ttR75ebmkp6e7vASqcp8PD2YfG8X3hjQ0qH8n/36tysicjZVbrTRwYMHqV27tkNZREQEycnJpV4zZswYgoOD7a/o6OiKDlOkXHRvVMth/8YPF5OYdNyiaEREnEOVS14KCgo4tRtOYWEhbm5upV4zatQo0tLS7K+9e/dWdJgi5SIiyJfbOsU4lF3+1nwGf7KM71ftsygqEZGqrcoNlQ4LC+PoUccRGElJSWfsrOvj44OPj09FhyZSIV65vgUt6gYz8se/7WULtx9l4faj7DqayRN9GlsYnYhI1VPlal7atWvH4sWLHcoWL15cYji1iCvpGB922vIP/tzOa79vJq/AVskRiYhUXVUuebntttuYM2cOc+fOBeC3335j06ZNDBw40OLIRCpO/VqBTH/kEpaMuoyGpywbMH5+IiN/XE9KZp5F0YmIVC1VLnmpV68ekydP5oEHHiAiIoKXX36ZX3/9lYCAAKtDE6lQF0UFExnsx4eD29I2JsTh2I+r99N97J8cTMvmtk+WMmHRTmuCFBGpAiybpK4iaZI6cQVPTlnHlDN02t312tWVGI2ISMWr8pPUiciZjerblGf6NrU6DBGRKkfJi0gVFRbgza2nDKM+WdzI6SzYllSJEYmIVA1KXkSqsECf4tkMnr26Kc9f08zh+O2fLufa/1vEL2v3l5gfSUTEVSl5EanimtSpAcAVzepw9yXxJY6v25vK8MlrueWjpSQfz63s8EREKp2SF5Eq7qcHurJk1GXE1PQHINTfC4DYmv60jg6xn7dsZwr//mG9FSGKiFQqjTYScTJ7krPYeDCNPhfV4b0523h39jaH4w0jAukQH8ZjvRsRHqiZp0XEeWi0kYiLiqnpz5XNI3FzcyPYz8tefklCOADbjhznm2V7uOWjpZqZV0RckpIXESd2beu6hPp70btZbb4a2pGn+zaxH9t+5DivTN/In5uPqDOviLgUNRuJOLncgkK8PdztK68v3HaUL5fs4o+Nh+3nDLk4jn9dGk+9UH+O55ort9fw9SrtliIilijr97eSFxEXtDcli0vH/lmi/MX+FzFh0U5y8m3MfaI7/t5VbmF5EanG1OdFpBqLDvPnpvb1qF8rgISTFnp8YeoGdiVncSg9h9W7U60LUETkAqjmRcTFbTucwSu/bWLeFsfZeK9rHUVyZh4pmXl886/OBPurGUlErKWaFxEBoGHtGnx2Z4cS5T+vPcCCbUfZcCCdqesPkJadb0F0IiLnTsmLSDXg7u5GeKA3AGNuaFHi+HM//0OrF//g13UHKjs0EZFzpuRFpJr48f6ufDakPde3qVvqOQ9PWsPwyWtIzcqrxMhERM6NkheRaiKmpj+XNamNj2fxf/uYMH9mPdrNvn4SwC9rD9D6P7MY8/smkjK0VpKIVD1KXkSqmaL5YIo0rF2DGSO6MahTjEP5/+bvoMMrsxn5w3oyctQfRkSqDiUvIgLA/d0bOCw3UGTyir20GP0Hb/+xha+X7bYgMhERR5qhSkQAc26Ylc/2wsPNjaU7k7lrwgpyT1ob6f252wHo1rAW4+cnMn9rEj8/2FWLP4pIpVPNi4jYeXm44+7uxsUNwpn4r06nPefSsX/y9bI97DuWzccLdlRyhCIiSl5EpBQd4sKoHXTmWpX/zd9B/w8WYrO53FyXIlKFKXkRqcZO6btbwsB20fbt/xvU9rTnrN+Xxj1fruSPDYfYcCCtxPG8Ahtr9hyjoNB2mqtFRM6d+ryISKnu6BLL5BV7uK51Xbo3rkXdED+O5xaQX2ijz0V1+GnNfgDmbD7CnM1HAGhUO5C7u8ZzaaNaRAb58uYfW/jorx08cUUjHrqsoZUfR0RchNY2EqmG4kZOByC2pj/zn+x5xnNtNgN3d7OKJq/AhpsbuLu54eHuxv7UbN6dtZUpq/ad9lpvT3fyTur0u+u1q8vpE4iIK9LaRiJSqhf7X4Sflwdv39TqrOcWJS5gJiNeHu54nCirG+LHGwNb0TEu7LTXnpy4AOTkF15A1CIiJiUvItXQnRfH8ffoK2gXe/qk41x5e5btR8nQL1bggpW9IlLJlLyIVFOeHuX33//kjr+vXt8CXy93BneO4a2BrfDz8rAfW7Q9mRaj/+DThTtZsSuFiUt3k5KpdZRE5Nyow66IXLCTlxwY1CnGYamBK5vXYd+xbPq8+xcAx3MLeGnaRvvxsTM2M6hTLAPb16NBrcDKC1pEnJZqXkTkgp1pxHWAjyeN69Tg+2FdTns8PaeA8fMTefuPrexIOl4xAYqIS1HNi4hcMPezzBcD0D4ujM0vXYmPpzs7j2Zy2yfLOJiWYz8+/e+DTP/7IP+7vR19LqpTgdGKiLNT8iIiF8z9bLPdneB7ov9L/VqBLPz3ZXw4bztv/rHV4Zz7vloFQO0gHy5uEI6ftwf/uiSe+PAA0rLzCfbzKrEyNoBhGKctFxHXo+RFRC7Y+eQMHu5u3NutAbkFNsadWPTxZIfTc+2T4H2zbA+BPp4czy3g1etbOPSpScvK58r3/qJtTCj/d9vpZwEWEdeiPi8icsHuubQ+AFeeY3OPt6c7j1/RmK0vX8W0hy8547nHcwsAePqnvxn08VKOpJtNTkt2JHMwLYfpfx9kd3LmeUQvIs5GNS8icsE61a/JsqcvJzzwzAs5lsbb053mdYP549FupGfnM/rXDTSuHUTtIB/+Oy+xxPmLE5MZNnEVb93Ump1HixOWZTtSiK0ZcN6fQ0Scg5YHEJEqyzAM4kf9Vubzr2sdxcOXN9SQaxEnpeUBRMTpndoB99KG4fz3DP1afl57gMvfms/KXSl0GTOHkT+sr+gQRcQCSl5EpEoL9DFbt9+7pTVfDe1E3xaR/HXKYpJDLo5z2B8wfgkH03KYvGIvSRm5vP3HFm78cDE7ko5reQIRF6BmIxGp0vamZLFuXypXt4h0qIm57M157DjR32X7K1cxZMIKFm4/etb7Bft5MWPEpUQG+1VYzCJyftRsJCIuITrMn2taRpVoQio86fcuTw93Phzclts7xzqc43OaBSPTsvPpMmYu1/93ETM3HCLzxCgmEXEeGm0kIk7JdkqlcQ1fL166rjn1awXw4q8bGXdrG7o1rMWkFXvYdyyLiUv3OJy/Zk+qfUK8a1pGMvKqJtQJ8i3XBStFpGIoeRERp2Sznb78zi5xXNMyilo1zGHbw7o3ACDM35v3TzMZHsC09QeZtv4gAGNvbMl7c7ZxW+cYHuiRUP6Bi8gFU58XEXFKnV+dw6ETE9Xteu3qMl2TnVfIQ9+sZs7mI2U638vDjfxCg36tohjRqyHBfl6s2JlCr2a18VINjUi5K+v3t2peRMQpFZ7H711+3h58fEd7er0zn/3Hsnn5uuYE+3lx74nmo1PlF5rv8eu6A/y67oC9fMjFcYzuf9H5BS4iF0zJi4g4pVB/L5Iycs/5Ond3N34ffik2m5nMAPzxaDdiwvxxd3PjpzX7ePW3zaRl55d6j88X72LjwXTeGNCS2kG+9gUnRaRyqNlIRJzSxgPpPPbdWh6/ojG9m9Uu9/tvOJCGj6c7USF+/Pb3IZ6Ysu605wV4e1A31A8fTw/eGNiSJnWKf+bsO5bF7uQsuiaE28sycwtIycwjOsy/3GMWcXZl/f5W8iIiUgYXj5nDgbScMp9/V9c4JizaBcAHg9pwTcsoALqMmcPBtBzmP9lD6zCJnELzvIiIlKMB7eqd0/lFiQvAQ9+sofOrc/hkwQ4OnkiAur8xj7/3pZVniCLVhpIXEZEyeOiyhrxzcyvuuTTeXrZ45GVc36YuLesF4+HuhvcpI5DiahY3DR1Kz+Hl6Zscjvf7YCEpmXkATF6+h//O267lC0TKQB12RUTKwNvTnevb1ONweqK9LNTfm3dubg3AnuQsQgO8mLJyH/+ZtpF/X9mE+3s0oP3Lszl6vPSOxW1fmmUfkg2wdEcKt3WKIS0rn36tonB3Bx9Ps0NwTn4hPp7uJWYbFqlulLyIiJwD95PyBl+v4pqWmBO1LHdeHEfH+DCaRZrt9afOBHw6RYkLwF9bk/hraxIAT/2wnhq+nvz1ZE+2Jx1n4PglPN67EQ9f3rA8PoqI01KzkYjIOXA/qdbjdDUgHu5uNK8bjPuJLKcsycuZZOQU0OalWQwcvwSAt2ZtJa/AhmEYZOUV2JudwFzEct6WI2p6Epen5EVE5Bxc27ouvl7u9GpatuHZhbbiRGLNc70djv3v9nZ0b1SLXk0jAGgVHcKqZ3sx5OK4M96z0bO/0/yFmTR7fiZtX5rF27O2cjAtm6vfX8CQCSt4f87pl0EQcRUaKi0ico6y8grw9fSw166cyUXPzyAzrxAwlzHo/8FC1p8YZVS0rEFuQSGTlu3hqhaR1A7yBWD2xsPsOHqc6FB/7v969TnHOPuxbiRE1OCf/WkkJh3nimZ17JPyiVRVWh5ARKSC+HuX/UfnqcsYnNy/pYiPpwdDusY7lPVqVhswa3e61K/Jkh3JADSpU4MOcWF8tXT3Gd+319t/UTfEj/2p2fayL+7uSGyYP/VC/fD0cMcwDI4ez7MvYiniLJS8iIhUoGHdG/Du7G1c0zISgILCUpbDPoOP7mhHi9F/APDvq5rQs3EEN3eIZu3eVJpFBXHDfxfbz21ZL9hes3Ny4gJw52fL7duv39iCcXO3s+9YNhPu6sDuo5kcSMvhyT6N+eivHWw5lEGAjycv9Gum5Q+kylGzkYhIBSootLFy9zFaR4fg6+XB/K1J3PnZcu7tVp+n+zYt831mbzzM+v1pPNqroUNHYcMweG3GZsIDfLinW33yC230/2ARmw6mc3fXeLYezmDh9qPnHf+5xilyIbQ8gJIXEamiUrPyCPbzqtD5Wmw2A3d3NwzDYNH2ZFbtPsY7s7ee171evb4FkSG+XBQVREQNs0/O3pQsVu85Rv9WUZp3RsqNU/d5eeihh/jqq68IDQ21l82fP5/Y2FgLoxIRKR8h/t4V/h5FnYnd3Ny4pGE4qdl5pz3vnkvjmbR8L8dzC0q919M//Q1AzQBv/nVpfRIiAnliyjrSsvM5nlvAoI4x9gQmv9DGzA2HuLRhLYL9vMr5U4mYqmTNy0MPPUTNmjV58cUXz+t61byIiDjKLSjkX1+spKDQsHf+feKKRjx0mTnh3b5jWczaeJijx3NJysjlu5X7ALMPzT/707CV4ZsiLMDbPu9Mn4tq87/b21fMhxGX5dQ1LwAhISFWhyAi4jJ8PD34amgnNh9K58p3FwAQ6FP8FVAv1J+7Tox4yi+0EVszgB6Na3FRVDBp2flk5RXwxJR1LNqeXOp7nDxh3swNh7nk9bm0qhdCcmYuB9NyaFArkFFXNSE80IfQALP2yTAMDIMyDTsXKeISyUtubi65ucVrh6Snp1dARCIizu/khCXQ9/TNOl4e7jzYM8G+H+znRbCfFxOHduKvbUcZMmE5Ad6evHpDCzbsTyMxKZPZmw6XuM++Y9nsO1Y84ml3chZzNx+x77u7gc2ADnGhvNDvIt6dvZXbOsfSs3FEeXxUcWFVttno+++/x9vbmwYNGjBq1CiuuOKKUs8fPXr0aZuY1GwkIuIoLSufVv8xh12Pu7UN/VpFnfM9lu9MoYavJ00ji3++rtlzjAAfTwJ9PAnw8WTwJ8v4e3/aecX4x6PdiAnzx9fLg7+2JjHm9828en1zmkYG8cXiXfRtEUl0mP/ZbyROx6lHG9lsNtzd3SksLGTmzJkMGjSIOXPm0K5du9Oef7qal+joaCUvIiKnKCi0kfDM7wB8MKgN17Q89+SlLI7nFrAk0Wxi2nggnWNZeSQmHWdPShYFhQZNI4NYty+VpIzTr7gdHuhDr6YRTF6xt8SxppFBDL88gdpBvtQN9SPM3xtPj7OvdpOVV8CaPal0qV9TzVRVlFMnL6e6//77CQ8P56WXXirT+eqwKyJSuriR0wH4v0FtufrE5HlWGjJhOfO2JF3QPWr4epJfaOODW9tyedMI3Nzc2HY4g2nrD3J7l1jCA3147Lu1/Lh6P6P7NSsxo7FUDU7fYfdkBQUFeHtX/NBCEZHqINjPi7TsfNrHhZ795Erw39vasu9YNtGh/rww9R9sBlwUFcTh9FxshkGgjye/rjvAtiPHS71HRo451PtfX64Eij8jwHtzttG4dg22HM4AYPSvG+lUvyZfL9tNdp6NixvUpE6wL10TwgH47e+DhPp706VBzYr82HIBqmTNy8yZM+nduzfu7u788ccf3HLLLSxcuJBmzZqV6XrVvIiIlO54bgEZOflEBvtZHUqZrdubynX/XUTRN5aHuxst6wWzZk9qudzfzQ2++VdnIoJ8uPyt+YA5lPyurvH4e3vw5ZLdNKgVyCUNw8vl/U5nd3ImSRm5tI8Lq7D3qOqcutnoyiuvZPXq1fj7+xMTE8NLL71E9+7dy3y9khcREdeTkpnHzqPHycm32WtJ8gps5BYUknw8jwOp2Qz6ZJnDNXE1/dmVnFVuMSS+2hePCuovU9ScN/fx7tSvFVgh71HVOXWz0YwZM6wOQUREqpiwAG/CAhxrJbw93fH2dKeGrxdx4ebcNMt3pjDrse6EB3rj4+nBksRkbv14KQBP922Cj6cHL0zdgJsb1AnypdBmcKSUjsOnavrcDHo2qcXAdtH4ennw7M9/M7B9NP+6NB4fz/NfwDK3oNC+vX5fWrVNXsqqSiYvIiIi5+N/t7cjr8BGjZPmsKlVo7jPZP3wQHo1q80dXWLtSxoUFNr4cF4iWw5nkFdgIy48gPjwAH5cvY/mdYOZsGiX/fq8QhszNxxm5obieW3emLmFN2Zuse/XC/Vj37FshlwcR5CvJ0MvrU9qVh4v/rqR4Zc3pFV0SIm4D6Xl2Lez8gpLHBdHSl5ERMRl+Hh6lKgBqRngY98umtn35MUkPT3cefjyhiXudWvHGACe6duUlMw8Xpi6gd//OXTWGIom5vt88S4A3p+73X5s7uYjdKlfkyFd41iwLYnLm9amZ+MI9p80md/BtGzkzKpkn5cLpT4vIiJSxGYzqP/0bwAsf/pyIoJ8z/teP67ex2PfrSOihg+XN63NpOV77Me8PdzJK7Sd8z2bRQZRYLOx9bDjaKqrW0Ty7yubMHPDIW5oWxcD8Pf2sC98GR5oJmWGYbDhQDpN6tQ463w36Tn5eLi5EeBTNesunLrD7oVS8iIiIidbuzeV4zkF5TJa6GBaNrVr+HIsK492L88GYESvhozo1YhDaTlsOJBGg1qBzNp4mD82HmLFrmMX/J6n8vF05+t/dSLE34t5W5J4efombm4fzai+TfD0cHdYBgJgR9JxdiVn8vSP/1DD15OZI7pVyYn6lLwoeRERkQqUX2ij4YnZip+6sjEP9Eg47Xn7jmUx459DNI0MIiuvkGZRQaRn53MsM89hdNTAdvXoEBfGUz+sv+DYYmv6c3fXeOrXCiAzt5BhE1c5HF/wVM8SSywYhuHQnGYFpx5tJCIiUtV5ndRE43mGWox6of7869L6DmV1Q/zILSikfq0AfD09+OaeToT4m/1xbuoQDcCWQxk8PGl1ieakstidnMULUzeUevzhSWv4/K4OfLZoF5OX76FVdAgrd6WQEBHITe2j2XIog2E9Gtibpqoa1byIiIicp6K5Wb68uyPdGtU65+ttNgMDzjh3TH6hjfX7Ulm2M4Vf1x3k5vb1GP3rRm7tGOPQ56YiXNownLwCG6H+3qzfl8rFCeEM616fhIgaFfJ+ajZS8iIiIhVs1e5jbD6UzqCOMZXa5FJoM/Bwd6P/BwtZv89cvfuj29vh6+XB/K1JBPl6MXnFHm5qH83wyxuy71g2eYXmEOxeb/9lv4+flwfZ+ec+NPvpvk24t1uD8vkwJ1HyouRFRERc3Mgf1ttX3t712tVlvu5AajapWfk0qVODzYcy+Hntfu7v3oDMvAI+mLudJnVqsPXIcXYdzWTJjmRaR4c4LMVQO8iHJSMvL/dOv+rzIiIi4uKeurIJGbkF3Nw++pyuiwrxIyrEXNuqWVQQzaLMRCE0wJvXbmzpcG5egQ1vT3f+2Z/Gr+sO8L+/dnA4PZd1+1JpE2PN4p5KXkRERJxUWIA3/zeobYW+h7en2TG5ed1gmtcNJsjPiyA/L2JrBlTo+56JkhcREREpswd7nn5IeGU681R8IiIiIlWMkhcRERFxKkpeRERExKkoeRERERGnouRFREREnIqSFxEREXEqSl5ERETEqSh5EREREafikpPUFS3XlJ6ebnEkIiIiUlZF39tnW3bRJZOXjIwMAKKjz22tBxEREbFeRkYGwcHBpR53yVWlbTYbBw4coEaNGuW6RHl6ejrR0dHs3btXq1VXID3nyqNnXTn0nCuHnnPlqahnbRgGGRkZREVF4e5ees8Wl6x5cXd3p169ehV2/6CgIP3HqAR6zpVHz7py6DlXDj3nylMRz/pMNS5F1GFXREREnIqSFxEREXEqSl7OgY+PDy+88AI+Pj5Wh+LS9Jwrj5515dBzrhx6zpXH6mftkh12RURExHWp5kVEREScipIXERERcSpKXkRERMSpKHkpo+zsbO69915iY2OpV68eTz311FmnL5bTmzt3Ll27diUhIYEGDRowbtw4+7Fdu3bRu3dvYmNjSUhIYOLEiQ7XTpo0iaZNm1KvXj169uzJzp07Kzt8p3P//ffTpEkT+/6aNWvo3LkzsbGxNGvWjFmzZjmc/+6775KQkEDdunW5/vrrSU5OruyQnc7y5cvp1q0bsbGxREVF8eOPPwJ61uVp//799OvXj7p161K/fn1eeukl+zE95wtnGAZffvklXbp0cSi/kGebnJzMwIEDiYmJITY2lrfeeqtcA5YyuP/++42hQ4ca+fn5RmpqqtG+fXvj/ffftzosp/TII48YmzdvNgzDMBITE426desav//+u1FQUGA0b97cmDBhgmEYhrFhwwYjNDTUWLNmjWEYhrF48WIjLi7O2L17t2EYhvHKK68Y7dq1s+IjOI09e/YY/v7+RuPGjQ3DMIz09HSjbt26xqxZswzDMIx58+YZwcHBxsGDBw3DMIxvv/3WaNOmjZGcnGwUFBQYw4YNM2644QbL4ncGmzZtMiIjI+3PNDc31zh8+LCedTm77LLLjKeeesqw2WxGcnKy0apVK2PChAl6zuXg999/N5o3b240aNDA/rPCMC7858VVV11ljB492rDZbMb+/fuN2NhYY+rUqeUSs5KXMsjIyDD8/f2N5ORke9kPP/xgtG7d2sKoXMejjz5qPPnkk8bMmTNLPNOHH37YGDFihGEYhnHrrbca7777rv1Yfn6+ERYWZqxdu7ZS43UmN954o/Hggw/afyD973//M6677jqHc/r162d/rl26dDF+/vln+7GkpCTD09PT4d++OLrhhhuMV199tUS5nnX5Cg0NNf7++2/7/jPPPGM8+OCDes7l4PvvvzemT59u/Pnnnw7Jy4U82y1bthi1atUy8vPz7cffeuutEvc7X2o2KoNVq1YRHx9PWFiYvaxTp078888/FBYWWhiZa0hKSiI4OJglS5bQtWtXh2OdOnVi7dq1ACWOe3p60rZtW/txcTR9+nSSk5MZMGCAvexMz7igoICVK1c6HA8PDycuLo6///670uJ2Jjk5OUybNo277rqrxDE96/I1YMAAPvjgA/Ly8ti9eze//PILAwYM0HMuBzfeeCN9+/YtUX4hz3bJkiV07NgRT0/PEteWByUvZXDw4EFq167tUBYREUFBQQFpaWkWReUali9fzrRp0xg0aFCpz7moDfVsx6VYcnIyjzzyCB9++KFD+Zme4dGjRyksLCQ8PPy0x6WkrVu34ufnx59//knLli2pX78+9913H+np6XrW5eyVV15hxowZhIaGEh8fT8+ePenRo4eecwW6kGdb0T+vlbyUQUFBQYnOuUU1LuW5anV1M3nyZPr3788XX3xBfHx8qc+56Bmf7biYDMNg6NChjBgxwqGjLpz5GRYUFNivP91xKSkjI8P+G+jy5ctZt24dSUlJDB8+XM+6HBUWFtK3b19GjBhBWloa+/fvZ926dbz33nt6zhXoQp5tRf+8VvJSBmFhYRw9etShLCkpCV9f3zKtfimOCgsLeeCBB3jxxReZOXMm/fv3B0p/znXq1CnTcTG99tpr5Ofn89BDD5U4dqZnGBoaimEYHDt27LTHpaTw8HDy8/N57bXX8PX1pUaNGowePZqpU6fqWZejuXPnkpeXx4gRI/D09CQyMpK3336bsWPH6jlXoAt5thX981rJSxm0bduWLVu2OPwlLV68mE6dOuHurkd4rkaMGMGOHTtYuXIlrVq1spe3a9eOxYsXO5y7ePFi+9C9U4/n5eWxatUqOnfuXDmBO4n333+fBQsWEBoaSkhICNdccw3btm0jJCTkjM84ICCAxo0bOxw/ePAghw8fdvh7kmKxsbF4e3uTk5NjL3N3d8fX11fPuhzl5eU59J0A8PLyIi8vT8+5Al3Is23Xrh3Lli3DZrOVuLZclEu332qgf//+xrBhw4z8/HwjKSnJaNGihfHTTz9ZHZbTyc7ONjw8PIwDBw6UOJaZmWlERkYaX331lWEYhrFixQojMjLS2Lt3r2EYhvHjjz8acXFxxt69e42CggLj2WefLbee667s5BEEe/fuNUJCQow5c+YYhmEY06dPN2JjY43jx48bhmEYb7/9ttG+fXvj2LFjRm5urnHnnXfaR3vJ6T3wwAPGPffcY+Tn5xs5OTnGDTfcYDz11FN61uUoNTXViIqKMr755hvDMMwRoNdcc40xbNgwPedydOpoowt5tjabzWjVqpXx6quvGoWFhUZiYqIRExNjrFy5slxiVfJSRklJSUb//v2N8PBwIzY21hg3bpzVITmlDRs2GG5ubkZsbKzD64orrjAMwzBWrlxptGnTxqhVq5bRokUL488//3S4fuzYsUZkZKRRu3Zt4+abbzZSUlIs+BTO5dQfSDNmzDAaN25s1KpVy+jSpYuxfv16+7HCwkLj8ccfN2rVqmVERkYaw4YNM3JycqwI22lkZGQYgwcPNiIiIowGDRoYTz31lJGbm2sYhp51efr777+N3r17G7GxsUZ8fLwxYsQIIzMz0zAMPefycurPCsO4sGebmJhodO/e3QgPDzcaNmxofPfdd+UWq1aVFhEREaeiDhsiIiLiVJS8iIiIiFNR8iIiIiJORcmLiIiIOBUlLyIiIuJUlLyIiIiIU1HyIiIiIk5FyYuIiIg4FSUvIuLSRo8ezbBhw6wOQ0TKkZIXERERcSpKXkRERMSpKHkRkUqTkpLC7bffTv369WnYsCFjx44FzKadBx98kKeffpqEhATq1q3LsGHDyMrKsl+7ePFievToQf369YmPj+f+++8nPT3dfvzo0aMMHTqUhg0bEhUVxaBBg+zHCgsLefTRR2nQoAFRUVG8/vrrlfehRaTcKXkRkUphGAbXX3898fHxJCYmsmzZMiZOnMjPP/8MwKRJk2jWrBnbt29n8+bNJCYm8vzzzwOwadMm+vfvzwsvvMCOHTvYuHEjWVlZDB06FICCggKuuOIK6taty8aNGzlw4ADPPPOM/b2/++47+vTpQ2JiItOnT+e5555jy5Ytlf4MRKR8aFVpEakUK1euZMCAAezcuRM3NzcAPvjgA1asWEF8fDxLlixh5syZ9vOXLl3KzTffzO7du3nkkUfw8/NzqDFJT08nNDSU5ORkFixYwPPPP8+aNWtKvO/o0aNZuXIl06ZNs5d17tyZxx9/nIEDB1bgJxaRiuJpdQAiUj3s2LGDw4cPEx8fby/Lz8+nffv2AA7lABERESQnJwOQmJjIgAEDHI4HBQURHh7O3r172bJlCy1atCj1vevVq+ewHxISQmZm5gV9HhGxjpqNRKRSREVF0bhxY3bt2mV/7d+/n19++QXAnqgU2bhxIw0aNAAgOjqabdu2ORzPyMggJSWF+Ph4IiMjSUxMrJwPIiKWU/IiIpWiU6dO5OTk8NFHH1HUWr1mzRp70jF9+nRmz54NwKFDh3juued4+OGHAbjvvvsYP3488+bNAyAnJ4fhw4dz1113ERgYyDXXXMPu3bsZN24cNpsNgFWrVlXyJxSRyqLkRUQqhZeXF9OmTeOnn34iOjqahIQEXnzxRby9vQG44YYb+Oijj4iOjqZ79+7ccccd9g65bdq0YcqUKYwcOZKYmBhat25NZGQk77//PgDBwcHMmTOHGTNmEB0dTXx8PN98841ln1VEKpY67IqI5UaPHs2hQ4cYP3681aGIiBNQzYuIiIg4FSUvIiIi4lTUbCQiIiJORTUvIiIi4lSUvIiIiIhTUfIiIiIiTkXJi4iIiDgVJS8iIiLiVJS8iIiIiFNR8iIiIiJORcmLiIiIOJX/B68bblOSMYrPAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(n_epoch), train_loss_list, label = 'train loss')\n",
    "plt.plot(range(n_epoch), val_loss_list, label = 'validation loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim(3, 30)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장\n",
    "\n",
    "## 모델 전체 저장 및 불러오기\n",
    "- 모델구조, 파라미터 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장하기\n",
    "save_path = 'models/boston_model.pt'\n",
    "torch.save(boston_model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "load_boston_model_1 = torch.load(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BostonModel                              [200, 1]                  --\n",
       "├─Linear: 1-1                            [200, 32]                 448\n",
       "├─Linear: 1-2                            [200, 16]                 528\n",
       "├─Linear: 1-3                            [200, 1]                  17\n",
       "==========================================================================================\n",
       "Total params: 993\n",
       "Trainable params: 993\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.20\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.08\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(load_boston_model_1, (200, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = 0.0\n",
    "load_boston_model_1.to(device)\n",
    "load_boston_model_1.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in boston_test_loader:\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "        # 1. 추정\n",
    "        pred_val = load_boston_model_1(X_val)\n",
    "        # 2. loss 계산\n",
    "        val_loss += loss_fn(pred_val, y_val).item()\n",
    "\n",
    "    val_loss /= len(boston_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.5853328704834"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## state_dict 저장 및 로딩\n",
    "- 모델 파라미터만 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path2 = 'models/boston_state_dict_pt'\n",
    "model_sd = boston_model.state_dict()\n",
    "# model_sd  # 학습 완료된 parameter\n",
    "\n",
    "torch.save(model_sd, save_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.5853328704834"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dict를 로딩\n",
    "## 1. 모델객체를 생성\n",
    "load_boston_model_2 = BostonModel().to(device)\n",
    "## 2. state_dict 불러오기\n",
    "load_sd = torch.load(save_path2)\n",
    "## 3. 불러온 state_dict(파라미터들)을 모델에 덮어 씌우기\n",
    "load_boston_model_2.load_state_dict(load_sd)\n",
    "\n",
    "# 추정 - 평가모드로 변환\n",
    "val_loss = 0.0\n",
    "load_boston_model_2.to(device)\n",
    "load_boston_model_2.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in boston_test_loader:\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "        # 1. 추정\n",
    "        pred_val = load_boston_model_2(X_val)\n",
    "        # 2. loss 계산\n",
    "        val_loss += loss_fn(pred_val, y_val).item()\n",
    "\n",
    "    val_loss /= len(boston_test_loader)\n",
    "    \n",
    "val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 마지막 에폭을 저장한 값임으로 validation 결과값이 overfitting되었을 수 도 있다.\n",
    "- 그래서 validation 결과값이 가장 좋을 때 저장하는 것이 좋다.\n",
    "- 따라서 학습할 때 전과 비교하면서 성능이 좋을 때 validation 결과값을 저장해야한다.(단, 속도가 느려질 수 있다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 분류 (Classification)\n",
    "\n",
    "## Fashion MNIST Dataset - 다중분류(Multi-Class Classification) 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "10개의 범주(category)와 70,000개의 흑백 이미지로 구성된 [패션 MNIST](https://github.com/zalandoresearch/fashion-mnist) 데이터셋. \n",
    "이미지는 해상도(28x28 픽셀)가 낮고 다음처럼 개별 의류 품목을 나타낸다:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>그림</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">패션-MNIST 샘플</a> (Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "이미지는 28x28 크기이며 Gray scale이다. *레이블*(label)은 0에서 9까지의 정수 배열이다. 아래 표는 이미지에 있는 의류의 **클래스**(class)들이다.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>레이블</th>\n",
    "    <th>클래스</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>T-shirt/top</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Trousers</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>2</td>\n",
    "    <td>Pullover</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>3</td>\n",
    "    <td>Dress</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>4</td>\n",
    "    <td>Coat</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>5</td>\n",
    "    <td>Sandal</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>6</td>\n",
    "    <td>Shirt</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>7</td>\n",
    "    <td>Sneaker</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>8</td>\n",
    "    <td>Bag</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>9</td>\n",
    "    <td>Ankle boot</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import torchinfo\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-shirt/top\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "index_to_class = np.array(['T-shirt/top', 'Trousers', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'])\n",
    "print(index_to_class[0])\n",
    "class_to_index = {key:value for value, key in enumerate(index_to_class)}\n",
    "print(class_to_index['T-shirt/top'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # channel first 처리. 0 ~ 1 scaling(= MinMaxScaling), torch.Tensor 변환\n",
    "    transforms.Normalize(mean = 0.5, std = 0.5)  # 표준화((pixel - mean) / std) 작업. -1 ~ 1 scaling(= StandardScaling)\n",
    "])\n",
    "\n",
    "# Dataset Loading\n",
    "fmnist_trainset = datasets.FashionMNIST(root = 'datasets', train = True, download = True, transform = transform)\n",
    "fmnist_testset = datasets.FashionMNIST(root = 'datasets', train = False, download = True, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: datasets\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=0.5, std=0.5)\n",
      "           )\n",
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: datasets\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=0.5, std=0.5)\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(fmnist_trainset)\n",
    "print(fmnist_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_trainset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T-shirt/top': 0,\n",
       " 'Trouser': 1,\n",
       " 'Pullover': 2,\n",
       " 'Dress': 3,\n",
       " 'Coat': 4,\n",
       " 'Sandal': 5,\n",
       " 'Shirt': 6,\n",
       " 'Sneaker': 7,\n",
       " 'Bag': 8,\n",
       " 'Ankle boot': 9}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_trainset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -1.0000,\n",
       "          -1.0000, -0.8980, -0.4275, -1.0000, -1.0000, -0.9922, -0.9686,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -0.9922, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9765, -1.0000,\n",
       "          -0.7176,  0.0667, -0.0039, -0.5137, -0.5765, -1.0000, -1.0000,\n",
       "          -1.0000, -0.9922, -0.9765, -0.9686, -1.0000, -1.0000, -0.9765],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9529, -1.0000,\n",
       "          -0.2000,  0.6000,  0.3804,  0.0510,  0.1294, -0.0353, -0.8196,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -0.9059, -0.9216, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           0.2157,  0.8510,  0.6235,  0.3961, -0.1608,  0.2235,  0.2627,\n",
       "          -0.1451, -0.4980, -0.8196, -0.3961,  0.0196, -0.4353, -0.8824],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -1.0000, -0.4588,\n",
       "           0.6235,  0.7490,  0.7098,  0.6941,  0.6941,  0.2784, -0.0039,\n",
       "          -0.0510, -0.0431,  0.1451,  0.1059, -0.3098,  0.3490, -0.4824],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -0.9922, -0.9922, -0.9922, -1.0000,  0.5686,\n",
       "           0.8196,  0.8196,  0.8275,  0.7961,  0.7490,  0.7490,  0.6863,\n",
       "           0.6706,  0.2863, -0.0039, -0.0353,  0.5373,  0.7961, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.4353,\n",
       "           0.7647,  0.6941,  0.7490,  0.7882,  0.8431,  0.7804,  0.7569,\n",
       "           0.7412,  0.7569,  0.7333,  0.7490,  0.9216,  0.3569, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.5137,\n",
       "           0.7882,  0.7098,  0.6706,  0.5529,  0.4118,  0.6627,  0.6471,\n",
       "           0.6549,  0.6706,  0.7490,  0.7255,  0.9059,  0.5843, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -0.9922, -0.9765, -1.0000, -0.9059,  0.7176,\n",
       "           0.7255,  0.6627,  0.7098,  0.5059,  0.3255,  0.7804,  0.6314,\n",
       "           0.7098,  0.7569,  0.6627,  0.7725,  0.5451,  0.6392, -0.5922],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -0.9529, -1.0000, -0.2235,  0.9137,\n",
       "           0.7412,  0.7255,  0.7098,  0.5922,  0.5529,  0.7333,  0.6863,\n",
       "           0.6706,  0.7412,  0.7255,  0.9216, -0.0667,  0.3098, -0.5608],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -0.9686, -1.0000, -1.0000, -0.5686,  0.8510,\n",
       "           0.7882,  0.8039,  0.7882,  0.8824,  0.8196,  0.6706,  0.7098,\n",
       "           0.7490,  0.8353,  0.7020,  0.7020,  0.6392, -0.2784, -1.0000],\n",
       "         [-1.0000, -1.0000, -0.9922, -0.9686, -0.9529, -0.9451, -0.9843,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.8588,  0.7725,\n",
       "           0.7020,  0.7490,  0.7412,  0.7176,  0.7412,  0.7333,  0.6941,\n",
       "           0.7490,  0.7961,  0.6863,  0.7098,  1.0000, -0.3961, -1.0000],\n",
       "         [-1.0000, -0.9765, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -0.5137,  0.1373,  0.6000,  0.7882,  0.6235,\n",
       "           0.6706,  0.7333,  0.7098,  0.6314,  0.6549,  0.7098,  0.7569,\n",
       "           0.7490,  0.7176,  0.6863,  0.7569,  0.9137,  0.2471, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -0.8588, -0.6549, -0.3569,\n",
       "          -0.1608,  0.4824,  0.7882,  0.7255,  0.7412,  0.7020,  0.7725,\n",
       "           0.5686,  0.6078,  0.6549,  0.8039,  0.7569,  0.8353,  0.3804,\n",
       "           0.4745,  0.9608,  0.9451,  0.8275,  0.8667,  0.6863, -1.0000],\n",
       "         [-1.0000, -0.5529,  0.4667,  0.6314,  0.7569,  0.7333,  0.7569,\n",
       "           0.6314,  0.6000,  0.6784,  0.6314,  0.6392,  0.5686,  0.2471,\n",
       "           0.9216,  0.5137,  0.6157,  0.7490,  1.0000,  1.0000,  0.7333,\n",
       "           0.8353,  0.7333,  0.6549,  0.7255,  0.8196,  0.9294, -1.0000],\n",
       "         [-0.9765,  0.5843,  0.7882,  0.7569,  0.7333,  0.6549,  0.6549,\n",
       "           0.6784,  0.6078,  0.6078,  0.6078,  0.7255,  0.8824, -0.3725,\n",
       "           0.1765,  1.0000,  0.7961,  0.7333,  0.4745,  0.2078,  0.4980,\n",
       "           0.6471,  0.6000,  0.6392,  0.7412,  0.7882,  0.7647, -1.0000],\n",
       "         [-0.2314,  0.8275,  0.5529,  0.6471,  0.7412,  0.7961,  0.7961,\n",
       "           0.8353,  0.9529,  0.7255,  0.5216,  0.6863,  0.7020,  0.8902,\n",
       "          -0.4902, -0.4275, -0.1686, -0.0824,  0.3176,  0.7176,  0.7333,\n",
       "           0.6863,  0.7020,  0.7490,  0.7490,  0.7569,  0.7961, -0.7725],\n",
       "         [-0.4118,  0.6000,  0.6627,  0.6000,  0.5137,  0.6078,  0.6549,\n",
       "           0.7647,  0.6941,  0.4510,  0.5451,  0.6157,  0.5529,  0.6706,\n",
       "           0.8824,  0.5294,  0.7804,  0.9216,  0.8745,  0.7490,  0.7098,\n",
       "           0.6627,  0.6392,  0.7412,  0.7255,  0.7333,  0.8039, -0.4745],\n",
       "         [-0.6235,  0.5922,  0.4353,  0.5216,  0.6706,  0.5451,  0.4510,\n",
       "           0.4902,  0.5216,  0.5059,  0.5843,  0.6784,  0.7176,  0.7333,\n",
       "           0.7255,  0.8510,  0.7647,  0.6941,  0.5608,  0.6157,  0.4588,\n",
       "           0.4196,  0.3882,  0.3490,  0.4196,  0.6078,  0.6157, -0.0980],\n",
       "         [-1.0000, -0.0431,  0.7176,  0.5137,  0.4039,  0.3412,  0.4353,\n",
       "           0.5373,  0.6000,  0.6471,  0.6706,  0.6235,  0.6549,  0.6471,\n",
       "           0.5686,  0.5373,  0.5216,  0.4980,  0.5294,  0.4980,  0.5529,\n",
       "           0.5059,  0.3804,  0.2235,  0.3098,  0.3882,  0.6471, -0.2784],\n",
       "         [-1.0000, -1.0000, -0.4196,  0.4824,  0.6627,  0.4980,  0.3725,\n",
       "           0.3490,  0.3725,  0.4196,  0.4510,  0.4745,  0.4824,  0.4745,\n",
       "           0.5137,  0.5529,  0.6000,  0.6392,  0.6471,  0.6471,  0.6549,\n",
       "           0.4745,  0.4745,  0.5216,  0.5059,  0.6941,  0.3333, -1.0000],\n",
       "         [-0.9843, -1.0000, -1.0000, -1.0000, -0.4824,  0.5686,  0.7412,\n",
       "           0.8588,  0.8745,  0.8980,  0.9294,  0.9059,  0.9137,  0.7333,\n",
       "           0.7255,  0.5137,  0.4980,  0.4039,  0.4275,  0.4275,  0.4196,\n",
       "           0.3804,  0.3020,  0.3176, -0.2235, -0.5451, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.6863, -0.5216, -0.6549, -0.4353, -0.6784, -0.7255, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = fmnist_trainset[0]\n",
    "print(y)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[0], cmap ='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "fmnist_train_loader = DataLoader(fmnist_trainset, batch_size = 128, shuffle = True, drop_last = True)\n",
    "fmnist_test_loader = DataLoader(fmnist_testset, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(28 * 28, 2048)\n",
    "        self.lr2 = nn.Linear(2048, 1024)\n",
    "        self.lr3 = nn.Linear(1024, 512)\n",
    "        self.lr4 = nn.Linear(512, 256)\n",
    "        self.lr5 = nn.Linear(256, 128)\n",
    "        self.lr6 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 10)   # out_feature: 10개(class 별 확률)\n",
    "        \n",
    "        # torch.flatten() => (batch_size, channel, height, width) -> (batch, c*h*w)\n",
    "        # nn.Flatten()    => \n",
    "        # torch.flatten()과 nn.Flatten()은 같은 개념이나 사용할 때 설정이 다름으로 유의해야 한다.\n",
    "    def forward(self, X):\n",
    "        out = nn.Flatten()(X)  # start_dim = 1 => default\n",
    "        out = nn.ReLU()(self.lr1(out))\n",
    "        out = nn.ReLU()(self.lr2(out))\n",
    "        out = nn.ReLU()(self.lr3(out))\n",
    "        out = nn.ReLU()(self.lr4(out))\n",
    "        out = nn.ReLU()(self.lr5(out))\n",
    "        out = nn.ReLU()(self.lr6(out))\n",
    "        out = self.output(out)\n",
    "        \n",
    "        # nn.Softmax()(out) => CrossEntropyLoss()에서 softmax함수가 적용됨으로 사용안함.\n",
    "        # 다중분류의 output는 Softmax()함수로 계산해서 확률로 만들어서 출력함.\n",
    "        # 모델에서는 Linear를 통과한 결과를 반환.\n",
    "        # loss함수인 CrossEntropyLoss()에서 softmax를 적용함.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModel(\n",
       "  (lr1): Linear(in_features=784, out_features=2048, bias=True)\n",
       "  (lr2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (lr3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (lr4): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (lr5): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (lr6): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_model = FashionMNISTModel()\n",
    "f_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FashionMNISTModel                        [128, 10]                 --\n",
       "├─Linear: 1-1                            [128, 2048]               1,607,680\n",
       "├─Linear: 1-2                            [128, 1024]               2,098,176\n",
       "├─Linear: 1-3                            [128, 512]                524,800\n",
       "├─Linear: 1-4                            [128, 256]                131,328\n",
       "├─Linear: 1-5                            [128, 128]                32,896\n",
       "├─Linear: 1-6                            [128, 64]                 8,256\n",
       "├─Linear: 1-7                            [128, 10]                 650\n",
       "==========================================================================================\n",
       "Total params: 4,403,786\n",
       "Trainable params: 4,403,786\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 563.68\n",
       "==========================================================================================\n",
       "Input size (MB): 0.40\n",
       "Forward/backward pass size (MB): 4.14\n",
       "Params size (MB): 17.62\n",
       "Estimated Total Size (MB): 22.16\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(f_model, (128, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 28, 28])\n",
      "torch.Size([2, 10])\n",
      "tensor([ 0.0271,  0.0494, -0.1612,  0.1103,  0.0734,  0.1449,  0.1506,  0.0339,\n",
      "         0.0345,  0.1023], grad_fn=<SelectBackward0>)\n",
      "0.5652162432670593\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# 추정\n",
    "i = torch.ones((2, 1, 28, 28), dtype = torch.float32)\n",
    "print(i.shape)\n",
    "\n",
    "y_hat = f_model(i)\n",
    "print(y_hat.shape)\n",
    "print(y_hat[0])\n",
    "print(y_hat[0].sum().item())\n",
    "print(y_hat[0].argmax(dim = -1).item())  # label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0968, 0.0990, 0.0802, 0.1052, 0.1014, 0.1089, 0.1095, 0.0974, 0.0975,\n",
      "        0.1043], grad_fn=<SoftmaxBackward0>)\n",
      "1.0\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# 모델이 추정한 결과의 class를 알고 싶을 경우는 softmax를 계산할 필요가 없다.\n",
    "# 모델의 추정확률을 알고 싶을 경우 softmax를 계산함.\n",
    "y_hat2 = nn.Softmax(dim = -1)(y_hat[0])\n",
    "print(y_hat2)\n",
    "print(y_hat2.sum().item())  # 결과가 1에 가깝다.\n",
    "print(y_hat2.argmax(dim = -1).item())  # label은 softmax 함수 통과하기 전과 똑같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 / 1000] train loss: 0.58862 val loss: 0.45202 val_accuracy: 0.83900\n",
      "저장: 1 epoch - 이전 best_score: inf, 현재 score: 0.45201538465445557\n",
      "[2 / 1000] train loss: 0.40346 val loss: 0.40907 val_accuracy: 0.85240\n",
      "저장: 2 epoch - 이전 best_score: 0.45201538465445557, 현재 score: 0.4090650762561001\n",
      "[3 / 1000] train loss: 0.35796 val loss: 0.39002 val_accuracy: 0.86040\n",
      "저장: 3 epoch - 이전 best_score: 0.4090650762561001, 현재 score: 0.3900228105013884\n",
      "[4 / 1000] train loss: 0.33094 val loss: 0.38394 val_accuracy: 0.86160\n",
      "저장: 4 epoch - 이전 best_score: 0.3900228105013884, 현재 score: 0.3839438344481625\n",
      "[5 / 1000] train loss: 0.30544 val loss: 0.37135 val_accuracy: 0.86670\n",
      "저장: 5 epoch - 이전 best_score: 0.3839438344481625, 현재 score: 0.371348441778859\n",
      "[6 / 1000] train loss: 0.28959 val loss: 0.35993 val_accuracy: 0.87630\n",
      "저장: 6 epoch - 이전 best_score: 0.371348441778859, 현재 score: 0.35993222628213184\n",
      "[7 / 1000] train loss: 0.27183 val loss: 0.34571 val_accuracy: 0.88220\n",
      "저장: 7 epoch - 이전 best_score: 0.35993222628213184, 현재 score: 0.3457116627523416\n",
      "[8 / 1000] train loss: 0.25802 val loss: 0.36776 val_accuracy: 0.86880\n",
      "[9 / 1000] train loss: 0.24984 val loss: 0.33087 val_accuracy: 0.88630\n",
      "저장: 9 epoch - 이전 best_score: 0.3457116627523416, 현재 score: 0.33086512013798275\n",
      "[10 / 1000] train loss: 0.23246 val loss: 0.34730 val_accuracy: 0.88040\n",
      "[11 / 1000] train loss: 0.22145 val loss: 0.32497 val_accuracy: 0.88550\n",
      "저장: 11 epoch - 이전 best_score: 0.33086512013798275, 현재 score: 0.3249688531401791\n",
      "[12 / 1000] train loss: 0.21108 val loss: 0.32949 val_accuracy: 0.89220\n",
      "[13 / 1000] train loss: 0.19727 val loss: 0.35043 val_accuracy: 0.88730\n",
      "[14 / 1000] train loss: 0.19213 val loss: 0.32825 val_accuracy: 0.88930\n",
      "[15 / 1000] train loss: 0.18249 val loss: 0.34345 val_accuracy: 0.88640\n",
      "[16 / 1000] train loss: 0.17373 val loss: 0.38429 val_accuracy: 0.88570\n",
      "[17 / 1000] train loss: 0.16841 val loss: 0.35797 val_accuracy: 0.89460\n",
      "[18 / 1000] train loss: 0.15471 val loss: 0.40378 val_accuracy: 0.88550\n",
      "[19 / 1000] train loss: 0.14946 val loss: 0.37338 val_accuracy: 0.89040\n",
      "[20 / 1000] train loss: 0.14141 val loss: 0.39945 val_accuracy: 0.88750\n",
      "[21 / 1000] train loss: 0.13376 val loss: 0.39471 val_accuracy: 0.89240\n",
      "조기종료: epoch - 21, 0.32497에서 개선이 안됨.\n",
      "학습에 걸린 시간: 877.5175807476044초\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "import time\n",
    "\n",
    "# 모델 생성 + device 이동\n",
    "fmnist_model = FashionMNISTModel().to(device)\n",
    "\n",
    "# loss -> 다중분류: CrossEntropyLoss()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(fmnist_model.parameters(), lr = 0.001)\n",
    "\n",
    "# 결과저장할 리스트\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "######################################################\n",
    "# 학습할 때 이전 epoch보다 성능이 좋을 때 저장함. -> 성능이 개선될 때마다 저장\n",
    "# 필요한 변수들 정의\n",
    "######################################################\n",
    "best_score = torch.inf  # 초기값을 무한으로 지정  ==> 학습중 가장 좋은 평가지표(val_loss)를 저장\n",
    "save_model_path = 'models/fashion_mnist_best_model.pt'\n",
    "\n",
    "######################################################\n",
    "# 조기종료(Eearly Stopping) - 특정 epoch동안 성능 개선이 없으면 학습을 중단.\n",
    "######################################################\n",
    "patience = 10  # 성능 개선 여부를 몇 epoch동안 확인할 것인지.\n",
    "trigger_cnt = 0  # 몇 epoch째 성능 개선을 기다리는지를 저장할 변수\n",
    "\n",
    "N_EPOCH = 1000\n",
    "s = time.time()\n",
    "for epoch in range(N_EPOCH):  # N_EPOCH만큼 학습\n",
    "    ######################\n",
    "    # 학습\n",
    "    ######################\n",
    "    fmnist_model.train()\n",
    "    train_loss = 0.0\n",
    "    for X, y in fmnist_train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        pred = fmnist_model(X)\n",
    "        loss = loss_fn(pred, y)  # pred: Softmax(), y: OneHotEncoding 처리\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(fmnist_train_loader)  # 평균 loss 계산\n",
    "    # 여기까지가 1 epoch 학습 종료\n",
    "\n",
    "    ######################\n",
    "    # 검증\n",
    "    ######################\n",
    "    fmnist_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in fmnist_test_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            \n",
    "            pred_val = fmnist_model(X_val)  # softmax 적용 전 --> loss는 이 값으로 계산\n",
    "            pred_label = pred_val.argmax(dim = -1)  # ==> accuracy 계산하기 위함\n",
    "            \n",
    "            # val - loss 계산\n",
    "            loss_val = loss_fn(pred_val, y_val)\n",
    "            val_loss += loss_val.item()\n",
    "            \n",
    "            # val - accuracy 계산\n",
    "            val_acc += torch.sum(pred_label == y_val).item()  # 현 배치에서 맞은 것의 개수\n",
    "            \n",
    "        # val_loss, val_acc의 평균\n",
    "        val_loss /= len(fmnist_test_loader)  # step 수로 나눔\n",
    "        val_acc /= len(fmnist_test_loader.dataset)  # 총 데이터 개수로 나눔\n",
    "        \n",
    "    # 현재 epoch에 대한 학습, 검증 종료 - log 남기기\n",
    "    print(f'[{epoch + 1} / {N_EPOCH}] train loss: {train_loss:.5f} val loss: {val_loss:.5f} val_accuracy: {val_acc:.5f}')\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "    \n",
    "    #####################################\n",
    "    # 조기종료, 모델저장\n",
    "    # 현 epoch의 val_loss가 best_score보다 개선된 경우(작은 경우)\n",
    "    #####################################\n",
    "    if val_loss < best_score:  # 성능 개선되었을 때\n",
    "        # 저장/조기종료\n",
    "        print(f'저장: {epoch + 1} epoch - 이전 best_score: {best_score}, 현재 score: {val_loss}')\n",
    "        best_score = val_loss\n",
    "        torch.save(fmnist_model, save_model_path)\n",
    "        trigger_cnt = 0\n",
    "        \n",
    "    else:\n",
    "        # 저장안하기 / trigger_cnt 증가 => 조기종료\n",
    "        trigger_cnt += 1\n",
    "        if patience == trigger_cnt:\n",
    "            print(f'조기종료: epoch - {epoch + 1}, {best_score:.5f}에서 개선이 안됨.')\n",
    "            break\n",
    "    \n",
    "e = time.time()\n",
    "print(f'학습에 걸린 시간: {e - s}초')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 결과 시각화\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss_list, label = 'train loss')\n",
    "plt.plot(val_loss_list, label = 'validation loss')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_acc_list)\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_load_model = torch.load(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = 0., 0.\n",
    "\n",
    "with torch.no_grad():\n",
    "        for X_val, y_val in fmnist_test_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            \n",
    "            pred_val = fmnist_load_model(X_val)  # softmax 적용 전 --> loss는 이 값으로 계산\n",
    "            pred_label = pred_val.argmax(dim = -1)  # ==> accuracy 계산하기 위함\n",
    "            \n",
    "            # val -> loss 계산\n",
    "            loss_val = loss_fn(pred_val, y_val)\n",
    "            val_loss += loss_val.item()\n",
    "            \n",
    "            # val -> accuracy 계산\n",
    "            val_acc += torch.sum(pred_label == y_val).item()  # 현 배치에서 맞은 것의 개수\n",
    "            \n",
    "        # val_loss, val_acc의 평균\n",
    "        val_loss /= len(fmnist_test_loader)  # step 수로 나눔\n",
    "        val_acc /= len(fmnist_test_loader.dataset)  # 총 데이터 개수로 나눔   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3249688531401791, 0.8855)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 위스콘신 유방암 데이터셋 - 이진분류(Binary Classification) 문제\n",
    "\n",
    "- **이진 분류 문제 처리 모델의 두가지 방법**\n",
    "    1. positive(1)일 확률을 출력하도록 구현\n",
    "        - output layer: units=1, activation='sigmoid'\n",
    "        - loss: binary_crossentropy\n",
    "    2. negative(0)일 확률과 positive(1)일 확률을 출력하도록 구현 => 다중분류 처리 방식으로 해결\n",
    "        - output layer: units=2, activation='softmax', y(정답)은 one hot encoding 처리\n",
    "        - loss: categorical_crossentropy\n",
    "        \n",
    "- 위스콘신 대학교에서 제공한 종양의 악성/양성여부 분류를 위한 데이터셋\n",
    "- Feature\n",
    "    - 종양에 대한 다양한 측정값들\n",
    "- Target의 class\n",
    "    - 0 - malignant(악성종양)\n",
    "    - 1 - benign(양성종양)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y = True)\n",
    "y = y.reshape(-1, 1)  # 2차원으로 변경 ==> 모델 출력 shape과 맞춰준다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 0)\n",
    "\n",
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_tensor = torch.tensor(scaler.fit_transform(X_train), dtype = torch.float32)\n",
    "X_test_tensor = torch.tensor(scaler.transform(X_test), dtype = torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype = torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset - 데이터셋이 Tensor 객체로 메모리에 loading된 경우\n",
    "wb_train_set = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "wb_test_set = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# DataLoader\n",
    "wb_train_loader = DataLoader(wb_train_set, batch_size = len(wb_train_set), shuffle = True, drop_last = True)\n",
    "wb_test_loader = DataLoader(wb_test_set, batch_size = len(wb_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([426, 30]), torch.Size([426, 1]))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(wb_train_loader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-2.2387e-09,  7.8354e-09, -4.4773e-09,  2.2387e-09,  1.1193e-09,\n",
       "         -2.2387e-09,  6.7160e-09, -6.1563e-09, -8.9547e-09,  1.6790e-09,\n",
       "          6.7160e-09, -7.2757e-09, -6.7160e-09,  0.0000e+00,  6.7160e-09,\n",
       "         -1.1193e-08,  2.2387e-09,  6.7160e-09, -5.5967e-09,  0.0000e+00,\n",
       "         -4.4773e-09,  5.5967e-09,  7.2757e-09,  5.5967e-09,  0.0000e+00,\n",
       "          8.9547e-09,  6.1563e-09, -2.2387e-09, -7.8354e-09, -1.1193e-09]),\n",
       " tensor([1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012,\n",
       "         1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012,\n",
       "         1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012,\n",
       "         1.0012, 1.0012, 1.0012]))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(dim = 0), x.std(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 악성 / 양성 분류\n",
    "index_to_class = ['악성종양', '양성종양']\n",
    "class_to_index = {'악성종양': 0, '양성종양': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class BreastCancerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(30, 32)\n",
    "        self.lr2 = nn.Linear(32, 16)\n",
    "        self.output = nn.Linear(16, 1)  # 출력 - 값 1개(positive의 확률을 출력)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = nn.ReLU()(self.lr1(X))\n",
    "        out = nn.ReLU()(self.lr2(out))\n",
    "        out = self.output(out)\n",
    "        # 2진 분류 -> positive 확률을 출력 => output의 출력결과를 logistic 함수에 입력\n",
    "        out = nn.Sigmoid()(out)\n",
    "    \n",
    "        return out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5079],\n",
       "        [0.5079],\n",
       "        [0.5079],\n",
       "        [0.5079],\n",
       "        [0.5079]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_model_tmp = BreastCancerModel()\n",
    "\n",
    "# 모델 test\n",
    "tmp_x = torch.ones(5, 30)\n",
    "y_tmp = bc_model_tmp(tmp_x)\n",
    "y_tmp  # sigmoid함수를 통해야 아래와 같이 확률로 나옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/500] train loss: 0.7176641821861267, val loss: 0.7198870778083801, val accuracy: 0.3706293706293706\n",
      "1 epoch에서 저장. 이전 score: inf, 현재 score: 0.7198870778083801\n",
      "[2/500] train loss: 0.7125005125999451, val loss: 0.7150562405586243, val accuracy: 0.3706293706293706\n",
      "2 epoch에서 저장. 이전 score: 0.7198870778083801, 현재 score: 0.7150562405586243\n",
      "[3/500] train loss: 0.7074509263038635, val loss: 0.7103273272514343, val accuracy: 0.3706293706293706\n",
      "3 epoch에서 저장. 이전 score: 0.7150562405586243, 현재 score: 0.7103273272514343\n",
      "[4/500] train loss: 0.7024427056312561, val loss: 0.7056093215942383, val accuracy: 0.3706293706293706\n",
      "4 epoch에서 저장. 이전 score: 0.7103273272514343, 현재 score: 0.7056093215942383\n",
      "[5/500] train loss: 0.6974994540214539, val loss: 0.7009541392326355, val accuracy: 0.3706293706293706\n",
      "5 epoch에서 저장. 이전 score: 0.7056093215942383, 현재 score: 0.7009541392326355\n",
      "[6/500] train loss: 0.6925881505012512, val loss: 0.6963651180267334, val accuracy: 0.3706293706293706\n",
      "6 epoch에서 저장. 이전 score: 0.7009541392326355, 현재 score: 0.6963651180267334\n",
      "[7/500] train loss: 0.6876908540725708, val loss: 0.691855251789093, val accuracy: 0.3706293706293706\n",
      "7 epoch에서 저장. 이전 score: 0.6963651180267334, 현재 score: 0.691855251789093\n",
      "[8/500] train loss: 0.6828292608261108, val loss: 0.687395453453064, val accuracy: 0.3706293706293706\n",
      "8 epoch에서 저장. 이전 score: 0.691855251789093, 현재 score: 0.687395453453064\n",
      "[9/500] train loss: 0.6779525876045227, val loss: 0.6829385161399841, val accuracy: 0.3706293706293706\n",
      "9 epoch에서 저장. 이전 score: 0.687395453453064, 현재 score: 0.6829385161399841\n",
      "[10/500] train loss: 0.6730634570121765, val loss: 0.6784464120864868, val accuracy: 0.3916083916083916\n",
      "10 epoch에서 저장. 이전 score: 0.6829385161399841, 현재 score: 0.6784464120864868\n",
      "[11/500] train loss: 0.6681243181228638, val loss: 0.673903226852417, val accuracy: 0.4195804195804196\n",
      "11 epoch에서 저장. 이전 score: 0.6784464120864868, 현재 score: 0.673903226852417\n",
      "[12/500] train loss: 0.6631405353546143, val loss: 0.669297993183136, val accuracy: 0.4405594405594406\n",
      "12 epoch에서 저장. 이전 score: 0.673903226852417, 현재 score: 0.669297993183136\n",
      "[13/500] train loss: 0.6581180691719055, val loss: 0.6646658182144165, val accuracy: 0.44755244755244755\n",
      "13 epoch에서 저장. 이전 score: 0.669297993183136, 현재 score: 0.6646658182144165\n",
      "[14/500] train loss: 0.6530746817588806, val loss: 0.6600024700164795, val accuracy: 0.4825174825174825\n",
      "14 epoch에서 저장. 이전 score: 0.6646658182144165, 현재 score: 0.6600024700164795\n",
      "[15/500] train loss: 0.6479784250259399, val loss: 0.6553129553794861, val accuracy: 0.5664335664335665\n",
      "15 epoch에서 저장. 이전 score: 0.6600024700164795, 현재 score: 0.6553129553794861\n",
      "[16/500] train loss: 0.642819881439209, val loss: 0.6505834460258484, val accuracy: 0.5944055944055944\n",
      "16 epoch에서 저장. 이전 score: 0.6553129553794861, 현재 score: 0.6505834460258484\n",
      "[17/500] train loss: 0.6375858783721924, val loss: 0.6458101868629456, val accuracy: 0.6293706293706294\n",
      "17 epoch에서 저장. 이전 score: 0.6505834460258484, 현재 score: 0.6458101868629456\n",
      "[18/500] train loss: 0.6322705149650574, val loss: 0.64093416929245, val accuracy: 0.6503496503496503\n",
      "18 epoch에서 저장. 이전 score: 0.6458101868629456, 현재 score: 0.64093416929245\n",
      "[19/500] train loss: 0.6268733739852905, val loss: 0.6359559297561646, val accuracy: 0.6643356643356644\n",
      "19 epoch에서 저장. 이전 score: 0.64093416929245, 현재 score: 0.6359559297561646\n",
      "[20/500] train loss: 0.6213681101799011, val loss: 0.6308912038803101, val accuracy: 0.6923076923076923\n",
      "20 epoch에서 저장. 이전 score: 0.6359559297561646, 현재 score: 0.6308912038803101\n",
      "[21/500] train loss: 0.6157535314559937, val loss: 0.6256856322288513, val accuracy: 0.7062937062937062\n",
      "21 epoch에서 저장. 이전 score: 0.6308912038803101, 현재 score: 0.6256856322288513\n",
      "[22/500] train loss: 0.6100295782089233, val loss: 0.6203641295433044, val accuracy: 0.7062937062937062\n",
      "22 epoch에서 저장. 이전 score: 0.6256856322288513, 현재 score: 0.6203641295433044\n",
      "[23/500] train loss: 0.6042014956474304, val loss: 0.6149366497993469, val accuracy: 0.7412587412587412\n",
      "23 epoch에서 저장. 이전 score: 0.6203641295433044, 현재 score: 0.6149366497993469\n",
      "[24/500] train loss: 0.5982680916786194, val loss: 0.6094009876251221, val accuracy: 0.7552447552447552\n",
      "24 epoch에서 저장. 이전 score: 0.6149366497993469, 현재 score: 0.6094009876251221\n",
      "[25/500] train loss: 0.5922184586524963, val loss: 0.6037517189979553, val accuracy: 0.7622377622377622\n",
      "25 epoch에서 저장. 이전 score: 0.6094009876251221, 현재 score: 0.6037517189979553\n",
      "[26/500] train loss: 0.5860512256622314, val loss: 0.5980026721954346, val accuracy: 0.7692307692307693\n",
      "26 epoch에서 저장. 이전 score: 0.6037517189979553, 현재 score: 0.5980026721954346\n",
      "[27/500] train loss: 0.5797728896141052, val loss: 0.5921476483345032, val accuracy: 0.7692307692307693\n",
      "27 epoch에서 저장. 이전 score: 0.5980026721954346, 현재 score: 0.5921476483345032\n",
      "[28/500] train loss: 0.5733941197395325, val loss: 0.5861895084381104, val accuracy: 0.7972027972027972\n",
      "28 epoch에서 저장. 이전 score: 0.5921476483345032, 현재 score: 0.5861895084381104\n",
      "[29/500] train loss: 0.5669021010398865, val loss: 0.5801400542259216, val accuracy: 0.8041958041958042\n",
      "29 epoch에서 저장. 이전 score: 0.5861895084381104, 현재 score: 0.5801400542259216\n",
      "[30/500] train loss: 0.5603087544441223, val loss: 0.5740011930465698, val accuracy: 0.8181818181818182\n",
      "30 epoch에서 저장. 이전 score: 0.5801400542259216, 현재 score: 0.5740011930465698\n",
      "[31/500] train loss: 0.5536088347434998, val loss: 0.5677353739738464, val accuracy: 0.8181818181818182\n",
      "31 epoch에서 저장. 이전 score: 0.5740011930465698, 현재 score: 0.5677353739738464\n",
      "[32/500] train loss: 0.5468289852142334, val loss: 0.5613924860954285, val accuracy: 0.8111888111888111\n",
      "32 epoch에서 저장. 이전 score: 0.5677353739738464, 현재 score: 0.5613924860954285\n",
      "[33/500] train loss: 0.5399748086929321, val loss: 0.5549915432929993, val accuracy: 0.8111888111888111\n",
      "33 epoch에서 저장. 이전 score: 0.5613924860954285, 현재 score: 0.5549915432929993\n",
      "[34/500] train loss: 0.5330489277839661, val loss: 0.548538088798523, val accuracy: 0.8111888111888111\n",
      "34 epoch에서 저장. 이전 score: 0.5549915432929993, 현재 score: 0.548538088798523\n",
      "[35/500] train loss: 0.5260584950447083, val loss: 0.5420202612876892, val accuracy: 0.8111888111888111\n",
      "35 epoch에서 저장. 이전 score: 0.548538088798523, 현재 score: 0.5420202612876892\n",
      "[36/500] train loss: 0.5190039277076721, val loss: 0.5354504585266113, val accuracy: 0.8251748251748252\n",
      "36 epoch에서 저장. 이전 score: 0.5420202612876892, 현재 score: 0.5354504585266113\n",
      "[37/500] train loss: 0.511895477771759, val loss: 0.5288125872612, val accuracy: 0.8461538461538461\n",
      "37 epoch에서 저장. 이전 score: 0.5354504585266113, 현재 score: 0.5288125872612\n",
      "[38/500] train loss: 0.5047268867492676, val loss: 0.5221306085586548, val accuracy: 0.8461538461538461\n",
      "38 epoch에서 저장. 이전 score: 0.5288125872612, 현재 score: 0.5221306085586548\n",
      "[39/500] train loss: 0.4975031018257141, val loss: 0.5154039859771729, val accuracy: 0.8531468531468531\n",
      "39 epoch에서 저장. 이전 score: 0.5221306085586548, 현재 score: 0.5154039859771729\n",
      "[40/500] train loss: 0.4902369976043701, val loss: 0.508635938167572, val accuracy: 0.8531468531468531\n",
      "40 epoch에서 저장. 이전 score: 0.5154039859771729, 현재 score: 0.508635938167572\n",
      "[41/500] train loss: 0.482918381690979, val loss: 0.5018228888511658, val accuracy: 0.8531468531468531\n",
      "41 epoch에서 저장. 이전 score: 0.508635938167572, 현재 score: 0.5018228888511658\n",
      "[42/500] train loss: 0.4755549430847168, val loss: 0.49496906995773315, val accuracy: 0.8601398601398601\n",
      "42 epoch에서 저장. 이전 score: 0.5018228888511658, 현재 score: 0.49496906995773315\n",
      "[43/500] train loss: 0.4681532680988312, val loss: 0.4880683720111847, val accuracy: 0.8601398601398601\n",
      "43 epoch에서 저장. 이전 score: 0.49496906995773315, 현재 score: 0.4880683720111847\n",
      "[44/500] train loss: 0.46072351932525635, val loss: 0.4811272919178009, val accuracy: 0.8601398601398601\n",
      "44 epoch에서 저장. 이전 score: 0.4880683720111847, 현재 score: 0.4811272919178009\n",
      "[45/500] train loss: 0.45325061678886414, val loss: 0.47418880462646484, val accuracy: 0.8671328671328671\n",
      "45 epoch에서 저장. 이전 score: 0.4811272919178009, 현재 score: 0.47418880462646484\n",
      "[46/500] train loss: 0.44574955105781555, val loss: 0.46721965074539185, val accuracy: 0.8671328671328671\n",
      "46 epoch에서 저장. 이전 score: 0.47418880462646484, 현재 score: 0.46721965074539185\n",
      "[47/500] train loss: 0.4382110834121704, val loss: 0.4601920545101166, val accuracy: 0.8811188811188811\n",
      "47 epoch에서 저장. 이전 score: 0.46721965074539185, 현재 score: 0.4601920545101166\n",
      "[48/500] train loss: 0.43064141273498535, val loss: 0.45311063528060913, val accuracy: 0.8881118881118881\n",
      "48 epoch에서 저장. 이전 score: 0.4601920545101166, 현재 score: 0.45311063528060913\n",
      "[49/500] train loss: 0.42305365204811096, val loss: 0.44599613547325134, val accuracy: 0.8951048951048951\n",
      "49 epoch에서 저장. 이전 score: 0.45311063528060913, 현재 score: 0.44599613547325134\n",
      "[50/500] train loss: 0.41542893648147583, val loss: 0.43884366750717163, val accuracy: 0.8951048951048951\n",
      "50 epoch에서 저장. 이전 score: 0.44599613547325134, 현재 score: 0.43884366750717163\n",
      "[51/500] train loss: 0.40775826573371887, val loss: 0.43165817856788635, val accuracy: 0.8951048951048951\n",
      "51 epoch에서 저장. 이전 score: 0.43884366750717163, 현재 score: 0.43165817856788635\n",
      "[52/500] train loss: 0.40005069971084595, val loss: 0.4244239032268524, val accuracy: 0.8951048951048951\n",
      "52 epoch에서 저장. 이전 score: 0.43165817856788635, 현재 score: 0.4244239032268524\n",
      "[53/500] train loss: 0.39229002594947815, val loss: 0.41712257266044617, val accuracy: 0.8951048951048951\n",
      "53 epoch에서 저장. 이전 score: 0.4244239032268524, 현재 score: 0.41712257266044617\n",
      "[54/500] train loss: 0.3844861686229706, val loss: 0.40981194376945496, val accuracy: 0.8951048951048951\n",
      "54 epoch에서 저장. 이전 score: 0.41712257266044617, 현재 score: 0.40981194376945496\n",
      "[55/500] train loss: 0.37665340304374695, val loss: 0.40249890089035034, val accuracy: 0.8951048951048951\n",
      "55 epoch에서 저장. 이전 score: 0.40981194376945496, 현재 score: 0.40249890089035034\n",
      "[56/500] train loss: 0.3687949776649475, val loss: 0.39515772461891174, val accuracy: 0.8951048951048951\n",
      "56 epoch에서 저장. 이전 score: 0.40249890089035034, 현재 score: 0.39515772461891174\n",
      "[57/500] train loss: 0.36092880368232727, val loss: 0.38776707649230957, val accuracy: 0.8951048951048951\n",
      "57 epoch에서 저장. 이전 score: 0.39515772461891174, 현재 score: 0.38776707649230957\n",
      "[58/500] train loss: 0.35306796431541443, val loss: 0.38035979866981506, val accuracy: 0.8951048951048951\n",
      "58 epoch에서 저장. 이전 score: 0.38776707649230957, 현재 score: 0.38035979866981506\n",
      "[59/500] train loss: 0.3452114760875702, val loss: 0.37293165922164917, val accuracy: 0.8951048951048951\n",
      "59 epoch에서 저장. 이전 score: 0.38035979866981506, 현재 score: 0.37293165922164917\n",
      "[60/500] train loss: 0.3373700678348541, val loss: 0.3655029535293579, val accuracy: 0.8951048951048951\n",
      "60 epoch에서 저장. 이전 score: 0.37293165922164917, 현재 score: 0.3655029535293579\n",
      "[61/500] train loss: 0.3295644521713257, val loss: 0.3581516146659851, val accuracy: 0.8951048951048951\n",
      "61 epoch에서 저장. 이전 score: 0.3655029535293579, 현재 score: 0.3581516146659851\n",
      "[62/500] train loss: 0.32181066274642944, val loss: 0.3508865237236023, val accuracy: 0.8951048951048951\n",
      "62 epoch에서 저장. 이전 score: 0.3581516146659851, 현재 score: 0.3508865237236023\n",
      "[63/500] train loss: 0.31411832571029663, val loss: 0.3436867892742157, val accuracy: 0.8951048951048951\n",
      "63 epoch에서 저장. 이전 score: 0.3508865237236023, 현재 score: 0.3436867892742157\n",
      "[64/500] train loss: 0.3064853847026825, val loss: 0.3365439474582672, val accuracy: 0.8951048951048951\n",
      "64 epoch에서 저장. 이전 score: 0.3436867892742157, 현재 score: 0.3365439474582672\n",
      "[65/500] train loss: 0.2989312708377838, val loss: 0.3294376730918884, val accuracy: 0.9020979020979021\n",
      "65 epoch에서 저장. 이전 score: 0.3365439474582672, 현재 score: 0.3294376730918884\n",
      "[66/500] train loss: 0.29146796464920044, val loss: 0.32239586114883423, val accuracy: 0.9020979020979021\n",
      "66 epoch에서 저장. 이전 score: 0.3294376730918884, 현재 score: 0.32239586114883423\n",
      "[67/500] train loss: 0.28410646319389343, val loss: 0.3154675364494324, val accuracy: 0.9020979020979021\n",
      "67 epoch에서 저장. 이전 score: 0.32239586114883423, 현재 score: 0.3154675364494324\n",
      "[68/500] train loss: 0.27685895562171936, val loss: 0.30865082144737244, val accuracy: 0.9020979020979021\n",
      "68 epoch에서 저장. 이전 score: 0.3154675364494324, 현재 score: 0.30865082144737244\n",
      "[69/500] train loss: 0.2697359621524811, val loss: 0.30197128653526306, val accuracy: 0.9020979020979021\n",
      "69 epoch에서 저장. 이전 score: 0.30865082144737244, 현재 score: 0.30197128653526306\n",
      "[70/500] train loss: 0.2627412974834442, val loss: 0.29541000723838806, val accuracy: 0.9020979020979021\n",
      "70 epoch에서 저장. 이전 score: 0.30197128653526306, 현재 score: 0.29541000723838806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71/500] train loss: 0.2558855712413788, val loss: 0.28896892070770264, val accuracy: 0.9020979020979021\n",
      "71 epoch에서 저장. 이전 score: 0.29541000723838806, 현재 score: 0.28896892070770264\n",
      "[72/500] train loss: 0.24917122721672058, val loss: 0.28269121050834656, val accuracy: 0.9020979020979021\n",
      "72 epoch에서 저장. 이전 score: 0.28896892070770264, 현재 score: 0.28269121050834656\n",
      "[73/500] train loss: 0.24261526763439178, val loss: 0.2765768766403198, val accuracy: 0.9090909090909091\n",
      "73 epoch에서 저장. 이전 score: 0.28269121050834656, 현재 score: 0.2765768766403198\n",
      "[74/500] train loss: 0.23620431125164032, val loss: 0.2706432044506073, val accuracy: 0.9090909090909091\n",
      "74 epoch에서 저장. 이전 score: 0.2765768766403198, 현재 score: 0.2706432044506073\n",
      "[75/500] train loss: 0.22995752096176147, val loss: 0.2648622691631317, val accuracy: 0.9090909090909091\n",
      "75 epoch에서 저장. 이전 score: 0.2706432044506073, 현재 score: 0.2648622691631317\n",
      "[76/500] train loss: 0.22387245297431946, val loss: 0.25924041867256165, val accuracy: 0.916083916083916\n",
      "76 epoch에서 저장. 이전 score: 0.2648622691631317, 현재 score: 0.25924041867256165\n",
      "[77/500] train loss: 0.21794983744621277, val loss: 0.2537921965122223, val accuracy: 0.916083916083916\n",
      "77 epoch에서 저장. 이전 score: 0.25924041867256165, 현재 score: 0.2537921965122223\n",
      "[78/500] train loss: 0.21219001710414886, val loss: 0.2485082447528839, val accuracy: 0.9230769230769231\n",
      "78 epoch에서 저장. 이전 score: 0.2537921965122223, 현재 score: 0.2485082447528839\n",
      "[79/500] train loss: 0.20659139752388, val loss: 0.2433788925409317, val accuracy: 0.9300699300699301\n",
      "79 epoch에서 저장. 이전 score: 0.2485082447528839, 현재 score: 0.2433788925409317\n",
      "[80/500] train loss: 0.2011520266532898, val loss: 0.23841901123523712, val accuracy: 0.9300699300699301\n",
      "80 epoch에서 저장. 이전 score: 0.2433788925409317, 현재 score: 0.23841901123523712\n",
      "[81/500] train loss: 0.19588187336921692, val loss: 0.23361577093601227, val accuracy: 0.9300699300699301\n",
      "81 epoch에서 저장. 이전 score: 0.23841901123523712, 현재 score: 0.23361577093601227\n",
      "[82/500] train loss: 0.19078406691551208, val loss: 0.2289840579032898, val accuracy: 0.9300699300699301\n",
      "82 epoch에서 저장. 이전 score: 0.23361577093601227, 현재 score: 0.2289840579032898\n",
      "[83/500] train loss: 0.1858644187450409, val loss: 0.2245333194732666, val accuracy: 0.9300699300699301\n",
      "83 epoch에서 저장. 이전 score: 0.2289840579032898, 현재 score: 0.2245333194732666\n",
      "[84/500] train loss: 0.18110983073711395, val loss: 0.22022980451583862, val accuracy: 0.9300699300699301\n",
      "84 epoch에서 저장. 이전 score: 0.2245333194732666, 현재 score: 0.22022980451583862\n",
      "[85/500] train loss: 0.17651887238025665, val loss: 0.21610021591186523, val accuracy: 0.9300699300699301\n",
      "85 epoch에서 저장. 이전 score: 0.22022980451583862, 현재 score: 0.21610021591186523\n",
      "[86/500] train loss: 0.17208251357078552, val loss: 0.21212971210479736, val accuracy: 0.9300699300699301\n",
      "86 epoch에서 저장. 이전 score: 0.21610021591186523, 현재 score: 0.21212971210479736\n",
      "[87/500] train loss: 0.16780443489551544, val loss: 0.20832037925720215, val accuracy: 0.9300699300699301\n",
      "87 epoch에서 저장. 이전 score: 0.21212971210479736, 현재 score: 0.20832037925720215\n",
      "[88/500] train loss: 0.16367511451244354, val loss: 0.20465587079524994, val accuracy: 0.9300699300699301\n",
      "88 epoch에서 저장. 이전 score: 0.20832037925720215, 현재 score: 0.20465587079524994\n",
      "[89/500] train loss: 0.15968644618988037, val loss: 0.20112673938274384, val accuracy: 0.9300699300699301\n",
      "89 epoch에서 저장. 이전 score: 0.20465587079524994, 현재 score: 0.20112673938274384\n",
      "[90/500] train loss: 0.15584789216518402, val loss: 0.19774547219276428, val accuracy: 0.9300699300699301\n",
      "90 epoch에서 저장. 이전 score: 0.20112673938274384, 현재 score: 0.19774547219276428\n",
      "[91/500] train loss: 0.15215390920639038, val loss: 0.19449900090694427, val accuracy: 0.9370629370629371\n",
      "91 epoch에서 저장. 이전 score: 0.19774547219276428, 현재 score: 0.19449900090694427\n",
      "[92/500] train loss: 0.1485995054244995, val loss: 0.19137683510780334, val accuracy: 0.9370629370629371\n",
      "92 epoch에서 저장. 이전 score: 0.19449900090694427, 현재 score: 0.19137683510780334\n",
      "[93/500] train loss: 0.14517755806446075, val loss: 0.18839558959007263, val accuracy: 0.9370629370629371\n",
      "93 epoch에서 저장. 이전 score: 0.19137683510780334, 현재 score: 0.18839558959007263\n",
      "[94/500] train loss: 0.14187274873256683, val loss: 0.185538187623024, val accuracy: 0.9370629370629371\n",
      "94 epoch에서 저장. 이전 score: 0.18839558959007263, 현재 score: 0.185538187623024\n",
      "[95/500] train loss: 0.1386844515800476, val loss: 0.18279704451560974, val accuracy: 0.9370629370629371\n",
      "95 epoch에서 저장. 이전 score: 0.185538187623024, 현재 score: 0.18279704451560974\n",
      "[96/500] train loss: 0.13561509549617767, val loss: 0.1801752746105194, val accuracy: 0.9370629370629371\n",
      "96 epoch에서 저장. 이전 score: 0.18279704451560974, 현재 score: 0.1801752746105194\n",
      "[97/500] train loss: 0.13265451788902283, val loss: 0.17765209078788757, val accuracy: 0.9370629370629371\n",
      "97 epoch에서 저장. 이전 score: 0.1801752746105194, 현재 score: 0.17765209078788757\n",
      "[98/500] train loss: 0.1298011839389801, val loss: 0.17521297931671143, val accuracy: 0.9370629370629371\n",
      "98 epoch에서 저장. 이전 score: 0.17765209078788757, 현재 score: 0.17521297931671143\n",
      "[99/500] train loss: 0.12704947590827942, val loss: 0.17285899817943573, val accuracy: 0.9370629370629371\n",
      "99 epoch에서 저장. 이전 score: 0.17521297931671143, 현재 score: 0.17285899817943573\n",
      "[100/500] train loss: 0.12439735233783722, val loss: 0.17059823870658875, val accuracy: 0.9370629370629371\n",
      "100 epoch에서 저장. 이전 score: 0.17285899817943573, 현재 score: 0.17059823870658875\n",
      "[101/500] train loss: 0.12184412032365799, val loss: 0.16842620074748993, val accuracy: 0.9370629370629371\n",
      "101 epoch에서 저장. 이전 score: 0.17059823870658875, 현재 score: 0.16842620074748993\n",
      "[102/500] train loss: 0.11937224864959717, val loss: 0.16633565723896027, val accuracy: 0.9370629370629371\n",
      "102 epoch에서 저장. 이전 score: 0.16842620074748993, 현재 score: 0.16633565723896027\n",
      "[103/500] train loss: 0.11698577553033829, val loss: 0.16432547569274902, val accuracy: 0.9370629370629371\n",
      "103 epoch에서 저장. 이전 score: 0.16633565723896027, 현재 score: 0.16432547569274902\n",
      "[104/500] train loss: 0.11468487977981567, val loss: 0.16237157583236694, val accuracy: 0.9440559440559441\n",
      "104 epoch에서 저장. 이전 score: 0.16432547569274902, 현재 score: 0.16237157583236694\n",
      "[105/500] train loss: 0.11246658861637115, val loss: 0.16048461198806763, val accuracy: 0.9440559440559441\n",
      "105 epoch에서 저장. 이전 score: 0.16237157583236694, 현재 score: 0.16048461198806763\n",
      "[106/500] train loss: 0.11032716184854507, val loss: 0.1586611568927765, val accuracy: 0.9440559440559441\n",
      "106 epoch에서 저장. 이전 score: 0.16048461198806763, 현재 score: 0.1586611568927765\n",
      "[107/500] train loss: 0.10826533287763596, val loss: 0.15688981115818024, val accuracy: 0.9440559440559441\n",
      "107 epoch에서 저장. 이전 score: 0.1586611568927765, 현재 score: 0.15688981115818024\n",
      "[108/500] train loss: 0.10627048462629318, val loss: 0.15516042709350586, val accuracy: 0.951048951048951\n",
      "108 epoch에서 저장. 이전 score: 0.15688981115818024, 현재 score: 0.15516042709350586\n",
      "[109/500] train loss: 0.10433976352214813, val loss: 0.15347108244895935, val accuracy: 0.951048951048951\n",
      "109 epoch에서 저장. 이전 score: 0.15516042709350586, 현재 score: 0.15347108244895935\n",
      "[110/500] train loss: 0.10247331857681274, val loss: 0.1518383026123047, val accuracy: 0.951048951048951\n",
      "110 epoch에서 저장. 이전 score: 0.15347108244895935, 현재 score: 0.1518383026123047\n",
      "[111/500] train loss: 0.10066516697406769, val loss: 0.1502554714679718, val accuracy: 0.951048951048951\n",
      "111 epoch에서 저장. 이전 score: 0.1518383026123047, 현재 score: 0.1502554714679718\n",
      "[112/500] train loss: 0.09891953319311142, val loss: 0.14870157837867737, val accuracy: 0.951048951048951\n",
      "112 epoch에서 저장. 이전 score: 0.1502554714679718, 현재 score: 0.14870157837867737\n",
      "[113/500] train loss: 0.09723436832427979, val loss: 0.14720508456230164, val accuracy: 0.951048951048951\n",
      "113 epoch에서 저장. 이전 score: 0.14870157837867737, 현재 score: 0.14720508456230164\n",
      "[114/500] train loss: 0.09560752660036087, val loss: 0.14577041566371918, val accuracy: 0.951048951048951\n",
      "114 epoch에서 저장. 이전 score: 0.14720508456230164, 현재 score: 0.14577041566371918\n",
      "[115/500] train loss: 0.09403450787067413, val loss: 0.14438937604427338, val accuracy: 0.951048951048951\n",
      "115 epoch에서 저장. 이전 score: 0.14577041566371918, 현재 score: 0.14438937604427338\n",
      "[116/500] train loss: 0.09251432120800018, val loss: 0.14306211471557617, val accuracy: 0.951048951048951\n",
      "116 epoch에서 저장. 이전 score: 0.14438937604427338, 현재 score: 0.14306211471557617\n",
      "[117/500] train loss: 0.09104076027870178, val loss: 0.14179053902626038, val accuracy: 0.951048951048951\n",
      "117 epoch에서 저장. 이전 score: 0.14306211471557617, 현재 score: 0.14179053902626038\n",
      "[118/500] train loss: 0.08961410075426102, val loss: 0.14057961106300354, val accuracy: 0.951048951048951\n",
      "118 epoch에서 저장. 이전 score: 0.14179053902626038, 현재 score: 0.14057961106300354\n",
      "[119/500] train loss: 0.08823497593402863, val loss: 0.13939835131168365, val accuracy: 0.9440559440559441\n",
      "119 epoch에서 저장. 이전 score: 0.14057961106300354, 현재 score: 0.13939835131168365\n",
      "[120/500] train loss: 0.08690138906240463, val loss: 0.13826033473014832, val accuracy: 0.9440559440559441\n",
      "120 epoch에서 저장. 이전 score: 0.13939835131168365, 현재 score: 0.13826033473014832\n",
      "[121/500] train loss: 0.08561086654663086, val loss: 0.13716761767864227, val accuracy: 0.9440559440559441\n",
      "121 epoch에서 저장. 이전 score: 0.13826033473014832, 현재 score: 0.13716761767864227\n",
      "[122/500] train loss: 0.08436259627342224, val loss: 0.1361096054315567, val accuracy: 0.9440559440559441\n",
      "122 epoch에서 저장. 이전 score: 0.13716761767864227, 현재 score: 0.1361096054315567\n",
      "[123/500] train loss: 0.08315667510032654, val loss: 0.13509288430213928, val accuracy: 0.9440559440559441\n",
      "123 epoch에서 저장. 이전 score: 0.1361096054315567, 현재 score: 0.13509288430213928\n",
      "[124/500] train loss: 0.08198928087949753, val loss: 0.1341206431388855, val accuracy: 0.9440559440559441\n",
      "124 epoch에서 저장. 이전 score: 0.13509288430213928, 현재 score: 0.1341206431388855\n",
      "[125/500] train loss: 0.08085890859365463, val loss: 0.13318729400634766, val accuracy: 0.9440559440559441\n",
      "125 epoch에서 저장. 이전 score: 0.1341206431388855, 현재 score: 0.13318729400634766\n",
      "[126/500] train loss: 0.07976070791482925, val loss: 0.13228817284107208, val accuracy: 0.9440559440559441\n",
      "126 epoch에서 저장. 이전 score: 0.13318729400634766, 현재 score: 0.13228817284107208\n",
      "[127/500] train loss: 0.07869475334882736, val loss: 0.13142529129981995, val accuracy: 0.9440559440559441\n",
      "127 epoch에서 저장. 이전 score: 0.13228817284107208, 현재 score: 0.13142529129981995\n",
      "[128/500] train loss: 0.07766048610210419, val loss: 0.1306011974811554, val accuracy: 0.951048951048951\n",
      "128 epoch에서 저장. 이전 score: 0.13142529129981995, 현재 score: 0.1306011974811554\n",
      "[129/500] train loss: 0.07665571570396423, val loss: 0.12979762256145477, val accuracy: 0.951048951048951\n",
      "129 epoch에서 저장. 이전 score: 0.1306011974811554, 현재 score: 0.12979762256145477\n",
      "[130/500] train loss: 0.07568492740392685, val loss: 0.12902890145778656, val accuracy: 0.958041958041958\n",
      "130 epoch에서 저장. 이전 score: 0.12979762256145477, 현재 score: 0.12902890145778656\n",
      "[131/500] train loss: 0.0747460126876831, val loss: 0.128291517496109, val accuracy: 0.958041958041958\n",
      "131 epoch에서 저장. 이전 score: 0.12902890145778656, 현재 score: 0.128291517496109\n",
      "[132/500] train loss: 0.0738350972533226, val loss: 0.12758155167102814, val accuracy: 0.958041958041958\n",
      "132 epoch에서 저장. 이전 score: 0.128291517496109, 현재 score: 0.12758155167102814\n",
      "[133/500] train loss: 0.07295048981904984, val loss: 0.1268964409828186, val accuracy: 0.958041958041958\n",
      "133 epoch에서 저장. 이전 score: 0.12758155167102814, 현재 score: 0.1268964409828186\n",
      "[134/500] train loss: 0.07209087908267975, val loss: 0.12623527646064758, val accuracy: 0.958041958041958\n",
      "134 epoch에서 저장. 이전 score: 0.1268964409828186, 현재 score: 0.12623527646064758\n",
      "[135/500] train loss: 0.07125718891620636, val loss: 0.12559981644153595, val accuracy: 0.958041958041958\n",
      "135 epoch에서 저장. 이전 score: 0.12623527646064758, 현재 score: 0.12559981644153595\n",
      "[136/500] train loss: 0.07044602185487747, val loss: 0.12498452514410019, val accuracy: 0.958041958041958\n",
      "136 epoch에서 저장. 이전 score: 0.12559981644153595, 현재 score: 0.12498452514410019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[137/500] train loss: 0.06965713202953339, val loss: 0.1243937686085701, val accuracy: 0.958041958041958\n",
      "137 epoch에서 저장. 이전 score: 0.12498452514410019, 현재 score: 0.1243937686085701\n",
      "[138/500] train loss: 0.06888829171657562, val loss: 0.12383238226175308, val accuracy: 0.958041958041958\n",
      "138 epoch에서 저장. 이전 score: 0.1243937686085701, 현재 score: 0.12383238226175308\n",
      "[139/500] train loss: 0.06813915073871613, val loss: 0.12328445166349411, val accuracy: 0.958041958041958\n",
      "139 epoch에서 저장. 이전 score: 0.12383238226175308, 현재 score: 0.12328445166349411\n",
      "[140/500] train loss: 0.06740957498550415, val loss: 0.12275967746973038, val accuracy: 0.958041958041958\n",
      "140 epoch에서 저장. 이전 score: 0.12328445166349411, 현재 score: 0.12275967746973038\n",
      "[141/500] train loss: 0.06669912487268448, val loss: 0.12224334478378296, val accuracy: 0.958041958041958\n",
      "141 epoch에서 저장. 이전 score: 0.12275967746973038, 현재 score: 0.12224334478378296\n",
      "[142/500] train loss: 0.06600907444953918, val loss: 0.12174209207296371, val accuracy: 0.958041958041958\n",
      "142 epoch에서 저장. 이전 score: 0.12224334478378296, 현재 score: 0.12174209207296371\n",
      "[143/500] train loss: 0.0653396025300026, val loss: 0.12125620990991592, val accuracy: 0.958041958041958\n",
      "143 epoch에서 저장. 이전 score: 0.12174209207296371, 현재 score: 0.12125620990991592\n",
      "[144/500] train loss: 0.06468764692544937, val loss: 0.12078386545181274, val accuracy: 0.958041958041958\n",
      "144 epoch에서 저장. 이전 score: 0.12125620990991592, 현재 score: 0.12078386545181274\n",
      "[145/500] train loss: 0.06405236572027206, val loss: 0.12032432854175568, val accuracy: 0.958041958041958\n",
      "145 epoch에서 저장. 이전 score: 0.12078386545181274, 현재 score: 0.12032432854175568\n",
      "[146/500] train loss: 0.06343310326337814, val loss: 0.11987930536270142, val accuracy: 0.958041958041958\n",
      "146 epoch에서 저장. 이전 score: 0.12032432854175568, 현재 score: 0.11987930536270142\n",
      "[147/500] train loss: 0.06282965838909149, val loss: 0.11944767087697983, val accuracy: 0.958041958041958\n",
      "147 epoch에서 저장. 이전 score: 0.11987930536270142, 현재 score: 0.11944767087697983\n",
      "[148/500] train loss: 0.062241703271865845, val loss: 0.1190287321805954, val accuracy: 0.958041958041958\n",
      "148 epoch에서 저장. 이전 score: 0.11944767087697983, 현재 score: 0.1190287321805954\n",
      "[149/500] train loss: 0.06166795641183853, val loss: 0.11862052977085114, val accuracy: 0.958041958041958\n",
      "149 epoch에서 저장. 이전 score: 0.1190287321805954, 현재 score: 0.11862052977085114\n",
      "[150/500] train loss: 0.06110823154449463, val loss: 0.11822926253080368, val accuracy: 0.958041958041958\n",
      "150 epoch에서 저장. 이전 score: 0.11862052977085114, 현재 score: 0.11822926253080368\n",
      "[151/500] train loss: 0.060563936829566956, val loss: 0.11785108596086502, val accuracy: 0.958041958041958\n",
      "151 epoch에서 저장. 이전 score: 0.11822926253080368, 현재 score: 0.11785108596086502\n",
      "[152/500] train loss: 0.06003345921635628, val loss: 0.11747541278600693, val accuracy: 0.958041958041958\n",
      "152 epoch에서 저장. 이전 score: 0.11785108596086502, 현재 score: 0.11747541278600693\n",
      "[153/500] train loss: 0.059518661350011826, val loss: 0.11710653454065323, val accuracy: 0.958041958041958\n",
      "153 epoch에서 저장. 이전 score: 0.11747541278600693, 현재 score: 0.11710653454065323\n",
      "[154/500] train loss: 0.05901390314102173, val loss: 0.11674381792545319, val accuracy: 0.958041958041958\n",
      "154 epoch에서 저장. 이전 score: 0.11710653454065323, 현재 score: 0.11674381792545319\n",
      "[155/500] train loss: 0.058519717305898666, val loss: 0.11638861149549484, val accuracy: 0.958041958041958\n",
      "155 epoch에서 저장. 이전 score: 0.11674381792545319, 현재 score: 0.11638861149549484\n",
      "[156/500] train loss: 0.0580366849899292, val loss: 0.11604082584381104, val accuracy: 0.958041958041958\n",
      "156 epoch에서 저장. 이전 score: 0.11638861149549484, 현재 score: 0.11604082584381104\n",
      "[157/500] train loss: 0.05756416171789169, val loss: 0.11570719629526138, val accuracy: 0.958041958041958\n",
      "157 epoch에서 저장. 이전 score: 0.11604082584381104, 현재 score: 0.11570719629526138\n",
      "[158/500] train loss: 0.057101499289274216, val loss: 0.1153842955827713, val accuracy: 0.958041958041958\n",
      "158 epoch에서 저장. 이전 score: 0.11570719629526138, 현재 score: 0.1153842955827713\n",
      "[159/500] train loss: 0.05664931610226631, val loss: 0.11506931483745575, val accuracy: 0.958041958041958\n",
      "159 epoch에서 저장. 이전 score: 0.1153842955827713, 현재 score: 0.11506931483745575\n",
      "[160/500] train loss: 0.05620625242590904, val loss: 0.11476115137338638, val accuracy: 0.958041958041958\n",
      "160 epoch에서 저장. 이전 score: 0.11506931483745575, 현재 score: 0.11476115137338638\n",
      "[161/500] train loss: 0.05577130243182182, val loss: 0.11446372419595718, val accuracy: 0.958041958041958\n",
      "161 epoch에서 저장. 이전 score: 0.11476115137338638, 현재 score: 0.11446372419595718\n",
      "[162/500] train loss: 0.05534650757908821, val loss: 0.11417502164840698, val accuracy: 0.958041958041958\n",
      "162 epoch에서 저장. 이전 score: 0.11446372419595718, 현재 score: 0.11417502164840698\n",
      "[163/500] train loss: 0.054930392652750015, val loss: 0.11389529705047607, val accuracy: 0.958041958041958\n",
      "163 epoch에서 저장. 이전 score: 0.11417502164840698, 현재 score: 0.11389529705047607\n",
      "[164/500] train loss: 0.054523106664419174, val loss: 0.11362475901842117, val accuracy: 0.958041958041958\n",
      "164 epoch에서 저장. 이전 score: 0.11389529705047607, 현재 score: 0.11362475901842117\n",
      "[165/500] train loss: 0.054124537855386734, val loss: 0.11336837708950043, val accuracy: 0.958041958041958\n",
      "165 epoch에서 저장. 이전 score: 0.11362475901842117, 현재 score: 0.11336837708950043\n",
      "[166/500] train loss: 0.05373326316475868, val loss: 0.11311979591846466, val accuracy: 0.958041958041958\n",
      "166 epoch에서 저장. 이전 score: 0.11336837708950043, 현재 score: 0.11311979591846466\n",
      "[167/500] train loss: 0.053351204842329025, val loss: 0.11288334429264069, val accuracy: 0.958041958041958\n",
      "167 epoch에서 저장. 이전 score: 0.11311979591846466, 현재 score: 0.11288334429264069\n",
      "[168/500] train loss: 0.05297711119055748, val loss: 0.11265522986650467, val accuracy: 0.958041958041958\n",
      "168 epoch에서 저장. 이전 score: 0.11288334429264069, 현재 score: 0.11265522986650467\n",
      "[169/500] train loss: 0.05260871350765228, val loss: 0.11243346333503723, val accuracy: 0.958041958041958\n",
      "169 epoch에서 저장. 이전 score: 0.11265522986650467, 현재 score: 0.11243346333503723\n",
      "[170/500] train loss: 0.05224670097231865, val loss: 0.11222384870052338, val accuracy: 0.958041958041958\n",
      "170 epoch에서 저장. 이전 score: 0.11243346333503723, 현재 score: 0.11222384870052338\n",
      "[171/500] train loss: 0.05189328268170357, val loss: 0.11203283071517944, val accuracy: 0.958041958041958\n",
      "171 epoch에서 저장. 이전 score: 0.11222384870052338, 현재 score: 0.11203283071517944\n",
      "[172/500] train loss: 0.051546741276979446, val loss: 0.11183635145425797, val accuracy: 0.958041958041958\n",
      "172 epoch에서 저장. 이전 score: 0.11203283071517944, 현재 score: 0.11183635145425797\n",
      "[173/500] train loss: 0.05120624974370003, val loss: 0.1116449162364006, val accuracy: 0.958041958041958\n",
      "173 epoch에서 저장. 이전 score: 0.11183635145425797, 현재 score: 0.1116449162364006\n",
      "[174/500] train loss: 0.05087193474173546, val loss: 0.11146222800016403, val accuracy: 0.958041958041958\n",
      "174 epoch에서 저장. 이전 score: 0.1116449162364006, 현재 score: 0.11146222800016403\n",
      "[175/500] train loss: 0.050543710589408875, val loss: 0.11129005998373032, val accuracy: 0.958041958041958\n",
      "175 epoch에서 저장. 이전 score: 0.11146222800016403, 현재 score: 0.11129005998373032\n",
      "[176/500] train loss: 0.05022186413407326, val loss: 0.11112593114376068, val accuracy: 0.958041958041958\n",
      "176 epoch에서 저장. 이전 score: 0.11129005998373032, 현재 score: 0.11112593114376068\n",
      "[177/500] train loss: 0.04990851879119873, val loss: 0.1109650507569313, val accuracy: 0.958041958041958\n",
      "177 epoch에서 저장. 이전 score: 0.11112593114376068, 현재 score: 0.1109650507569313\n",
      "[178/500] train loss: 0.04960143566131592, val loss: 0.1108146607875824, val accuracy: 0.958041958041958\n",
      "178 epoch에서 저장. 이전 score: 0.1109650507569313, 현재 score: 0.1108146607875824\n",
      "[179/500] train loss: 0.04930029809474945, val loss: 0.11067847907543182, val accuracy: 0.958041958041958\n",
      "179 epoch에서 저장. 이전 score: 0.1108146607875824, 현재 score: 0.11067847907543182\n",
      "[180/500] train loss: 0.04900471493601799, val loss: 0.11054724454879761, val accuracy: 0.958041958041958\n",
      "180 epoch에서 저장. 이전 score: 0.11067847907543182, 현재 score: 0.11054724454879761\n",
      "[181/500] train loss: 0.04871318116784096, val loss: 0.11041832715272903, val accuracy: 0.958041958041958\n",
      "181 epoch에서 저장. 이전 score: 0.11054724454879761, 현재 score: 0.11041832715272903\n",
      "[182/500] train loss: 0.04842635989189148, val loss: 0.11029408127069473, val accuracy: 0.958041958041958\n",
      "182 epoch에서 저장. 이전 score: 0.11041832715272903, 현재 score: 0.11029408127069473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[183/500] train loss: 0.04814298450946808, val loss: 0.11017416417598724, val accuracy: 0.958041958041958\n",
      "183 epoch에서 저장. 이전 score: 0.11029408127069473, 현재 score: 0.11017416417598724\n",
      "[184/500] train loss: 0.04786420986056328, val loss: 0.11006103456020355, val accuracy: 0.958041958041958\n",
      "184 epoch에서 저장. 이전 score: 0.11017416417598724, 현재 score: 0.11006103456020355\n",
      "[185/500] train loss: 0.04758943244814873, val loss: 0.1099485531449318, val accuracy: 0.958041958041958\n",
      "185 epoch에서 저장. 이전 score: 0.11006103456020355, 현재 score: 0.1099485531449318\n",
      "[186/500] train loss: 0.04731929302215576, val loss: 0.10983853787183762, val accuracy: 0.958041958041958\n",
      "186 epoch에서 저장. 이전 score: 0.1099485531449318, 현재 score: 0.10983853787183762\n",
      "[187/500] train loss: 0.04705462232232094, val loss: 0.1097356528043747, val accuracy: 0.958041958041958\n",
      "187 epoch에서 저장. 이전 score: 0.10983853787183762, 현재 score: 0.1097356528043747\n",
      "[188/500] train loss: 0.04679351672530174, val loss: 0.10963736474514008, val accuracy: 0.958041958041958\n",
      "188 epoch에서 저장. 이전 score: 0.1097356528043747, 현재 score: 0.10963736474514008\n",
      "[189/500] train loss: 0.04653458297252655, val loss: 0.10954306274652481, val accuracy: 0.958041958041958\n",
      "189 epoch에서 저장. 이전 score: 0.10963736474514008, 현재 score: 0.10954306274652481\n",
      "[190/500] train loss: 0.046278420835733414, val loss: 0.10945325344800949, val accuracy: 0.958041958041958\n",
      "190 epoch에서 저장. 이전 score: 0.10954306274652481, 현재 score: 0.10945325344800949\n",
      "[191/500] train loss: 0.04602525383234024, val loss: 0.10936982184648514, val accuracy: 0.958041958041958\n",
      "191 epoch에서 저장. 이전 score: 0.10945325344800949, 현재 score: 0.10936982184648514\n",
      "[192/500] train loss: 0.0457780659198761, val loss: 0.10929015278816223, val accuracy: 0.958041958041958\n",
      "192 epoch에서 저장. 이전 score: 0.10936982184648514, 현재 score: 0.10929015278816223\n",
      "[193/500] train loss: 0.0455351285636425, val loss: 0.10921187698841095, val accuracy: 0.958041958041958\n",
      "193 epoch에서 저장. 이전 score: 0.10929015278816223, 현재 score: 0.10921187698841095\n",
      "[194/500] train loss: 0.04529649019241333, val loss: 0.10913345962762833, val accuracy: 0.958041958041958\n",
      "194 epoch에서 저장. 이전 score: 0.10921187698841095, 현재 score: 0.10913345962762833\n",
      "[195/500] train loss: 0.04506036639213562, val loss: 0.10904881358146667, val accuracy: 0.958041958041958\n",
      "195 epoch에서 저장. 이전 score: 0.10913345962762833, 현재 score: 0.10904881358146667\n",
      "[196/500] train loss: 0.044827431440353394, val loss: 0.10896777361631393, val accuracy: 0.958041958041958\n",
      "196 epoch에서 저장. 이전 score: 0.10904881358146667, 현재 score: 0.10896777361631393\n",
      "[197/500] train loss: 0.0445970855653286, val loss: 0.10888992995023727, val accuracy: 0.958041958041958\n",
      "197 epoch에서 저장. 이전 score: 0.10896777361631393, 현재 score: 0.10888992995023727\n",
      "[198/500] train loss: 0.04436996206641197, val loss: 0.10881439596414566, val accuracy: 0.958041958041958\n",
      "198 epoch에서 저장. 이전 score: 0.10888992995023727, 현재 score: 0.10881439596414566\n",
      "[199/500] train loss: 0.04414620250463486, val loss: 0.10874360054731369, val accuracy: 0.958041958041958\n",
      "199 epoch에서 저장. 이전 score: 0.10881439596414566, 현재 score: 0.10874360054731369\n",
      "[200/500] train loss: 0.0439242348074913, val loss: 0.10867777466773987, val accuracy: 0.958041958041958\n",
      "200 epoch에서 저장. 이전 score: 0.10874360054731369, 현재 score: 0.10867777466773987\n",
      "[201/500] train loss: 0.043704163283109665, val loss: 0.10861042141914368, val accuracy: 0.958041958041958\n",
      "201 epoch에서 저장. 이전 score: 0.10867777466773987, 현재 score: 0.10861042141914368\n",
      "[202/500] train loss: 0.043486081063747406, val loss: 0.10854070633649826, val accuracy: 0.958041958041958\n",
      "202 epoch에서 저장. 이전 score: 0.10861042141914368, 현재 score: 0.10854070633649826\n",
      "[203/500] train loss: 0.04326822608709335, val loss: 0.10846248269081116, val accuracy: 0.958041958041958\n",
      "203 epoch에서 저장. 이전 score: 0.10854070633649826, 현재 score: 0.10846248269081116\n",
      "[204/500] train loss: 0.043053120374679565, val loss: 0.10837909579277039, val accuracy: 0.958041958041958\n",
      "204 epoch에서 저장. 이전 score: 0.10846248269081116, 현재 score: 0.10837909579277039\n",
      "[205/500] train loss: 0.04284088686108589, val loss: 0.10829650610685349, val accuracy: 0.958041958041958\n",
      "205 epoch에서 저장. 이전 score: 0.10837909579277039, 현재 score: 0.10829650610685349\n",
      "[206/500] train loss: 0.04263179376721382, val loss: 0.1082143560051918, val accuracy: 0.958041958041958\n",
      "206 epoch에서 저장. 이전 score: 0.10829650610685349, 현재 score: 0.1082143560051918\n",
      "[207/500] train loss: 0.04242433235049248, val loss: 0.10813342779874802, val accuracy: 0.958041958041958\n",
      "207 epoch에서 저장. 이전 score: 0.1082143560051918, 현재 score: 0.10813342779874802\n",
      "[208/500] train loss: 0.04221820831298828, val loss: 0.10805437713861465, val accuracy: 0.958041958041958\n",
      "208 epoch에서 저장. 이전 score: 0.10813342779874802, 현재 score: 0.10805437713861465\n",
      "[209/500] train loss: 0.04201304912567139, val loss: 0.10797762125730515, val accuracy: 0.958041958041958\n",
      "209 epoch에서 저장. 이전 score: 0.10805437713861465, 현재 score: 0.10797762125730515\n",
      "[210/500] train loss: 0.04180982708930969, val loss: 0.1079036295413971, val accuracy: 0.958041958041958\n",
      "210 epoch에서 저장. 이전 score: 0.10797762125730515, 현재 score: 0.1079036295413971\n",
      "[211/500] train loss: 0.04160800576210022, val loss: 0.10783170163631439, val accuracy: 0.958041958041958\n",
      "211 epoch에서 저장. 이전 score: 0.1079036295413971, 현재 score: 0.10783170163631439\n",
      "[212/500] train loss: 0.04140839725732803, val loss: 0.10775937139987946, val accuracy: 0.958041958041958\n",
      "212 epoch에서 저장. 이전 score: 0.10783170163631439, 현재 score: 0.10775937139987946\n",
      "[213/500] train loss: 0.04121088609099388, val loss: 0.10768169164657593, val accuracy: 0.958041958041958\n",
      "213 epoch에서 저장. 이전 score: 0.10775937139987946, 현재 score: 0.10768169164657593\n",
      "[214/500] train loss: 0.041015129536390305, val loss: 0.10760129988193512, val accuracy: 0.958041958041958\n",
      "214 epoch에서 저장. 이전 score: 0.10768169164657593, 현재 score: 0.10760129988193512\n",
      "[215/500] train loss: 0.040821321308612823, val loss: 0.10751732438802719, val accuracy: 0.958041958041958\n",
      "215 epoch에서 저장. 이전 score: 0.10760129988193512, 현재 score: 0.10751732438802719\n",
      "[216/500] train loss: 0.040629252791404724, val loss: 0.10742682218551636, val accuracy: 0.958041958041958\n",
      "216 epoch에서 저장. 이전 score: 0.10751732438802719, 현재 score: 0.10742682218551636\n",
      "[217/500] train loss: 0.04043840989470482, val loss: 0.10732173174619675, val accuracy: 0.958041958041958\n",
      "217 epoch에서 저장. 이전 score: 0.10742682218551636, 현재 score: 0.10732173174619675\n",
      "[218/500] train loss: 0.04024890810251236, val loss: 0.10721656680107117, val accuracy: 0.958041958041958\n",
      "218 epoch에서 저장. 이전 score: 0.10732173174619675, 현재 score: 0.10721656680107117\n",
      "[219/500] train loss: 0.04006073996424675, val loss: 0.10711116343736649, val accuracy: 0.958041958041958\n",
      "219 epoch에서 저장. 이전 score: 0.10721656680107117, 현재 score: 0.10711116343736649\n",
      "[220/500] train loss: 0.039875488728284836, val loss: 0.10700653493404388, val accuracy: 0.958041958041958\n",
      "220 epoch에서 저장. 이전 score: 0.10711116343736649, 현재 score: 0.10700653493404388\n",
      "[221/500] train loss: 0.03969116881489754, val loss: 0.10690049082040787, val accuracy: 0.958041958041958\n",
      "221 epoch에서 저장. 이전 score: 0.10700653493404388, 현재 score: 0.10690049082040787\n",
      "[222/500] train loss: 0.0395076610147953, val loss: 0.10679652541875839, val accuracy: 0.958041958041958\n",
      "222 epoch에서 저장. 이전 score: 0.10690049082040787, 현재 score: 0.10679652541875839\n",
      "[223/500] train loss: 0.03932543098926544, val loss: 0.10670368373394012, val accuracy: 0.958041958041958\n",
      "223 epoch에서 저장. 이전 score: 0.10679652541875839, 현재 score: 0.10670368373394012\n",
      "[224/500] train loss: 0.039144180715084076, val loss: 0.10661127418279648, val accuracy: 0.958041958041958\n",
      "224 epoch에서 저장. 이전 score: 0.10670368373394012, 현재 score: 0.10661127418279648\n",
      "[225/500] train loss: 0.038964029401540756, val loss: 0.10651393234729767, val accuracy: 0.958041958041958\n",
      "225 epoch에서 저장. 이전 score: 0.10661127418279648, 현재 score: 0.10651393234729767\n",
      "[226/500] train loss: 0.03878578171133995, val loss: 0.10641390085220337, val accuracy: 0.958041958041958\n",
      "226 epoch에서 저장. 이전 score: 0.10651393234729767, 현재 score: 0.10641390085220337\n",
      "[227/500] train loss: 0.03860827162861824, val loss: 0.10631264001131058, val accuracy: 0.958041958041958\n",
      "227 epoch에서 저장. 이전 score: 0.10641390085220337, 현재 score: 0.10631264001131058\n",
      "[228/500] train loss: 0.038431987166404724, val loss: 0.10621122270822525, val accuracy: 0.958041958041958\n",
      "228 epoch에서 저장. 이전 score: 0.10631264001131058, 현재 score: 0.10621122270822525\n",
      "[229/500] train loss: 0.038255829364061356, val loss: 0.10610996931791306, val accuracy: 0.958041958041958\n",
      "229 epoch에서 저장. 이전 score: 0.10621122270822525, 현재 score: 0.10610996931791306\n",
      "[230/500] train loss: 0.03808048367500305, val loss: 0.1060073971748352, val accuracy: 0.958041958041958\n",
      "230 epoch에서 저장. 이전 score: 0.10610996931791306, 현재 score: 0.1060073971748352\n",
      "[231/500] train loss: 0.0379064716398716, val loss: 0.1059047281742096, val accuracy: 0.958041958041958\n",
      "231 epoch에서 저장. 이전 score: 0.1060073971748352, 현재 score: 0.1059047281742096\n",
      "[232/500] train loss: 0.03773318603634834, val loss: 0.10579659044742584, val accuracy: 0.958041958041958\n",
      "232 epoch에서 저장. 이전 score: 0.1059047281742096, 현재 score: 0.10579659044742584\n",
      "[233/500] train loss: 0.03756242245435715, val loss: 0.10568578541278839, val accuracy: 0.965034965034965\n",
      "233 epoch에서 저장. 이전 score: 0.10579659044742584, 현재 score: 0.10568578541278839\n",
      "[234/500] train loss: 0.037393514066934586, val loss: 0.10557747632265091, val accuracy: 0.965034965034965\n",
      "234 epoch에서 저장. 이전 score: 0.10568578541278839, 현재 score: 0.10557747632265091\n",
      "[235/500] train loss: 0.03722526133060455, val loss: 0.10547037422657013, val accuracy: 0.965034965034965\n",
      "235 epoch에서 저장. 이전 score: 0.10557747632265091, 현재 score: 0.10547037422657013\n",
      "[236/500] train loss: 0.037057891488075256, val loss: 0.10535771399736404, val accuracy: 0.965034965034965\n",
      "236 epoch에서 저장. 이전 score: 0.10547037422657013, 현재 score: 0.10535771399736404\n",
      "[237/500] train loss: 0.036890774965286255, val loss: 0.10524642467498779, val accuracy: 0.965034965034965\n",
      "237 epoch에서 저장. 이전 score: 0.10535771399736404, 현재 score: 0.10524642467498779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[238/500] train loss: 0.03672461956739426, val loss: 0.10513491928577423, val accuracy: 0.965034965034965\n",
      "238 epoch에서 저장. 이전 score: 0.10524642467498779, 현재 score: 0.10513491928577423\n",
      "[239/500] train loss: 0.03655858710408211, val loss: 0.10502185672521591, val accuracy: 0.965034965034965\n",
      "239 epoch에서 저장. 이전 score: 0.10513491928577423, 현재 score: 0.10502185672521591\n",
      "[240/500] train loss: 0.036393605172634125, val loss: 0.10490702837705612, val accuracy: 0.965034965034965\n",
      "240 epoch에서 저장. 이전 score: 0.10502185672521591, 현재 score: 0.10490702837705612\n",
      "[241/500] train loss: 0.036230072379112244, val loss: 0.10479319840669632, val accuracy: 0.965034965034965\n",
      "241 epoch에서 저장. 이전 score: 0.10490702837705612, 현재 score: 0.10479319840669632\n",
      "[242/500] train loss: 0.03606724739074707, val loss: 0.10468017309904099, val accuracy: 0.965034965034965\n",
      "242 epoch에서 저장. 이전 score: 0.10479319840669632, 현재 score: 0.10468017309904099\n",
      "[243/500] train loss: 0.03590504825115204, val loss: 0.10456966608762741, val accuracy: 0.965034965034965\n",
      "243 epoch에서 저장. 이전 score: 0.10468017309904099, 현재 score: 0.10456966608762741\n",
      "[244/500] train loss: 0.03574378043413162, val loss: 0.10446090996265411, val accuracy: 0.965034965034965\n",
      "244 epoch에서 저장. 이전 score: 0.10456966608762741, 현재 score: 0.10446090996265411\n",
      "[245/500] train loss: 0.035583287477493286, val loss: 0.10435653477907181, val accuracy: 0.965034965034965\n",
      "245 epoch에서 저장. 이전 score: 0.10446090996265411, 현재 score: 0.10435653477907181\n",
      "[246/500] train loss: 0.03542433679103851, val loss: 0.10425587743520737, val accuracy: 0.965034965034965\n",
      "246 epoch에서 저장. 이전 score: 0.10435653477907181, 현재 score: 0.10425587743520737\n",
      "[247/500] train loss: 0.035266898572444916, val loss: 0.1041642278432846, val accuracy: 0.965034965034965\n",
      "247 epoch에서 저장. 이전 score: 0.10425587743520737, 현재 score: 0.1041642278432846\n",
      "[248/500] train loss: 0.0351102240383625, val loss: 0.10407476127147675, val accuracy: 0.965034965034965\n",
      "248 epoch에서 저장. 이전 score: 0.1041642278432846, 현재 score: 0.10407476127147675\n",
      "[249/500] train loss: 0.03495366498827934, val loss: 0.10398843139410019, val accuracy: 0.965034965034965\n",
      "249 epoch에서 저장. 이전 score: 0.10407476127147675, 현재 score: 0.10398843139410019\n",
      "[250/500] train loss: 0.034797776490449905, val loss: 0.10390659421682358, val accuracy: 0.965034965034965\n",
      "250 epoch에서 저장. 이전 score: 0.10398843139410019, 현재 score: 0.10390659421682358\n",
      "[251/500] train loss: 0.03464338183403015, val loss: 0.10382697731256485, val accuracy: 0.965034965034965\n",
      "251 epoch에서 저장. 이전 score: 0.10390659421682358, 현재 score: 0.10382697731256485\n",
      "[252/500] train loss: 0.03448938578367233, val loss: 0.10374711453914642, val accuracy: 0.965034965034965\n",
      "252 epoch에서 저장. 이전 score: 0.10382697731256485, 현재 score: 0.10374711453914642\n",
      "[253/500] train loss: 0.03433673828840256, val loss: 0.10366921871900558, val accuracy: 0.965034965034965\n",
      "253 epoch에서 저장. 이전 score: 0.10374711453914642, 현재 score: 0.10366921871900558\n",
      "[254/500] train loss: 0.03418485075235367, val loss: 0.10358960181474686, val accuracy: 0.965034965034965\n",
      "254 epoch에서 저장. 이전 score: 0.10366921871900558, 현재 score: 0.10358960181474686\n",
      "[255/500] train loss: 0.03403323143720627, val loss: 0.10350916534662247, val accuracy: 0.965034965034965\n",
      "255 epoch에서 저장. 이전 score: 0.10358960181474686, 현재 score: 0.10350916534662247\n",
      "[256/500] train loss: 0.03388233855366707, val loss: 0.10342694073915482, val accuracy: 0.965034965034965\n",
      "256 epoch에서 저장. 이전 score: 0.10350916534662247, 현재 score: 0.10342694073915482\n",
      "[257/500] train loss: 0.03373175486922264, val loss: 0.10334697365760803, val accuracy: 0.965034965034965\n",
      "257 epoch에서 저장. 이전 score: 0.10342694073915482, 현재 score: 0.10334697365760803\n",
      "[258/500] train loss: 0.03358296677470207, val loss: 0.10326849669218063, val accuracy: 0.965034965034965\n",
      "258 epoch에서 저장. 이전 score: 0.10334697365760803, 현재 score: 0.10326849669218063\n",
      "[259/500] train loss: 0.03343472629785538, val loss: 0.10318629443645477, val accuracy: 0.965034965034965\n",
      "259 epoch에서 저장. 이전 score: 0.10326849669218063, 현재 score: 0.10318629443645477\n",
      "[260/500] train loss: 0.033286839723587036, val loss: 0.10310742259025574, val accuracy: 0.965034965034965\n",
      "260 epoch에서 저장. 이전 score: 0.10318629443645477, 현재 score: 0.10310742259025574\n",
      "[261/500] train loss: 0.03314011171460152, val loss: 0.10302755236625671, val accuracy: 0.965034965034965\n",
      "261 epoch에서 저장. 이전 score: 0.10310742259025574, 현재 score: 0.10302755236625671\n",
      "[262/500] train loss: 0.03299397975206375, val loss: 0.10294762998819351, val accuracy: 0.965034965034965\n",
      "262 epoch에서 저장. 이전 score: 0.10302755236625671, 현재 score: 0.10294762998819351\n",
      "[263/500] train loss: 0.03284803777933121, val loss: 0.10286982357501984, val accuracy: 0.965034965034965\n",
      "263 epoch에서 저장. 이전 score: 0.10294762998819351, 현재 score: 0.10286982357501984\n",
      "[264/500] train loss: 0.0327022410929203, val loss: 0.1027938723564148, val accuracy: 0.965034965034965\n",
      "264 epoch에서 저장. 이전 score: 0.10286982357501984, 현재 score: 0.1027938723564148\n",
      "[265/500] train loss: 0.03255675733089447, val loss: 0.10271568596363068, val accuracy: 0.965034965034965\n",
      "265 epoch에서 저장. 이전 score: 0.1027938723564148, 현재 score: 0.10271568596363068\n",
      "[266/500] train loss: 0.03241274878382683, val loss: 0.10263807326555252, val accuracy: 0.965034965034965\n",
      "266 epoch에서 저장. 이전 score: 0.10271568596363068, 현재 score: 0.10263807326555252\n",
      "[267/500] train loss: 0.032269593328237534, val loss: 0.10256265848875046, val accuracy: 0.965034965034965\n",
      "267 epoch에서 저장. 이전 score: 0.10263807326555252, 현재 score: 0.10256265848875046\n",
      "[268/500] train loss: 0.03212592005729675, val loss: 0.10248889029026031, val accuracy: 0.965034965034965\n",
      "268 epoch에서 저장. 이전 score: 0.10256265848875046, 현재 score: 0.10248889029026031\n",
      "[269/500] train loss: 0.03198312222957611, val loss: 0.10241816192865372, val accuracy: 0.965034965034965\n",
      "269 epoch에서 저장. 이전 score: 0.10248889029026031, 현재 score: 0.10241816192865372\n",
      "[270/500] train loss: 0.031841594725847244, val loss: 0.10234940052032471, val accuracy: 0.965034965034965\n",
      "270 epoch에서 저장. 이전 score: 0.10241816192865372, 현재 score: 0.10234940052032471\n",
      "[271/500] train loss: 0.031700920313596725, val loss: 0.10228140652179718, val accuracy: 0.965034965034965\n",
      "271 epoch에서 저장. 이전 score: 0.10234940052032471, 현재 score: 0.10228140652179718\n",
      "[272/500] train loss: 0.03156092017889023, val loss: 0.10221192240715027, val accuracy: 0.965034965034965\n",
      "272 epoch에서 저장. 이전 score: 0.10228140652179718, 현재 score: 0.10221192240715027\n",
      "[273/500] train loss: 0.0314209870994091, val loss: 0.10214059799909592, val accuracy: 0.965034965034965\n",
      "273 epoch에서 저장. 이전 score: 0.10221192240715027, 현재 score: 0.10214059799909592\n",
      "[274/500] train loss: 0.03128121420741081, val loss: 0.10206814110279083, val accuracy: 0.965034965034965\n",
      "274 epoch에서 저장. 이전 score: 0.10214059799909592, 현재 score: 0.10206814110279083\n",
      "[275/500] train loss: 0.031141778454184532, val loss: 0.10199975222349167, val accuracy: 0.965034965034965\n",
      "275 epoch에서 저장. 이전 score: 0.10206814110279083, 현재 score: 0.10199975222349167\n",
      "[276/500] train loss: 0.031004121527075768, val loss: 0.10193575173616409, val accuracy: 0.965034965034965\n",
      "276 epoch에서 저장. 이전 score: 0.10199975222349167, 현재 score: 0.10193575173616409\n",
      "[277/500] train loss: 0.030866868793964386, val loss: 0.10187709331512451, val accuracy: 0.965034965034965\n",
      "277 epoch에서 저장. 이전 score: 0.10193575173616409, 현재 score: 0.10187709331512451\n",
      "[278/500] train loss: 0.030730055645108223, val loss: 0.10182171314954758, val accuracy: 0.965034965034965\n",
      "278 epoch에서 저장. 이전 score: 0.10187709331512451, 현재 score: 0.10182171314954758\n",
      "[279/500] train loss: 0.03059396892786026, val loss: 0.10176648199558258, val accuracy: 0.965034965034965\n",
      "279 epoch에서 저장. 이전 score: 0.10182171314954758, 현재 score: 0.10176648199558258\n",
      "[280/500] train loss: 0.03045792505145073, val loss: 0.1017104834318161, val accuracy: 0.965034965034965\n",
      "280 epoch에서 저장. 이전 score: 0.10176648199558258, 현재 score: 0.1017104834318161\n",
      "[281/500] train loss: 0.030322525650262833, val loss: 0.10165523737668991, val accuracy: 0.965034965034965\n",
      "281 epoch에서 저장. 이전 score: 0.1017104834318161, 현재 score: 0.10165523737668991\n",
      "[282/500] train loss: 0.030187925323843956, val loss: 0.1015976145863533, val accuracy: 0.965034965034965\n",
      "282 epoch에서 저장. 이전 score: 0.10165523737668991, 현재 score: 0.1015976145863533\n",
      "[283/500] train loss: 0.03005407005548477, val loss: 0.10153816640377045, val accuracy: 0.965034965034965\n",
      "283 epoch에서 저장. 이전 score: 0.1015976145863533, 현재 score: 0.10153816640377045\n",
      "[284/500] train loss: 0.02992100827395916, val loss: 0.10148028284311295, val accuracy: 0.965034965034965\n",
      "284 epoch에서 저장. 이전 score: 0.10153816640377045, 현재 score: 0.10148028284311295\n",
      "[285/500] train loss: 0.02978820726275444, val loss: 0.10142586380243301, val accuracy: 0.965034965034965\n",
      "285 epoch에서 저장. 이전 score: 0.10148028284311295, 현재 score: 0.10142586380243301\n",
      "[286/500] train loss: 0.029655320569872856, val loss: 0.10137671232223511, val accuracy: 0.965034965034965\n",
      "286 epoch에서 저장. 이전 score: 0.10142586380243301, 현재 score: 0.10137671232223511\n",
      "[287/500] train loss: 0.029523534700274467, val loss: 0.1013321727514267, val accuracy: 0.965034965034965\n",
      "287 epoch에서 저장. 이전 score: 0.10137671232223511, 현재 score: 0.1013321727514267\n",
      "[288/500] train loss: 0.029392139986157417, val loss: 0.10128925740718842, val accuracy: 0.965034965034965\n",
      "288 epoch에서 저장. 이전 score: 0.1013321727514267, 현재 score: 0.10128925740718842\n",
      "[289/500] train loss: 0.029261276125907898, val loss: 0.10124664008617401, val accuracy: 0.965034965034965\n",
      "289 epoch에서 저장. 이전 score: 0.10128925740718842, 현재 score: 0.10124664008617401\n",
      "[290/500] train loss: 0.029130561277270317, val loss: 0.10120368003845215, val accuracy: 0.965034965034965\n",
      "290 epoch에서 저장. 이전 score: 0.10124664008617401, 현재 score: 0.10120368003845215\n",
      "[291/500] train loss: 0.029000556096434593, val loss: 0.10115625709295273, val accuracy: 0.965034965034965\n",
      "291 epoch에서 저장. 이전 score: 0.10120368003845215, 현재 score: 0.10115625709295273\n",
      "[292/500] train loss: 0.02887200005352497, val loss: 0.10110883414745331, val accuracy: 0.965034965034965\n",
      "292 epoch에서 저장. 이전 score: 0.10115625709295273, 현재 score: 0.10110883414745331\n",
      "[293/500] train loss: 0.028743108734488487, val loss: 0.10106071084737778, val accuracy: 0.965034965034965\n",
      "293 epoch에서 저장. 이전 score: 0.10110883414745331, 현재 score: 0.10106071084737778\n",
      "[294/500] train loss: 0.028613978996872902, val loss: 0.10101150721311569, val accuracy: 0.965034965034965\n",
      "294 epoch에서 저장. 이전 score: 0.10106071084737778, 현재 score: 0.10101150721311569\n",
      "[295/500] train loss: 0.028486374765634537, val loss: 0.10095717012882233, val accuracy: 0.965034965034965\n",
      "295 epoch에서 저장. 이전 score: 0.10101150721311569, 현재 score: 0.10095717012882233\n",
      "[296/500] train loss: 0.028358114883303642, val loss: 0.1008976474404335, val accuracy: 0.965034965034965\n",
      "296 epoch에서 저장. 이전 score: 0.10095717012882233, 현재 score: 0.1008976474404335\n",
      "[297/500] train loss: 0.028230715543031693, val loss: 0.10083739459514618, val accuracy: 0.965034965034965\n",
      "297 epoch에서 저장. 이전 score: 0.1008976474404335, 현재 score: 0.10083739459514618\n",
      "[298/500] train loss: 0.028103750199079514, val loss: 0.10077950358390808, val accuracy: 0.965034965034965\n",
      "298 epoch에서 저장. 이전 score: 0.10083739459514618, 현재 score: 0.10077950358390808\n",
      "[299/500] train loss: 0.02797665446996689, val loss: 0.10072361677885056, val accuracy: 0.965034965034965\n",
      "299 epoch에서 저장. 이전 score: 0.10077950358390808, 현재 score: 0.10072361677885056\n",
      "[300/500] train loss: 0.02784944884479046, val loss: 0.10066919028759003, val accuracy: 0.965034965034965\n",
      "300 epoch에서 저장. 이전 score: 0.10072361677885056, 현재 score: 0.10066919028759003\n",
      "[301/500] train loss: 0.02772568352520466, val loss: 0.10061461478471756, val accuracy: 0.965034965034965\n",
      "301 epoch에서 저장. 이전 score: 0.10066919028759003, 현재 score: 0.10061461478471756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[302/500] train loss: 0.02760441228747368, val loss: 0.10056021809577942, val accuracy: 0.965034965034965\n",
      "302 epoch에서 저장. 이전 score: 0.10061461478471756, 현재 score: 0.10056021809577942\n",
      "[303/500] train loss: 0.027483271434903145, val loss: 0.10050281137228012, val accuracy: 0.965034965034965\n",
      "303 epoch에서 저장. 이전 score: 0.10056021809577942, 현재 score: 0.10050281137228012\n",
      "[304/500] train loss: 0.027362290769815445, val loss: 0.10045913606882095, val accuracy: 0.965034965034965\n",
      "304 epoch에서 저장. 이전 score: 0.10050281137228012, 현재 score: 0.10045913606882095\n",
      "[305/500] train loss: 0.0272427499294281, val loss: 0.10041501373052597, val accuracy: 0.965034965034965\n",
      "305 epoch에서 저장. 이전 score: 0.10045913606882095, 현재 score: 0.10041501373052597\n",
      "[306/500] train loss: 0.027123158797621727, val loss: 0.10037398338317871, val accuracy: 0.965034965034965\n",
      "306 epoch에서 저장. 이전 score: 0.10041501373052597, 현재 score: 0.10037398338317871\n",
      "[307/500] train loss: 0.027004670351743698, val loss: 0.10033386945724487, val accuracy: 0.965034965034965\n",
      "307 epoch에서 저장. 이전 score: 0.10037398338317871, 현재 score: 0.10033386945724487\n",
      "[308/500] train loss: 0.02688603103160858, val loss: 0.10029472410678864, val accuracy: 0.965034965034965\n",
      "308 epoch에서 저장. 이전 score: 0.10033386945724487, 현재 score: 0.10029472410678864\n",
      "[309/500] train loss: 0.02676851861178875, val loss: 0.10025978088378906, val accuracy: 0.965034965034965\n",
      "309 epoch에서 저장. 이전 score: 0.10029472410678864, 현재 score: 0.10025978088378906\n",
      "[310/500] train loss: 0.026650777086615562, val loss: 0.10022727400064468, val accuracy: 0.965034965034965\n",
      "310 epoch에서 저장. 이전 score: 0.10025978088378906, 현재 score: 0.10022727400064468\n",
      "[311/500] train loss: 0.026533234864473343, val loss: 0.10019456595182419, val accuracy: 0.965034965034965\n",
      "311 epoch에서 저장. 이전 score: 0.10022727400064468, 현재 score: 0.10019456595182419\n",
      "[312/500] train loss: 0.026415692642331123, val loss: 0.10016129910945892, val accuracy: 0.965034965034965\n",
      "312 epoch에서 저장. 이전 score: 0.10019456595182419, 현재 score: 0.10016129910945892\n",
      "[313/500] train loss: 0.026299461722373962, val loss: 0.10012636333703995, val accuracy: 0.965034965034965\n",
      "313 epoch에서 저장. 이전 score: 0.10016129910945892, 현재 score: 0.10012636333703995\n",
      "[314/500] train loss: 0.0261840857565403, val loss: 0.10008867084980011, val accuracy: 0.965034965034965\n",
      "314 epoch에서 저장. 이전 score: 0.10012636333703995, 현재 score: 0.10008867084980011\n",
      "[315/500] train loss: 0.02606862410902977, val loss: 0.10005172342061996, val accuracy: 0.965034965034965\n",
      "315 epoch에서 저장. 이전 score: 0.10008867084980011, 현재 score: 0.10005172342061996\n",
      "[316/500] train loss: 0.025953903794288635, val loss: 0.10002104192972183, val accuracy: 0.965034965034965\n",
      "316 epoch에서 저장. 이전 score: 0.10005172342061996, 현재 score: 0.10002104192972183\n",
      "[317/500] train loss: 0.025839461013674736, val loss: 0.09999454021453857, val accuracy: 0.965034965034965\n",
      "317 epoch에서 저장. 이전 score: 0.10002104192972183, 현재 score: 0.09999454021453857\n",
      "[318/500] train loss: 0.025724785402417183, val loss: 0.09997014701366425, val accuracy: 0.965034965034965\n",
      "318 epoch에서 저장. 이전 score: 0.09999454021453857, 현재 score: 0.09997014701366425\n",
      "[319/500] train loss: 0.02561001479625702, val loss: 0.09994598478078842, val accuracy: 0.965034965034965\n",
      "319 epoch에서 저장. 이전 score: 0.09997014701366425, 현재 score: 0.09994598478078842\n",
      "[320/500] train loss: 0.025494977831840515, val loss: 0.09992578625679016, val accuracy: 0.965034965034965\n",
      "320 epoch에서 저장. 이전 score: 0.09994598478078842, 현재 score: 0.09992578625679016\n",
      "[321/500] train loss: 0.025380291044712067, val loss: 0.09990572929382324, val accuracy: 0.965034965034965\n",
      "321 epoch에서 저장. 이전 score: 0.09992578625679016, 현재 score: 0.09990572929382324\n",
      "[322/500] train loss: 0.025266345590353012, val loss: 0.09988715499639511, val accuracy: 0.965034965034965\n",
      "322 epoch에서 저장. 이전 score: 0.09990572929382324, 현재 score: 0.09988715499639511\n",
      "[323/500] train loss: 0.025152109563350677, val loss: 0.09986834973096848, val accuracy: 0.965034965034965\n",
      "323 epoch에서 저장. 이전 score: 0.09988715499639511, 현재 score: 0.09986834973096848\n",
      "[324/500] train loss: 0.02503805235028267, val loss: 0.09984789788722992, val accuracy: 0.965034965034965\n",
      "324 epoch에서 저장. 이전 score: 0.09986834973096848, 현재 score: 0.09984789788722992\n",
      "[325/500] train loss: 0.024925366044044495, val loss: 0.09982499480247498, val accuracy: 0.965034965034965\n",
      "325 epoch에서 저장. 이전 score: 0.09984789788722992, 현재 score: 0.09982499480247498\n",
      "[326/500] train loss: 0.024812819436192513, val loss: 0.0998033881187439, val accuracy: 0.965034965034965\n",
      "326 epoch에서 저장. 이전 score: 0.09982499480247498, 현재 score: 0.0998033881187439\n",
      "[327/500] train loss: 0.024700526148080826, val loss: 0.09978371858596802, val accuracy: 0.965034965034965\n",
      "327 epoch에서 저장. 이전 score: 0.0998033881187439, 현재 score: 0.09978371858596802\n",
      "[328/500] train loss: 0.02458794414997101, val loss: 0.09976444393396378, val accuracy: 0.965034965034965\n",
      "328 epoch에서 저장. 이전 score: 0.09978371858596802, 현재 score: 0.09976444393396378\n",
      "[329/500] train loss: 0.024476947262883186, val loss: 0.09974605590105057, val accuracy: 0.965034965034965\n",
      "329 epoch에서 저장. 이전 score: 0.09976444393396378, 현재 score: 0.09974605590105057\n",
      "[330/500] train loss: 0.02436487004160881, val loss: 0.09973055124282837, val accuracy: 0.965034965034965\n",
      "330 epoch에서 저장. 이전 score: 0.09974605590105057, 현재 score: 0.09973055124282837\n",
      "[331/500] train loss: 0.02425277605652809, val loss: 0.09972085803747177, val accuracy: 0.965034965034965\n",
      "331 epoch에서 저장. 이전 score: 0.09973055124282837, 현재 score: 0.09972085803747177\n",
      "[332/500] train loss: 0.024141089990735054, val loss: 0.09971582144498825, val accuracy: 0.965034965034965\n",
      "332 epoch에서 저장. 이전 score: 0.09972085803747177, 현재 score: 0.09971582144498825\n",
      "[333/500] train loss: 0.02402990683913231, val loss: 0.09971421957015991, val accuracy: 0.965034965034965\n",
      "333 epoch에서 저장. 이전 score: 0.09971582144498825, 현재 score: 0.09971421957015991\n",
      "[334/500] train loss: 0.02391935884952545, val loss: 0.09971174597740173, val accuracy: 0.965034965034965\n",
      "334 epoch에서 저장. 이전 score: 0.09971421957015991, 현재 score: 0.09971174597740173\n",
      "[335/500] train loss: 0.02380993403494358, val loss: 0.09970857203006744, val accuracy: 0.965034965034965\n",
      "335 epoch에서 저장. 이전 score: 0.09971174597740173, 현재 score: 0.09970857203006744\n",
      "[336/500] train loss: 0.023699646815657616, val loss: 0.09970900416374207, val accuracy: 0.965034965034965\n",
      "[337/500] train loss: 0.02359057404100895, val loss: 0.09971436113119125, val accuracy: 0.965034965034965\n",
      "[338/500] train loss: 0.02348182164132595, val loss: 0.09972067922353745, val accuracy: 0.965034965034965\n",
      "[339/500] train loss: 0.02337236888706684, val loss: 0.09973068535327911, val accuracy: 0.965034965034965\n",
      "[340/500] train loss: 0.023263070732355118, val loss: 0.09974118322134018, val accuracy: 0.965034965034965\n",
      "[341/500] train loss: 0.023155320435762405, val loss: 0.09974965453147888, val accuracy: 0.965034965034965\n",
      "[342/500] train loss: 0.023046860471367836, val loss: 0.099755197763443, val accuracy: 0.965034965034965\n",
      "[343/500] train loss: 0.022939292713999748, val loss: 0.0997651144862175, val accuracy: 0.965034965034965\n",
      "[344/500] train loss: 0.022832650691270828, val loss: 0.09978068619966507, val accuracy: 0.965034965034965\n",
      "[345/500] train loss: 0.022724764421582222, val loss: 0.0997987687587738, val accuracy: 0.965034965034965\n",
      "344 epoch에서 종료. 0.09970857203006744에서 성능이 개선되지 않음\n",
      "걸린 시간: 3.6029112339019775초\n"
     ]
    }
   ],
   "source": [
    "# 학습처리\n",
    "N_EPOCH = 500\n",
    "train_loss_list, val_loss_list, val_acc_list = [], [], []\n",
    "\n",
    "model = BreastCancerModel().to(device)\n",
    "loss_fn = nn.BCELoss()  # 이진분류 - postive 확률 출력 모델의 loss-binary cross entorpy loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "# early stopping, 모델 저장을 위한 변수\n",
    "best_score = torch.inf\n",
    "save_model_path_bc = 'models/breast_cancer_best_model.pt'\n",
    "\n",
    "patience = 10  # 개선될때까지 몇 번 기다릴지\n",
    "trigger_cnt = 0  # 몇 번째 기다렸는지\n",
    "\n",
    "s= time.time()\n",
    "for epoch in range(N_EPOCH):\n",
    "    ## 학습\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X, y in wb_train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # 추정\n",
    "        pred = model(X)  # positive 확률 ==> loss 계산\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # 파라미터 업데이트\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    # 학습종료 -> train loss 평균\n",
    "    train_loss /= len(wb_train_loader)\n",
    "    \n",
    "    ## 검증\n",
    "    model.eval()\n",
    "    \n",
    "    val_loss, val_acc = 0., 0.\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in wb_test_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            \n",
    "            # 추정\n",
    "            pred_val = model(X_val)  # postive의 확률 => 0.xx 형태로 반환 ==> loss 계산할 때 사용\n",
    "            \n",
    "            # 0.5 기준으로 크면 양성, bool type(True / False)을 int type(1, 0)로 변환\n",
    "            pred_label = (pred_val >= 0.5).type(torch.int32)  # ==> accuracy 계산할 때 사용\n",
    "            \n",
    "            loss_val = loss_fn(pred_val, y_val)\n",
    "            val_loss += loss_val.item()\n",
    "            val_acc += torch.sum(pred_label == y_val).item()\n",
    "            \n",
    "        val_loss /= len(wb_test_loader)\n",
    "        val_acc /= len(wb_test_loader.dataset)\n",
    "        \n",
    "    # 현재 epoch에 대한 학습/검증 종료\n",
    "    # 로그출력\n",
    "    print(f'[{epoch + 1}/{N_EPOCH}] train loss: {train_loss}, val loss: {val_loss}, val accuracy: {val_acc}')\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "    \n",
    "    # early stopping, model 저장 처리\n",
    "    if val_loss < best_score:  # 성능개선되었다면\n",
    "        print(f'{epoch + 1} epoch에서 저장. 이전 score: {best_score}, 현재 score: {val_loss}')\n",
    "        best_score = val_loss\n",
    "        torch.save(model, save_model_path_bc)\n",
    "        trigger_cnt = 0\n",
    "    else:  # 성능이 개선 안되었다면\n",
    "        trigger_cnt += 1\n",
    "        if patience == trigger_cnt:  # 조기 종료\n",
    "            print(f'{epoch} epoch에서 종료. {best_score}에서 성능이 개선되지 않음')\n",
    "            break\n",
    "\n",
    "e = time.time()\n",
    "print(f'걸린 시간: {e-s}초')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BreastCancerModel                        [100, 1]                  --\n",
       "├─Linear: 1-1                            [100, 32]                 992\n",
       "├─Linear: 1-2                            [100, 16]                 528\n",
       "├─Linear: 1-3                            [100, 1]                  17\n",
       "==========================================================================================\n",
       "Total params: 1,537\n",
       "Trainable params: 1,537\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.15\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.04\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.06\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model, (100, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('LOSS')\n",
    "plt.plot(train_loss_list, label = 'train')\n",
    "plt.plot(val_loss_list, label = 'validation')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(val_acc_list)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BreastCancerModel(\n",
       "  (lr1): Linear(in_features=30, out_features=32, bias=True)\n",
       "  (lr2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (output): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "load_model = torch.load(save_model_path_bc)\n",
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = 0., 0.\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in wb_test_loader:\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "\n",
    "        # 추정\n",
    "        pred_val = load_model(X_val)  # postive의 확률 => 0.xx 형태로 반환 ==> loss 계산할 때 사용\n",
    "\n",
    "        # 0.5 기준으로 크면 양성, bool type(True / False)을 int type(1, 0)로 변환\n",
    "        pred_label = (pred_val >= 0.5).type(torch.int32)  # ==> accuracy 계산할 때 사용\n",
    "\n",
    "        loss_val = loss_fn(pred_val, y_val)\n",
    "        val_loss += loss_val.item()\n",
    "        val_acc += torch.sum(pred_label == y_val).item()\n",
    "\n",
    "    val_loss /= len(wb_test_loader)\n",
    "    val_acc /= len(wb_test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09970857203006744, 0.965034965034965)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진 분류\n",
    "model = BreastCancerModel()\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100] - Train loss: 0.70824 Train Accucracy: 0.26526 || Validation Loss: 0.70639 Validation Accuracy: 0.25874\n",
      "====================================================================================================\n",
      "저장: 1 - 이전 : inf, 현재: 0.706386387348175\n",
      "Epoch[2/100] - Train loss: 0.70139 Train Accucracy: 0.34272 || Validation Loss: 0.69991 Validation Accuracy: 0.31469\n",
      "====================================================================================================\n",
      "저장: 2 - 이전 : 0.706386387348175, 현재: 0.6999145746231079\n",
      "Epoch[3/100] - Train loss: 0.69463 Train Accucracy: 0.46948 || Validation Loss: 0.69361 Validation Accuracy: 0.46853\n",
      "====================================================================================================\n",
      "저장: 3 - 이전 : 0.6999145746231079, 현재: 0.6936080455780029\n",
      "Epoch[4/100] - Train loss: 0.68794 Train Accucracy: 0.60094 || Validation Loss: 0.68742 Validation Accuracy: 0.60839\n",
      "====================================================================================================\n",
      "저장: 4 - 이전 : 0.6936080455780029, 현재: 0.6874219179153442\n",
      "Epoch[5/100] - Train loss: 0.68137 Train Accucracy: 0.73709 || Validation Loss: 0.68130 Validation Accuracy: 0.69930\n",
      "====================================================================================================\n",
      "저장: 5 - 이전 : 0.6874219179153442, 현재: 0.6813010573387146\n",
      "Epoch[6/100] - Train loss: 0.67494 Train Accucracy: 0.79577 || Validation Loss: 0.67534 Validation Accuracy: 0.75524\n",
      "====================================================================================================\n",
      "저장: 6 - 이전 : 0.6813010573387146, 현재: 0.6753443479537964\n",
      "Epoch[7/100] - Train loss: 0.66863 Train Accucracy: 0.83568 || Validation Loss: 0.66945 Validation Accuracy: 0.80420\n",
      "====================================================================================================\n",
      "저장: 7 - 이전 : 0.6753443479537964, 현재: 0.6694453954696655\n",
      "Epoch[8/100] - Train loss: 0.66242 Train Accucracy: 0.86854 || Validation Loss: 0.66366 Validation Accuracy: 0.86014\n",
      "====================================================================================================\n",
      "저장: 8 - 이전 : 0.6694453954696655, 현재: 0.6636608839035034\n",
      "Epoch[9/100] - Train loss: 0.65629 Train Accucracy: 0.88498 || Validation Loss: 0.65790 Validation Accuracy: 0.87413\n",
      "====================================================================================================\n",
      "저장: 9 - 이전 : 0.6636608839035034, 현재: 0.6579005122184753\n",
      "Epoch[10/100] - Train loss: 0.65019 Train Accucracy: 0.90141 || Validation Loss: 0.65218 Validation Accuracy: 0.89510\n",
      "====================================================================================================\n",
      "저장: 10 - 이전 : 0.6579005122184753, 현재: 0.6521832942962646\n",
      "Epoch[11/100] - Train loss: 0.64410 Train Accucracy: 0.92019 || Validation Loss: 0.64649 Validation Accuracy: 0.90909\n",
      "====================================================================================================\n",
      "저장: 11 - 이전 : 0.6521832942962646, 현재: 0.6464910507202148\n",
      "Epoch[12/100] - Train loss: 0.63797 Train Accucracy: 0.92254 || Validation Loss: 0.64079 Validation Accuracy: 0.90909\n",
      "====================================================================================================\n",
      "저장: 12 - 이전 : 0.6464910507202148, 현재: 0.6407874822616577\n",
      "Epoch[13/100] - Train loss: 0.63181 Train Accucracy: 0.92723 || Validation Loss: 0.63498 Validation Accuracy: 0.91608\n",
      "====================================================================================================\n",
      "저장: 13 - 이전 : 0.6407874822616577, 현재: 0.6349831819534302\n",
      "Epoch[14/100] - Train loss: 0.62555 Train Accucracy: 0.92958 || Validation Loss: 0.62916 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 14 - 이전 : 0.6349831819534302, 현재: 0.6291593313217163\n",
      "Epoch[15/100] - Train loss: 0.61917 Train Accucracy: 0.93192 || Validation Loss: 0.62326 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 15 - 이전 : 0.6291593313217163, 현재: 0.6232640147209167\n",
      "Epoch[16/100] - Train loss: 0.61266 Train Accucracy: 0.93427 || Validation Loss: 0.61725 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 16 - 이전 : 0.6232640147209167, 현재: 0.6172486543655396\n",
      "Epoch[17/100] - Train loss: 0.60604 Train Accucracy: 0.93427 || Validation Loss: 0.61116 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 17 - 이전 : 0.6172486543655396, 현재: 0.6111593842506409\n",
      "Epoch[18/100] - Train loss: 0.59928 Train Accucracy: 0.93427 || Validation Loss: 0.60495 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 18 - 이전 : 0.6111593842506409, 현재: 0.6049458384513855\n",
      "Epoch[19/100] - Train loss: 0.59238 Train Accucracy: 0.93427 || Validation Loss: 0.59852 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 19 - 이전 : 0.6049458384513855, 현재: 0.5985233783721924\n",
      "Epoch[20/100] - Train loss: 0.58530 Train Accucracy: 0.93192 || Validation Loss: 0.59187 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 20 - 이전 : 0.5985233783721924, 현재: 0.5918731689453125\n",
      "Epoch[21/100] - Train loss: 0.57803 Train Accucracy: 0.93427 || Validation Loss: 0.58505 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 21 - 이전 : 0.5918731689453125, 현재: 0.5850529670715332\n",
      "Epoch[22/100] - Train loss: 0.57058 Train Accucracy: 0.93662 || Validation Loss: 0.57803 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 22 - 이전 : 0.5850529670715332, 현재: 0.5780261754989624\n",
      "Epoch[23/100] - Train loss: 0.56291 Train Accucracy: 0.94131 || Validation Loss: 0.57080 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 23 - 이전 : 0.5780261754989624, 현재: 0.5707951784133911\n",
      "Epoch[24/100] - Train loss: 0.55501 Train Accucracy: 0.94131 || Validation Loss: 0.56337 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 24 - 이전 : 0.5707951784133911, 현재: 0.5633715391159058\n",
      "Epoch[25/100] - Train loss: 0.54687 Train Accucracy: 0.94131 || Validation Loss: 0.55571 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 25 - 이전 : 0.5633715391159058, 현재: 0.555713951587677\n",
      "Epoch[26/100] - Train loss: 0.53847 Train Accucracy: 0.94131 || Validation Loss: 0.54784 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 26 - 이전 : 0.555713951587677, 현재: 0.547840416431427\n",
      "Epoch[27/100] - Train loss: 0.52983 Train Accucracy: 0.94366 || Validation Loss: 0.53975 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 27 - 이전 : 0.547840416431427, 현재: 0.5397519469261169\n",
      "Epoch[28/100] - Train loss: 0.52090 Train Accucracy: 0.94366 || Validation Loss: 0.53149 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 28 - 이전 : 0.5397519469261169, 현재: 0.5314928889274597\n",
      "Epoch[29/100] - Train loss: 0.51172 Train Accucracy: 0.94366 || Validation Loss: 0.52306 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 29 - 이전 : 0.5314928889274597, 현재: 0.5230638980865479\n",
      "Epoch[30/100] - Train loss: 0.50231 Train Accucracy: 0.94601 || Validation Loss: 0.51436 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 30 - 이전 : 0.5230638980865479, 현재: 0.5143627524375916\n",
      "Epoch[31/100] - Train loss: 0.49263 Train Accucracy: 0.94601 || Validation Loss: 0.50546 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 31 - 이전 : 0.5143627524375916, 현재: 0.5054587721824646\n",
      "Epoch[32/100] - Train loss: 0.48272 Train Accucracy: 0.94601 || Validation Loss: 0.49632 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 32 - 이전 : 0.5054587721824646, 현재: 0.4963151216506958\n",
      "Epoch[33/100] - Train loss: 0.47258 Train Accucracy: 0.94601 || Validation Loss: 0.48697 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 33 - 이전 : 0.4963151216506958, 현재: 0.48696932196617126\n",
      "Epoch[34/100] - Train loss: 0.46224 Train Accucracy: 0.94601 || Validation Loss: 0.47741 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 34 - 이전 : 0.48696932196617126, 현재: 0.47740793228149414\n",
      "Epoch[35/100] - Train loss: 0.45172 Train Accucracy: 0.94601 || Validation Loss: 0.46769 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 35 - 이전 : 0.47740793228149414, 현재: 0.4676884412765503\n",
      "Epoch[36/100] - Train loss: 0.44107 Train Accucracy: 0.94601 || Validation Loss: 0.45783 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 36 - 이전 : 0.4676884412765503, 현재: 0.45783111453056335\n",
      "Epoch[37/100] - Train loss: 0.43032 Train Accucracy: 0.94601 || Validation Loss: 0.44786 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 37 - 이전 : 0.45783111453056335, 현재: 0.4478628635406494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[38/100] - Train loss: 0.41951 Train Accucracy: 0.94601 || Validation Loss: 0.43780 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 38 - 이전 : 0.4478628635406494, 현재: 0.4377982020378113\n",
      "Epoch[39/100] - Train loss: 0.40867 Train Accucracy: 0.94601 || Validation Loss: 0.42771 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 39 - 이전 : 0.4377982020378113, 현재: 0.4277065396308899\n",
      "Epoch[40/100] - Train loss: 0.39783 Train Accucracy: 0.94601 || Validation Loss: 0.41764 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 40 - 이전 : 0.4277065396308899, 현재: 0.4176439940929413\n",
      "Epoch[41/100] - Train loss: 0.38702 Train Accucracy: 0.94601 || Validation Loss: 0.40764 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 41 - 이전 : 0.4176439940929413, 현재: 0.4076385498046875\n",
      "Epoch[42/100] - Train loss: 0.37626 Train Accucracy: 0.94601 || Validation Loss: 0.39770 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 42 - 이전 : 0.4076385498046875, 현재: 0.397696852684021\n",
      "Epoch[43/100] - Train loss: 0.36559 Train Accucracy: 0.94601 || Validation Loss: 0.38783 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 43 - 이전 : 0.397696852684021, 현재: 0.3878295421600342\n",
      "Epoch[44/100] - Train loss: 0.35503 Train Accucracy: 0.94601 || Validation Loss: 0.37804 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 44 - 이전 : 0.3878295421600342, 현재: 0.3780445158481598\n",
      "Epoch[45/100] - Train loss: 0.34462 Train Accucracy: 0.94601 || Validation Loss: 0.36838 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 45 - 이전 : 0.3780445158481598, 현재: 0.3683791756629944\n",
      "Epoch[46/100] - Train loss: 0.33439 Train Accucracy: 0.94836 || Validation Loss: 0.35887 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 46 - 이전 : 0.3683791756629944, 현재: 0.35886651277542114\n",
      "Epoch[47/100] - Train loss: 0.32434 Train Accucracy: 0.94836 || Validation Loss: 0.34952 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 47 - 이전 : 0.35886651277542114, 현재: 0.3495185077190399\n",
      "Epoch[48/100] - Train loss: 0.31451 Train Accucracy: 0.94836 || Validation Loss: 0.34034 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 48 - 이전 : 0.3495185077190399, 현재: 0.34034010767936707\n",
      "Epoch[49/100] - Train loss: 0.30489 Train Accucracy: 0.95070 || Validation Loss: 0.33133 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 49 - 이전 : 0.34034010767936707, 현재: 0.33132854104042053\n",
      "Epoch[50/100] - Train loss: 0.29550 Train Accucracy: 0.95070 || Validation Loss: 0.32252 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 50 - 이전 : 0.33132854104042053, 현재: 0.32252442836761475\n",
      "Epoch[51/100] - Train loss: 0.28637 Train Accucracy: 0.95070 || Validation Loss: 0.31394 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 51 - 이전 : 0.32252442836761475, 현재: 0.3139435946941376\n",
      "Epoch[52/100] - Train loss: 0.27749 Train Accucracy: 0.95070 || Validation Loss: 0.30560 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 52 - 이전 : 0.3139435946941376, 현재: 0.30559635162353516\n",
      "Epoch[53/100] - Train loss: 0.26889 Train Accucracy: 0.95540 || Validation Loss: 0.29749 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 53 - 이전 : 0.30559635162353516, 현재: 0.2974877953529358\n",
      "Epoch[54/100] - Train loss: 0.26057 Train Accucracy: 0.95540 || Validation Loss: 0.28964 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 54 - 이전 : 0.2974877953529358, 현재: 0.2896368205547333\n",
      "Epoch[55/100] - Train loss: 0.25252 Train Accucracy: 0.95540 || Validation Loss: 0.28207 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 55 - 이전 : 0.2896368205547333, 현재: 0.2820667326450348\n",
      "Epoch[56/100] - Train loss: 0.24476 Train Accucracy: 0.95540 || Validation Loss: 0.27475 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 56 - 이전 : 0.2820667326450348, 현재: 0.27475497126579285\n",
      "Epoch[57/100] - Train loss: 0.23727 Train Accucracy: 0.95540 || Validation Loss: 0.26769 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 57 - 이전 : 0.27475497126579285, 현재: 0.26769036054611206\n",
      "Epoch[58/100] - Train loss: 0.23007 Train Accucracy: 0.95540 || Validation Loss: 0.26089 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 58 - 이전 : 0.26769036054611206, 현재: 0.26088619232177734\n",
      "Epoch[59/100] - Train loss: 0.22314 Train Accucracy: 0.95540 || Validation Loss: 0.25433 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 59 - 이전 : 0.26088619232177734, 현재: 0.254334419965744\n",
      "Epoch[60/100] - Train loss: 0.21647 Train Accucracy: 0.95540 || Validation Loss: 0.24799 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 60 - 이전 : 0.254334419965744, 현재: 0.24798615276813507\n",
      "Epoch[61/100] - Train loss: 0.21008 Train Accucracy: 0.95540 || Validation Loss: 0.24187 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 61 - 이전 : 0.24798615276813507, 현재: 0.24187195301055908\n",
      "Epoch[62/100] - Train loss: 0.20393 Train Accucracy: 0.95775 || Validation Loss: 0.23601 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 62 - 이전 : 0.24187195301055908, 현재: 0.23600515723228455\n",
      "Epoch[63/100] - Train loss: 0.19803 Train Accucracy: 0.95775 || Validation Loss: 0.23039 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 63 - 이전 : 0.23600515723228455, 현재: 0.23038935661315918\n",
      "Epoch[64/100] - Train loss: 0.19238 Train Accucracy: 0.96009 || Validation Loss: 0.22500 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 64 - 이전 : 0.23038935661315918, 현재: 0.2249954342842102\n",
      "Epoch[65/100] - Train loss: 0.18696 Train Accucracy: 0.96009 || Validation Loss: 0.21980 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 65 - 이전 : 0.2249954342842102, 현재: 0.21979773044586182\n",
      "Epoch[66/100] - Train loss: 0.18178 Train Accucracy: 0.96009 || Validation Loss: 0.21481 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 66 - 이전 : 0.21979773044586182, 현재: 0.21480776369571686\n",
      "Epoch[67/100] - Train loss: 0.17681 Train Accucracy: 0.96244 || Validation Loss: 0.21002 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 67 - 이전 : 0.21480776369571686, 현재: 0.21001897752285004\n",
      "Epoch[68/100] - Train loss: 0.17206 Train Accucracy: 0.96244 || Validation Loss: 0.20540 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 68 - 이전 : 0.21001897752285004, 현재: 0.2053990364074707\n",
      "Epoch[69/100] - Train loss: 0.16751 Train Accucracy: 0.96714 || Validation Loss: 0.20100 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 69 - 이전 : 0.2053990364074707, 현재: 0.20099836587905884\n",
      "Epoch[70/100] - Train loss: 0.16315 Train Accucracy: 0.96714 || Validation Loss: 0.19679 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 70 - 이전 : 0.20099836587905884, 현재: 0.19678567349910736\n",
      "Epoch[71/100] - Train loss: 0.15899 Train Accucracy: 0.96948 || Validation Loss: 0.19276 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 71 - 이전 : 0.19678567349910736, 현재: 0.19276213645935059\n",
      "Epoch[72/100] - Train loss: 0.15499 Train Accucracy: 0.96948 || Validation Loss: 0.18892 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 72 - 이전 : 0.19276213645935059, 현재: 0.1889183670282364\n",
      "Epoch[73/100] - Train loss: 0.15117 Train Accucracy: 0.96948 || Validation Loss: 0.18525 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 73 - 이전 : 0.1889183670282364, 현재: 0.18525227904319763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[74/100] - Train loss: 0.14751 Train Accucracy: 0.97183 || Validation Loss: 0.18175 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 74 - 이전 : 0.18525227904319763, 현재: 0.18175268173217773\n",
      "Epoch[75/100] - Train loss: 0.14399 Train Accucracy: 0.97183 || Validation Loss: 0.17840 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 75 - 이전 : 0.18175268173217773, 현재: 0.17840033769607544\n",
      "Epoch[76/100] - Train loss: 0.14063 Train Accucracy: 0.97183 || Validation Loss: 0.17520 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 76 - 이전 : 0.17840033769607544, 현재: 0.1751953661441803\n",
      "Epoch[77/100] - Train loss: 0.13740 Train Accucracy: 0.97183 || Validation Loss: 0.17214 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 77 - 이전 : 0.1751953661441803, 현재: 0.17213597893714905\n",
      "Epoch[78/100] - Train loss: 0.13430 Train Accucracy: 0.97183 || Validation Loss: 0.16919 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 78 - 이전 : 0.17213597893714905, 현재: 0.16919437050819397\n",
      "Epoch[79/100] - Train loss: 0.13133 Train Accucracy: 0.97418 || Validation Loss: 0.16638 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 79 - 이전 : 0.16919437050819397, 현재: 0.1663847267627716\n",
      "Epoch[80/100] - Train loss: 0.12847 Train Accucracy: 0.97418 || Validation Loss: 0.16371 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 80 - 이전 : 0.1663847267627716, 현재: 0.16370752453804016\n",
      "Epoch[81/100] - Train loss: 0.12574 Train Accucracy: 0.97653 || Validation Loss: 0.16115 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 81 - 이전 : 0.16370752453804016, 현재: 0.16115185618400574\n",
      "Epoch[82/100] - Train loss: 0.12311 Train Accucracy: 0.97653 || Validation Loss: 0.15870 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 82 - 이전 : 0.16115185618400574, 현재: 0.15870237350463867\n",
      "Epoch[83/100] - Train loss: 0.12058 Train Accucracy: 0.97653 || Validation Loss: 0.15638 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 83 - 이전 : 0.15870237350463867, 현재: 0.1563788205385208\n",
      "Epoch[84/100] - Train loss: 0.11815 Train Accucracy: 0.97887 || Validation Loss: 0.15416 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 84 - 이전 : 0.1563788205385208, 현재: 0.1541638821363449\n",
      "Epoch[85/100] - Train loss: 0.11581 Train Accucracy: 0.98122 || Validation Loss: 0.15205 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 85 - 이전 : 0.1541638821363449, 현재: 0.15205435454845428\n",
      "Epoch[86/100] - Train loss: 0.11356 Train Accucracy: 0.98357 || Validation Loss: 0.15007 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 86 - 이전 : 0.15205435454845428, 현재: 0.1500660479068756\n",
      "Epoch[87/100] - Train loss: 0.11140 Train Accucracy: 0.98357 || Validation Loss: 0.14817 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 87 - 이전 : 0.1500660479068756, 현재: 0.14816837012767792\n",
      "Epoch[88/100] - Train loss: 0.10931 Train Accucracy: 0.98357 || Validation Loss: 0.14636 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 88 - 이전 : 0.14816837012767792, 현재: 0.14635664224624634\n",
      "Epoch[89/100] - Train loss: 0.10730 Train Accucracy: 0.98592 || Validation Loss: 0.14463 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 89 - 이전 : 0.14635664224624634, 현재: 0.1446274369955063\n",
      "Epoch[90/100] - Train loss: 0.10536 Train Accucracy: 0.98592 || Validation Loss: 0.14297 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 90 - 이전 : 0.1446274369955063, 현재: 0.1429709941148758\n",
      "Epoch[91/100] - Train loss: 0.10349 Train Accucracy: 0.98592 || Validation Loss: 0.14138 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 91 - 이전 : 0.1429709941148758, 현재: 0.14137953519821167\n",
      "Epoch[92/100] - Train loss: 0.10169 Train Accucracy: 0.98592 || Validation Loss: 0.13986 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 92 - 이전 : 0.14137953519821167, 현재: 0.13986293971538544\n",
      "Epoch[93/100] - Train loss: 0.09995 Train Accucracy: 0.98592 || Validation Loss: 0.13841 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 93 - 이전 : 0.13986293971538544, 현재: 0.13840989768505096\n",
      "Epoch[94/100] - Train loss: 0.09827 Train Accucracy: 0.98592 || Validation Loss: 0.13702 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 94 - 이전 : 0.13840989768505096, 현재: 0.13702340424060822\n",
      "Epoch[95/100] - Train loss: 0.09665 Train Accucracy: 0.98592 || Validation Loss: 0.13569 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 95 - 이전 : 0.13702340424060822, 현재: 0.13569097220897675\n",
      "Epoch[96/100] - Train loss: 0.09508 Train Accucracy: 0.98592 || Validation Loss: 0.13441 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 96 - 이전 : 0.13569097220897675, 현재: 0.1344076693058014\n",
      "Epoch[97/100] - Train loss: 0.09358 Train Accucracy: 0.98592 || Validation Loss: 0.13318 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 97 - 이전 : 0.1344076693058014, 현재: 0.13317520916461945\n",
      "Epoch[98/100] - Train loss: 0.09212 Train Accucracy: 0.98592 || Validation Loss: 0.13199 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 98 - 이전 : 0.13317520916461945, 현재: 0.13198725879192352\n",
      "Epoch[99/100] - Train loss: 0.09071 Train Accucracy: 0.98592 || Validation Loss: 0.13084 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 99 - 이전 : 0.13198725879192352, 현재: 0.13084100186824799\n",
      "Epoch[100/100] - Train loss: 0.08936 Train Accucracy: 0.98592 || Validation Loss: 0.12974 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 100 - 이전 : 0.13084100186824799, 현재: 0.12974423170089722\n",
      "1.1574528217315674 초\n"
     ]
    }
   ],
   "source": [
    "result = train.fit(wb_train_loader,\n",
    "                   wb_test_loader,\n",
    "                   model,\n",
    "                   loss_fn,\n",
    "                   optimizer,\n",
    "                   100,\n",
    "                   save_best_model = True,\n",
    "                   save_model_path = 'models/bc_model.pt',\n",
    "                   patience = 5,\n",
    "                   device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 분류\n",
    "model2 = FashionMNISTModel()\n",
    "loss_fn2 = nn.CrossEntropyLoss()\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/5] - Train loss: 0.47464 Train Accucracy: 0.81858 || Validation Loss: 0.51285 Validation Accuracy: 0.80860\n",
      "====================================================================================================\n",
      "저장: 1 - 이전 : inf, 현재: 0.5128534719159331\n",
      "Epoch[2/5] - Train loss: 0.35745 Train Accucracy: 0.87215 || Validation Loss: 0.40750 Validation Accuracy: 0.85560\n",
      "====================================================================================================\n",
      "저장: 2 - 이전 : 0.5128534719159331, 현재: 0.4075034079672415\n",
      "Epoch[3/5] - Train loss: 0.31932 Train Accucracy: 0.88325 || Validation Loss: 0.37835 Validation Accuracy: 0.86290\n",
      "====================================================================================================\n",
      "저장: 3 - 이전 : 0.4075034079672415, 현재: 0.3783528844389734\n",
      "Epoch[4/5] - Train loss: 0.34485 Train Accucracy: 0.87645 || Validation Loss: 0.42046 Validation Accuracy: 0.85590\n",
      "====================================================================================================\n",
      "Epoch[5/5] - Train loss: 0.27774 Train Accucracy: 0.89757 || Validation Loss: 0.35082 Validation Accuracy: 0.87000\n",
      "====================================================================================================\n",
      "저장: 5 - 이전 : 0.3783528844389734, 현재: 0.35081603849613213\n",
      "247.88622856140137 초\n"
     ]
    }
   ],
   "source": [
    "result2 = train.fit(fmnist_train_loader,\n",
    "                   fmnist_test_loader,\n",
    "                   model2,\n",
    "                   loss_fn2,\n",
    "                   optimizer2,\n",
    "                   5,\n",
    "                   save_model_path = 'models/fminist_model.pt',\n",
    "                   early_stopping = False,\n",
    "                   device = device,\n",
    "                   mode = 'multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
